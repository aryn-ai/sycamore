from abc import abstractmethod
from typing import Any, Optional, List, Dict, Tuple

from sycamore.llms.prompts.default_prompts import EntityExtractorMessagesPrompt
from sycamore.query.execution.metrics import SycamoreQueryLogger
from sycamore.query.operators.count import Count
from sycamore.query.operators.filter import Filter
from sycamore.query.operators.limit import Limit
from sycamore.query.operators.llmextract import LlmExtract
from sycamore.query.operators.llmfilter import LlmFilter
from sycamore.query.operators.llmgenerate import LlmGenerate
from sycamore.query.operators.loaddata import LoadData
from sycamore.query.operators.topk import TopK
from sycamore.query.operators.join import Join

from sycamore.query.execution.operations import (
    llm_generate_operation,
    llm_filter_operation,
    range_filter_operation,
    match_filter_operation,
    count_operation,
    top_k_operation,
    join_operation,
)
from sycamore.llms import OpenAI, OpenAIModels
from sycamore.transforms.extract_entity import OpenAIEntityExtractor
from sycamore.utils.cache import S3Cache

from sycamore import DocSet, Context
from sycamore.query.operators.logical_operator import LogicalOperator
from sycamore.query.execution.physical_operator import PhysicalOperator, get_var_name, get_str_for_dict


class SycamoreOperator(PhysicalOperator):
    """
    This interface is a Sycamore platform implementation of a Logical Operator generated by the query planner.
    It serves 2 purposes:
    1. Execute the node using Sycamore tools (possibly lazy)
    2. Return a python script in string form that can be run to achieve the same result

    Args:
        context (Context): The Sycamore context to use.
        logical_node (Operator): The logical query plan node to execute. Contains runtime params based on type.
        query_id (str): Query id
        inputs (List[Any]): List of inputs required to execute the node. Varies based on node type.
    """

    def __init__(
        self,
        context: Context,
        logical_node: LogicalOperator,
        query_id: str,
        inputs: Optional[List[Any]] = None,
        trace_dir: Optional[str] = None,
    ) -> None:
        super().__init__(logical_node, query_id, inputs)
        self.context = context
        self.trace_dir = trace_dir

    @abstractmethod
    def execute(self) -> Any:
        """
        execute the node
        :return: execution result, can be a Lazy DocSet plan, or executed result like a integer (for count)
        """
        pass

    @abstractmethod
    def script(self, input_var: Optional[str] = None, output_var: Optional[str] = None) -> Tuple[str, List[str]]:
        pass

    def get_node_args(self) -> Dict:
        return {"name": str(self.logical_node.node_id)}

    def get_execute_args(self) -> Dict:
        intermediate_datasink_kwargs: Dict[str, Any] = {
            "query_id": self.query_id,
            "node_id": self.logical_node.node_id,
            "path": "none",
        }
        if self.trace_dir:
            intermediate_datasink_kwargs.update(
                {"makedirs": True, "verbose": True, "path": f"{self.trace_dir}/{self.query_id}/"}
            )
        args = {
            "write_intermediate_data": True,
            "intermediate_datasink": SycamoreQueryLogger,
            "intermediate_datasink_kwargs": intermediate_datasink_kwargs,
        }
        args.update(self.get_node_args())
        return args


class SycamoreLoadData(SycamoreOperator):
    """
    Currently only supports an OpenSearch scan load implementation.
    Args:
        os_client_args (dict): OpenSearch client args passed to OpenSearchScan to initialize the client.
    """

    def __init__(
        self,
        context: Context,
        logical_node: LoadData,
        query_id: str,
        os_client_args: Dict,
        trace_dir: Optional[str] = None,
    ) -> None:
        super().__init__(context=context, logical_node=logical_node, query_id=query_id, trace_dir=trace_dir)
        self.os_client_args = os_client_args

    def execute(self) -> Any:
        assert self.logical_node.data and "index" in self.logical_node.data
        result = self.context.read.opensearch(
            os_client_args=self.os_client_args, index_name=self.logical_node.data["index"]
        )
        return result

    def script(self, input_var: Optional[str] = None, output_var: Optional[str] = None) -> Tuple[str, List[str]]:
        assert self.logical_node.data is not None and "index" in self.logical_node.data
        return (
            f"""
os_client_args = {self.os_client_args}
{output_var or get_var_name(self.logical_node)} = context.read.opensearch(
    os_client_args=os_client_args,
    index_name='{self.logical_node.data["index"]}'
)
""",
            [],
        )


class SycamoreLlmGenerate(SycamoreOperator):
    """
    Use an LLM to generate a response based on the user input question and provided result set.
    Args:
        s3_cache_path (str): Optional S3 path to use for caching
    """

    def __init__(
        self,
        context: Context,
        logical_node: LlmGenerate,
        query_id: str,
        inputs: Optional[List[Any]] = None,
        trace_dir: Optional[str] = None,
        s3_cache_path: Optional[str] = None,
    ) -> None:
        super().__init__(context, logical_node, query_id, inputs, trace_dir=trace_dir)
        self.s3_cache_path = s3_cache_path

    def execute(self) -> Any:
        assert self.inputs and len(self.inputs) >= 1, "LlmGenerate requires at least 1 input node"
        assert self.logical_node.data is not None
        question = self.logical_node.data.get("question")
        assert question is not None and isinstance(question, str)
        description = self.logical_node.data.get("description")
        assert description is not None and isinstance(description, str)
        result = llm_generate_operation(
            client=OpenAI(OpenAIModels.GPT_4O.value, cache=S3Cache(self.s3_cache_path) if self.s3_cache_path else None),
            question=question,
            result_description=description,
            result_data=self.inputs,
            **self.get_execute_args(),
        )
        return result

    def script(self, input_var: Optional[str] = None, output_var: Optional[str] = None) -> Tuple[str, List[str]]:
        assert self.logical_node.data is not None
        question = self.logical_node.data.get("question")
        assert question is not None and isinstance(question, str)
        description = self.logical_node.data.get("description")
        assert description is not None and isinstance(description, str)
        assert self.logical_node.dependencies is not None and len(self.logical_node.dependencies) >= 1

        cache_string = ""
        if self.s3_cache_path:
            cache_string = f", cache=S3Cache('{self.s3_cache_path}')"
        result = f"""
{output_var or get_var_name(self.logical_node)} = llm_generate_operation(
    client=OpenAI(OpenAIModels.GPT_4O.value{cache_string}),
    question='{question}',
    result_description='{description}',
    result_data={[input_var or get_var_name(inp) for inp in self.logical_node.dependencies]}
)
print({output_var or get_var_name(self.logical_node)})
"""
        return result, [
            "from sycamore.query.execution.operations import llm_generate_operation",
            "from sycamore.llms import OpenAI, OpenAIModels",
        ]


class SycamoreLlmFilter(SycamoreOperator):
    """
    Use an LLM to filter records on a Docset.
    Args:
        s3_cache_path (str): Optional S3 path to use for caching
    """

    def __init__(
        self,
        context: Context,
        logical_node: LlmFilter,
        query_id: str,
        inputs: Optional[List[Any]] = None,
        trace_dir: Optional[str] = None,
        s3_cache_path: Optional[str] = None,
    ) -> None:
        super().__init__(context, logical_node, query_id, inputs, trace_dir=trace_dir)
        self.s3_cache_path = s3_cache_path

    def execute(self) -> Any:
        assert self.inputs and len(self.inputs) == 1, "LlmFilter requires 1 input node"
        assert isinstance(self.inputs[0], DocSet), "LlmFilter requires a DocSet input"
        assert self.logical_node.data is not None
        question = self.logical_node.data.get("question")
        assert question is not None and isinstance(question, str)
        field = self.logical_node.data.get("field")
        assert field is not None and isinstance(field, str)

        # load into local vars for Ray serialization magic
        s3_cache_path = self.s3_cache_path

        result = llm_filter_operation(
            client=OpenAI(OpenAIModels.GPT_4O.value, cache=S3Cache(s3_cache_path) if s3_cache_path else None),
            docset=self.inputs[0],
            filter_question=question,
            field=field,
            messages=None,
            threshold=3,
            **self.get_node_args(),
        )
        return result

    def script(self, input_var: Optional[str] = None, output_var: Optional[str] = None) -> Tuple[str, List[str]]:
        assert self.logical_node.dependencies is not None and len(self.logical_node.dependencies) == 1
        assert self.logical_node.data is not None
        question = self.logical_node.data.get("question")
        assert question is not None and isinstance(question, str)
        field = self.logical_node.data.get("field")
        assert field is not None and isinstance(field, str)

        cache_string = ""
        if self.s3_cache_path:
            cache_string = f", cache=S3Cache('{self.s3_cache_path}')"
        result = f"""
{output_var or get_var_name(self.logical_node)} = llm_filter_operation(
    client=OpenAI(OpenAIModels.GPT_4O.value{cache_string}),
    docset={input_var or get_var_name(self.logical_node.dependencies[0])},
    filter_question='{question}',
    field='{field}',
    threshold=3,
    **{self.get_node_args()},
)
"""
        return result, [
            "from sycamore.query.execution.operations import llm_filter_operation",
            "from sycamore.llms import OpenAI, OpenAIModels",
        ]


class SycamoreFilter(SycamoreOperator):
    """
    Filter a DocSet
    """

    def __init__(
        self,
        context: Context,
        logical_node: Filter,
        query_id: str,
        inputs: Optional[List[Any]] = None,
        trace_dir: Optional[str] = None,
    ) -> None:
        super().__init__(context, logical_node, query_id, inputs, trace_dir=trace_dir)

    def execute(self) -> Any:
        assert self.inputs and len(self.inputs) == 1, "Filter requires 1 input node"
        assert isinstance(self.inputs[0], DocSet), "Filter requires a DocSet input"

        # Load into local vars for Ray serialization magic.
        logical_node = self.logical_node
        assert logical_node.data is not None

        if logical_node.data.get("rangeFilter"):
            field = logical_node.data.get("field")
            assert field is not None and isinstance(field, str)
            start = logical_node.data.get("start")
            end = logical_node.data.get("end")
            date = logical_node.data.get("date") or False

            result = self.inputs[0].filter(
                lambda doc: range_filter_operation(doc=doc, field=str(field), start=start, end=end, date=date),
                **self.get_node_args(),
            )
        else:
            query = logical_node.data.get("query")
            assert query is not None
            field = logical_node.data.get("field")
            assert field is not None and isinstance(field, str)
            result = self.inputs[0].filter(
                lambda doc: match_filter_operation(doc=doc, query=query, field=field),
                **self.get_node_args(),
            )
        return result

    def script(self, input_var: Optional[str] = None, output_var: Optional[str] = None) -> Tuple[str, List[str]]:
        assert self.logical_node.data is not None
        assert self.logical_node.dependencies is not None and len(self.logical_node.dependencies) == 1
        script = ""
        imports = []
        if self.logical_node.data.get("rangeFilter"):
            field = self.logical_node.data.get("field")
            assert field is not None and isinstance(field, str)
            start = self.logical_node.data.get("start")
            assert start is None or isinstance(start, str)
            end = self.logical_node.data.get("end")
            assert end is None or isinstance(end, str)
            date = self.logical_node.data.get("date") or False

            script = f"""
{output_var or get_var_name(self.logical_node)} = {input_var or get_var_name(self.logical_node.dependencies[0])}.filter(
    lambda doc: range_filter_operation(
        doc=doc,
        field='{field}',
        start='{start}',
        end='{end}',
        date='{date}',
    ),
    **{self.get_node_args()},
)
            """
            imports = ["from sycamore.query.execution.operations import range_filter_operation"]
        else:
            script = f"""
{output_var or get_var_name(self.logical_node)} = {input_var or get_var_name(self.logical_node.dependencies[0])}.filter(
    lambda doc: match_filter_operation(
        doc=doc,
        query='{self.logical_node.data.get("query")}',
        field='{self.logical_node.data.get("field")}',
    ),
    **{self.get_node_args()},
)
"""
            imports = ["from sycamore.query.execution.operations import match_filter_operation"]
        return script, imports


class SycamoreCount(SycamoreOperator):
    """
    Count documents in a DocSet. Can do a unique count optionally.
    """

    def __init__(
        self,
        context: Context,
        logical_node: Count,
        query_id: str,
        inputs: Optional[List[Any]] = None,
        trace_dir: Optional[str] = None,
    ) -> None:
        super().__init__(context, logical_node, query_id, inputs, trace_dir=trace_dir)

    def execute(self) -> Any:
        assert self.inputs and len(self.inputs) == 1, "Count requires 1 input node"
        assert isinstance(self.inputs[0], DocSet), "Count requires a DocSet input"

        # load into local vars for Ray serialization magic
        logical_node = self.logical_node
        assert logical_node.data is not None
        assert "field" in logical_node.data or "primaryField" in logical_node.data

        result = count_operation(
            docset=self.inputs[0],
            field=logical_node.data.get("field"),
            primary_field=logical_node.data.get("primaryField"),
            **self.get_execute_args(),
        )
        return result

    def script(self, input_var: Optional[str] = None, output_var: Optional[str] = None) -> Tuple[str, List[str]]:
        assert self.logical_node.dependencies is not None and len(self.logical_node.dependencies) == 1
        assert self.logical_node.data is not None
        assert "field" in self.logical_node.data or "primaryField" in self.logical_node.data
        imports = ["from sycamore.query.execution.operations import count_operation"]
        script = f"""
{output_var or get_var_name(self.logical_node)} = count_operation(
    docset={input_var or get_var_name(self.logical_node.dependencies[0])},
    """
        if self.logical_node.data.get("field"):
            script += f"""field='{self.logical_node.data.get("field")}',
    """
        if self.logical_node.data.get("primaryField"):
            script += f"""primaryField='{self.logical_node.data.get("primaryField")}',
    """
        script += f"""**{get_str_for_dict(self.get_execute_args())},
)
"""
        return script, imports


class SycamoreLlmExtract(SycamoreOperator):
    """
    Use an LLM to extract information from your data. The data is available for downstream tasks to consume.
    Args:
        s3_cache_path (str): Optional S3 path to use for caching
    """

    def __init__(
        self,
        context: Context,
        logical_node: LlmExtract,
        query_id: str,
        inputs: Optional[List[Any]] = None,
        trace_dir: Optional[str] = None,
        s3_cache_path: Optional[str] = None,
    ) -> None:
        super().__init__(context, logical_node, query_id, inputs, trace_dir=trace_dir)
        self.s3_cache_path = s3_cache_path

    def execute(self) -> Any:
        assert self.inputs and len(self.inputs) == 1, "LlmExtract requires 1 input node"
        assert isinstance(self.inputs[0], DocSet), "LlmExtract requires a DocSet input"
        # load into local vars for Ray serialization magic
        s3_cache_path = self.s3_cache_path
        logical_node = self.logical_node
        assert logical_node.data is not None
        question = logical_node.data.get("question")
        assert question is None or isinstance(question, str)
        new_field = logical_node.data.get("newField")
        assert new_field is not None and isinstance(new_field, str)
        field = logical_node.data.get("field")
        assert field is None or isinstance(field, str)
        fmt = logical_node.data.get("format")
        assert fmt is None or isinstance(fmt, str)
        discrete = logical_node.data.get("discrete") or False

        messages = EntityExtractorMessagesPrompt(
            question=question, field=field, format=fmt, discrete=discrete
        ).get_messages_dict()

        entity_extractor = OpenAIEntityExtractor(
            entity_name=new_field,
            llm=OpenAI(OpenAIModels.GPT_4O.value, cache=S3Cache(s3_cache_path) if s3_cache_path else None),
            use_elements=False,
            messages=messages,
            field=field,
        )
        result = self.inputs[0].extract_entity(entity_extractor=entity_extractor, **self.get_node_args())
        return result

    def script(self, input_var: Optional[str] = None, output_var: Optional[str] = None) -> Tuple[str, List[str]]:
        logical_node = self.logical_node
        assert logical_node.data is not None
        question = logical_node.data.get("question")
        assert question is None or isinstance(question, str)
        new_field = logical_node.data.get("newField")
        assert new_field is not None and isinstance(new_field, str)
        field = logical_node.data.get("field")
        assert field is None or isinstance(field, str)
        fmt = logical_node.data.get("format")
        assert fmt is None or isinstance(fmt, str)
        discrete = logical_node.data.get("discrete") or False
        assert logical_node.dependencies is not None and len(logical_node.dependencies) == 1

        cache_string = ""
        if self.s3_cache_path:
            cache_string = f", cache=S3Cache('{self.s3_cache_path}')"
        result = f"""
        messages = EntityExtractorMessagesPrompt(
                question='{question}', field='{field}', format='{fmt}, discrete={discrete}
            ).get_messages_dict()

        entity_extractor = OpenAIEntityExtractor(
            entity_name='{new_field}',
            llm=OpenAI(OpenAIModels.GPT_4O.value{cache_string}),
            use_elements=False,
            messages=messages,
            field='{field}',
        )
    {output_var or get_var_name(logical_node)} = 
        {input_var or get_var_name(logical_node.dependencies[0])}.extract_entity(
                entity_extractor=entity_extractor,
                **{self.get_node_args()}
            )
    """
        return result, [
            "from sycamore.llms.prompts.default_prompts import EntityExtractorMessagesPrompt",
            "from sycamore.transforms.extract_entity import OpenAIEntityExtractor",
            "from sycamore.llms import OpenAI, OpenAIModels",
        ]


class SycamoreSort(SycamoreOperator):
    """
    Sort a DocSet on a given key.
    """

    def __init__(
        self,
        context: Context,
        logical_node: LogicalOperator,
        query_id: str,
        inputs: Optional[List[Any]] = None,
        trace_dir: Optional[str] = None,
    ) -> None:
        super().__init__(context, logical_node, query_id, inputs, trace_dir=trace_dir)

    def execute(self) -> Any:
        assert self.inputs and len(self.inputs) == 1, "Sort requires 1 input node"
        assert isinstance(self.inputs[0], DocSet), "Sort requires a DocSet input"

        # load into local vars for Ray serialization magic
        logical_node = self.logical_node
        assert logical_node.data is not None
        descending = logical_node.data.get("descending") or False
        field = logical_node.data.get("field")
        default_val = logical_node.data.get("defaultValue")
        assert field is not None and isinstance(field, str)

        result = self.inputs[0].sort(descending=descending, field=field, default_val=default_val)

        return result

    def script(self, input_var: Optional[str] = None, output_var: Optional[str] = None) -> Tuple[str, List[str]]:
        logical_node = self.logical_node
        assert logical_node.data is not None
        descending = logical_node.data.get("descending") or False
        field = logical_node.data.get("field")
        assert field is not None and isinstance(field, str)
        assert logical_node.dependencies is not None and len(logical_node.dependencies) == 1

        result = f"""
{output_var or get_var_name(self.logical_node)} = {input_var or get_var_name(logical_node.dependencies[0])}.sort(
    descending={descending},
    field='{field}'
)
"""
        return result, []


class SycamoreTopK(SycamoreOperator):
    """
    Return the Top-K values from a DocSet
    Args:
        s3_cache_path (str): Optional S3 path to use for caching when using an LLM
    """

    def __init__(
        self,
        context: Context,
        logical_node: TopK,
        query_id: str,
        inputs: Optional[List[Any]] = None,
        trace_dir: Optional[str] = None,
        s3_cache_path: Optional[str] = None,
    ) -> None:
        super().__init__(context, logical_node, query_id, inputs, trace_dir=trace_dir)
        self.s3_cache_path = s3_cache_path

    def execute(self) -> Any:
        assert self.inputs and len(self.inputs) == 1, "TopK requires 1 input node"
        assert isinstance(self.inputs[0], DocSet), "TopK requires a DocSet input"
        # load into local vars for Ray serialization magic
        s3_cache_path = self.s3_cache_path
        logical_node = self.logical_node
        assert logical_node.data is not None
        field = logical_node.data.get("field")
        assert field is not None and isinstance(field, str)
        k = logical_node.data.get("K")
        assert k is None or isinstance(k, int), f"K is {k}, expected None or int"
        description = logical_node.data.get("description")
        assert description is not None and isinstance(description, str)
        descending = logical_node.data.get("descending") or False
        use_llm = logical_node.data.get("useLLM") or False
        unique_field = logical_node.data.get("primaryField")
        assert unique_field is None or isinstance(unique_field, str)

        result = top_k_operation(
            client=OpenAI(OpenAIModels.GPT_4O.value, cache=S3Cache(s3_cache_path) if s3_cache_path else None),
            docset=self.inputs[0],
            field=field,
            k=k,
            description=description,
            descending=descending,
            use_llm=use_llm,
            unique_field=unique_field,
            **self.get_execute_args(),
        )
        return result

    def script(self, input_var: Optional[str] = None, output_var: Optional[str] = None) -> Tuple[str, List[str]]:
        logical_node = self.logical_node
        assert logical_node.data is not None
        field = logical_node.data.get("field")
        assert field is not None and isinstance(field, str)
        k = logical_node.data.get("K")
        assert k is not None and isinstance(k, int)
        description = logical_node.data.get("description")
        assert description is not None and isinstance(description, str)
        descending = logical_node.data.get("descending") or False
        use_llm = logical_node.data.get("useLLM") or False
        unique_field = logical_node.data.get("primaryField")
        assert unique_field is None or isinstance(unique_field, str)
        assert logical_node.dependencies is not None and len(logical_node.dependencies) == 1

        cache_string = ""
        if self.s3_cache_path:
            cache_string = f", cache=S3Cache('{self.s3_cache_path}')"
        result = f"""
{output_var or get_var_name(self.logical_node)} = top_k_operation(
    client=OpenAI(OpenAIModels.GPT_4O.value{cache_string}),
    docset={input_var or get_var_name(logical_node.dependencies[0])},
    field='{field}',
    k={k},
    description='{description}',
    descending={descending}',
    use_llm={use_llm},
    unique_field='{unique_field}',
    **{self.get_execute_args()},
)
"""
        return result, [
            "from sycamore.query.execution.operations import top_k_operation",
            "from sycamore.llms import OpenAI, OpenAIModels",
        ]


class SycamoreJoin(SycamoreOperator):
    """
    Return 2 DocSets joined
    """

    def __init__(
        self,
        context: Context,
        logical_node: Join,
        query_id: str,
        inputs: Optional[List[Any]] = None,
        trace_dir: Optional[str] = None,
    ) -> None:
        super().__init__(
            context=context, logical_node=logical_node, query_id=query_id, inputs=inputs, trace_dir=trace_dir
        )

    def execute(self) -> Any:
        assert self.inputs and len(self.inputs) == 2, "Join requires 2 input nodes"
        assert isinstance(self.inputs[0], DocSet) and isinstance(
            self.inputs[1], DocSet
        ), "Join requires 2 DocSet inputs"

        logical_node = self.logical_node
        assert logical_node.data is not None
        field1 = logical_node.data.get("fieldOne")
        assert field1 is not None and isinstance(field1, str)
        field2 = logical_node.data.get("fieldTwo")
        assert field2 is not None and isinstance(field2, str)

        result = join_operation(
            docset1=self.inputs[0],
            docset2=self.inputs[1],
            field1=field1,
            field2=field2,
        )
        return result

    def script(self, input_var: Optional[str] = None, output_var: Optional[str] = None) -> Tuple[str, List[str]]:
        logical_node = self.logical_node
        assert logical_node.data is not None
        field1 = logical_node.data.get("fieldOne")
        assert field1 is not None and isinstance(field1, str)
        field2 = logical_node.data.get("fieldTwo")
        assert field2 is not None and isinstance(field2, str)
        assert logical_node.dependencies is not None and len(logical_node.dependencies) == 2

        result = f"""
{output_var or get_var_name(self.logical_node)} = join_operation(
    docset1={input_var or get_var_name(logical_node.dependencies[0])},
    docset2={input_var or get_var_name(logical_node.dependencies[2])},
    field1='{field1}',
    field2='{field2}'
)
"""
        return result, ["from sycamore.query.execution.operations import join_operation"]


class SycamoreLimit(SycamoreOperator):
    """
    Limit the number of results on a DocSet
    """

    def __init__(
        self,
        context: Context,
        logical_node: Limit,
        query_id: str,
        inputs: Optional[List[Any]] = None,
        trace_dir: Optional[str] = None,
    ) -> None:
        super().__init__(context, logical_node, query_id, inputs, trace_dir=trace_dir)

    def execute(self) -> Any:
        assert self.inputs and len(self.inputs) == 1, "Limit requires 1 input node"
        assert isinstance(self.inputs[0], DocSet), "Limit requires a DocSet input"

        # load into local vars for Ray serialization magic
        logical_node = self.logical_node
        assert logical_node.data is not None
        k = logical_node.data.get("K")
        assert k is not None and isinstance(k, int)

        result = self.inputs[0].limit(k)
        return result

    def script(self, input_var: Optional[str] = None, output_var: Optional[str] = None) -> Tuple[str, List[str]]:
        logical_node = self.logical_node
        assert logical_node.data is not None
        k = logical_node.data.get("K")
        assert k is not None and isinstance(k, int)
        assert logical_node.dependencies is not None and len(logical_node.dependencies) == 1

        result = f"""
{output_var or get_var_name(self.logical_node)} = {input_var or get_var_name(logical_node.dependencies[0])}.limit(
    {k},
    **{self.get_execute_args()},
)
"""
        return result, []
