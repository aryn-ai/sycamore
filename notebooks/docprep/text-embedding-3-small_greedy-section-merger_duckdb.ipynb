{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install sycamore-ai[duckdb]\n",
        "# DocPrep code uses the Sycamore document ETL library: https://github.com/aryn-ai/sycamore "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!apt-get install poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pyarrow.fs\n",
        "import sycamore\n",
        "import json\n",
        "import os\n",
        "import duckdb\n",
        "from sycamore.functions.tokenizer import OpenAITokenizer\n",
        "from sycamore.llms import OpenAIModels, OpenAI\n",
        "from sycamore.transforms import COALESCE_WHITESPACE\n",
        "from sycamore.transforms.merge_elements import GreedySectionMerger\n",
        "from sycamore.transforms.partition import ArynPartitioner\n",
        "from sycamore.transforms.embed import OpenAIEmbedder\n",
        "from sycamore.materialize_config import MaterializeSourceMode\n",
        "from sycamore.utils.pdf_utils import show_pages\n",
        "from sycamore.transforms.summarize_images import SummarizeImages\n",
        "from sycamore.context import ExecMode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# It's best to store API keys in a configuration file or set them as environment variables.  \n",
        "# For quick testing, you can define them here:\n",
        "#\n",
        "# os.environ[\"ARYN_API_KEY\"] = \"YOUR_ARYN_API_KEY\"\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sycamore uses lazy execution for efficiency, so the ETL pipeline will only execute when running cells with specific functions.\n",
        "  \n",
        "paths = [\"s3://aryn-public/ntsb/59.pdf\"]\n",
        "# Configure your AWS credentials here if the bucket is private\n",
        "fsys = pyarrow.fs.S3FileSystem(region=\"us-east-1\", anonymous=True)\n",
        "# Initialize the Sycamore context\n",
        "ctx = sycamore.init(ExecMode.LOCAL)\n",
        "# Set the embedding model and its parameters\n",
        "model_name = \"text-embedding-3-small\"\n",
        "max_tokens = 8191\n",
        "dimensions = 1536\n",
        "# Initialize the tokenizer\n",
        "tokenizer = OpenAITokenizer(model_name)\n",
        "\n",
        "ds = (\n",
        "    ctx.read.binary(paths, binary_format=\"pdf\", filesystem=fsys)\n",
        "    # Partition and extract tables and images\n",
        "    .partition(partitioner=ArynPartitioner(\n",
        "        threshold=\"auto\",\n",
        "        use_ocr=True,\n",
        "        extract_table_structure=True,\n",
        "        extract_images=True\n",
        "    ))\n",
        "    # Use materialize to cache output. If changing upstream code or input files, change setting from USE_STORED to RECOMPUTE to create a new cache.\n",
        "    .materialize(path=\"./materialize/partitioned\", source_mode=MaterializeSourceMode.USE_STORED)\n",
        "    # Merge elements into larger chunks\n",
        "    .merge(merger=GreedySectionMerger(\n",
        "      tokenizer=tokenizer,  max_tokens=max_tokens, merge_across_pages=False\n",
        "    ))\n",
        "    # Split elements that are too big to embed\n",
        "    .split_elements(tokenizer=tokenizer, max_tokens=max_tokens)\n",
        ")\n",
        "\n",
        "ds.execute()\n",
        "\n",
        "# Display the first 3 pages after chunking\n",
        "show_pages(ds, limit=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embedded_ds = (\n",
        "    # Copy document properties to each Document's sub-elements\n",
        "    ds.spread_properties([\"path\", \"entity\"])\n",
        "    # Convert all Elements to Documents\n",
        "    .explode() \n",
        "    # Embed each Document. You can change the embedding model. Make your target vector index matches this number of dimensions.\n",
        "    .embed(embedder=OpenAIEmbedder(model_name=model_name))\n",
        ")\n",
        "# To know more about docset transforms, please visit https://sycamore.readthedocs.io/en/latest/sycamore/transforms.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "db_url = \"test-1.db\"\n",
        "table_name = \"test_table\"\n",
        "# Execute the write operation to DuckDB\n",
        "embedded_ds.write.duckdb(\n",
        "    db_url=db_url,\n",
        "    table_name=table_name,\n",
        "    dimensions=dimensions\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify data has been loaded using DocSet Query to retrieve chunks\n",
        "# If you previously used a DuckDB in Colab with a different number of vector dimensions, you may need to restart the runtime.\n",
        "query = f\"SELECT * from {table_name}\"\n",
        "query_docs = ctx.read.duckdb(db_url=db_url, table_name=table_name, query=query)\n",
        "query_docs.show(show_embedding=False)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
