{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Aryn Partitioner in this job is configured to use the Aryn Partitioning Service to provide fast, GPU-powered performance. Go to [aryn.ai/sign-up ](aryn.ai/sign-up) to get a free API key for the service. This is the recommended configuration.\n",
    "\n",
    "You can also run the Aryn Partitioner locally by setting `use_partitioning_service` to `False`. Though you can use CPU to run the Aryn Partitioner, it is recommended to use an NVIDIA GPU for good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.fs\n",
    "import sycamore\n",
    "import json\n",
    "from sycamore.functions.tokenizer import HuggingFaceTokenizer\n",
    "from sycamore.llms import OpenAIModels, OpenAI\n",
    "from sycamore.transforms import COALESCE_WHITESPACE\n",
    "from sycamore.transforms.merge_elements import MarkedMerger\n",
    "from sycamore.transforms.partition import SycamorePartitioner\n",
    "from sycamore.transforms.extract_schema import OpenAISchemaExtractor, OpenAIPropertyExtractor\n",
    "from sycamore.transforms.embed import SentenceTransformerEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function used for cleaning and formatting later on in the notebook\n",
    "\n",
    "from sycamore.data.document import Document\n",
    "from dateutil import parser\n",
    "def convert_timestamp(doc: Document) -> Document:\n",
    "    if \"dateAndTime\" not in doc.properties['entity'] and \"dateTime\" not in doc.properties['entity']:\n",
    "        return doc\n",
    "    raw_date: str = doc.properties['entity'].get('dateAndTime') or doc.properties['entity'].get('dateTime')\n",
    "    raw_date = raw_date.replace(\"Local\", \"\")\n",
    "    parsed_date = parser.parse(raw_date, fuzzy=True)\n",
    "    extracted_date = parsed_date.date()\n",
    "    doc.properties['entity']['day'] = extracted_date.day\n",
    "    doc.properties['entity']['month'] = extracted_date.month\n",
    "    doc.properties['entity']['year'] = extracted_date.year\n",
    "    if parsed_date.utcoffset():\n",
    "        doc.properties['entity']['dateTime'] = parsed_date.isoformat()\n",
    "    else:\n",
    "        doc.properties['entity']['dateTime'] = parsed_date.isoformat() + \"Z\"\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the `YOUR-ARYN-API-KEY` with your key and execute the export command\n",
    "\n",
    "`export ARYN_API_KEY=\"YOUR-ARYN-API-KEY\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sycamore.utils.aryn_config import ArynConfig\n",
    "aryn_api_key = ArynConfig.get_aryn_api_key()\n",
    "\n",
    "# you can also read directly from env variable like this\n",
    "# import os\n",
    "# aryn_api_key = os.environ.get(\"ARYN_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"s3://aryn-public/ntsb/\"]\n",
    "fsys = pyarrow.fs.S3FileSystem(region=\"us-east-1\", anonymous=True)\n",
    "\n",
    "llm = OpenAI(OpenAIModels.GPT_3_5_TURBO.value)\n",
    "tokenizer = HuggingFaceTokenizer(\"thenlper/gte-small\")\n",
    "\n",
    "ctx = sycamore.init()\n",
    "\n",
    "ds = (\n",
    "    ctx.read.binary(paths, binary_format=\"pdf\", filesystem=fsys)\n",
    "    # Partition with the Aryn partitioner remotely, pulling out tables and images.\n",
    "    .partition(partitioner=ArynPartitioner(aryn_api_key=aryn_api_key, extract_images=True,  extract_table_structure=True))\n",
    "    # Get rid of spurious whitespace charaters\n",
    "    .regex_replace(COALESCE_WHITESPACE)\n",
    "    # Automatically determine a schema of additional metadata to extract from Documents\n",
    "    .extract_batch_schema(schema_extractor=OpenAISchemaExtractor(\"FlightAccidentReport\", llm=llm, num_of_elements=35))\n",
    "    # Extract the metadata specified by that schema\n",
    "    .extract_properties(property_extractor=OpenAIPropertyExtractor(llm=llm, num_of_elements=35))\n",
    "    # Merge elements into larger chunks\n",
    "    .mark_bbox_preset(tokenizer=tokenizer)\n",
    "    .merge(merger=MarkedMerger())\n",
    "    # Convert extracted timestamps to better-structured form using the function above\n",
    "    .map(convert_timestamp)\n",
    "    # Copy document properties to each Document's sub-elements\n",
    "    .spread_properties([\"path\", \"entity\"])\n",
    "    # Split elements that are too big to embed\n",
    "    .split_elements(tokenizer=tokenizer, max_tokens=512)\n",
    "    # Convert all Elements to Documents\n",
    "    .explode()\n",
    "    # Generate a series of hashes to represent each Document. For use with near-duplicate detection\n",
    "    .sketch()\n",
    "    # Embed each Document\n",
    "    .embed(embedder=SentenceTransformerEmbedder(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", batch_size=100))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to a persistent DuckDB database\n",
    "# - into a specific database (as specified by url) \n",
    "# - into a specific table (as specified by table name)\n",
    "persistent_db = \"demo.db\"\n",
    "persistent_table = \"demo_table\"\n",
    "ds.write.duckdb(\n",
    "    db_url=persistent_db,\n",
    "    table_name=persistent_table,\n",
    "    dimensions=384\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We connect to the DuckDB to perform operations\n",
    "import duckdb\n",
    "data_conn = duckdb.connect(\"demo.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DuckDB's efficient Vector Similarity Search on persistent databases is still an experimental feature (https://duckdb.org/docs/extensions/vss.html),\n",
    "# so we load our data into an in-memory database. Once this feature is more stable, we can query the persistent database directly using VSS.\n",
    "\n",
    "# Load from disk into Pandas Dataframe as an intermediate step\n",
    "df = data_conn.execute(\"SELECT * FROM demo_table\").fetchdf()\n",
    "df.dropna(subset=[\"embeddings\"],inplace=True)\n",
    "\n",
    "\n",
    "# Now, we load the data into an in-memory database, to help with faster computation\n",
    "\n",
    "in_memory_db = duckdb.connect(\":default:\")\n",
    "in_memory_db.install_extension(\"vss\")\n",
    "in_memory_db.load_extension(\"vss\")\n",
    "\n",
    "# To allow RAG to work properly on this database using Langchain (shown later), we must rename the properties column to metadata, \n",
    "# add a 'source' key within it, and convert its datatype to string. Note that the value 'default' can be modified to other options if needed\n",
    "new_key = 'source'\n",
    "new_value = 'default'\n",
    "df['properties'] = df['properties'].apply(lambda x: dict(zip(x['key'], x['value'])))\n",
    "df['properties'] = df['properties'].apply(lambda x: {**x, new_key: new_value}).apply(lambda x: json.dumps(x))\n",
    "df = df.rename(columns={'properties': 'metadata'})\n",
    "\n",
    "# Notice that we specify beforehand since we need the 'metadata' column (must be specified as of type FLOAT[N] where N is specified) \n",
    "# for Vector Similarity Search in DuckDB to work.\n",
    "\n",
    "schema = {\n",
    "            \"doc_id\": \"VARCHAR\",\n",
    "            \"embeddings\": \"FLOAT[384]\",\n",
    "            \"metadata\": \"VARCHAR\",\n",
    "            \"text_representation\": \"VARCHAR\",\n",
    "            \"bbox\": \"DOUBLE[]\",\n",
    "            \"shingles\": \"BIGINT[]\",\n",
    "            \"type\": \"VARCHAR\",\n",
    "        }\n",
    "in_memory_db.execute(f\"\"\"CREATE TABLE in_memory_table (doc_id {schema.get('doc_id')},\n",
    "                      embeddings {schema.get('embeddings')}, metadata {schema.get('metadata')}, \n",
    "                      text_representation {schema.get('text_representation')}, bbox {schema.get('bbox')}, \n",
    "                      shingles {schema.get('shingles')}, type {schema.get('type')})\"\"\"\n",
    "    )\n",
    "in_memory_db.execute(\"\"\"INSERT INTO in_memory_table SELECT * FROM df; \n",
    "                      CREATE INDEX in_memory_table_index ON in_memory_table USING HNSW(embeddings)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For queries, let's define an embedding function for the question that helps us easily run NN search \n",
    "# by comparing the two vectors and generating optimal results\n",
    "from sentence_transformers import SentenceTransformer\n",
    "minilm = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "class Embedder():\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "    \n",
    "    def embed_query(self, question):\n",
    "        v = self.llm.encode(question).tolist()\n",
    "        return v\n",
    "\n",
    "embedder = Embedder(minilm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use DuckDB's ANN HNSW semantic search for retrieval. We use Array Euclidean Distance as the nearest neighbor lookup, and pretty print the result\n",
    "from tabulate import tabulate\n",
    "\n",
    "q = \"What automobile type is the most accident prone?\"\n",
    "result = in_memory_db.sql(f\"SELECT doc_id, text_representation, metadata FROM in_memory_table ORDER BY array_distance(embeddings, {embedder.embed_query(q)}::FLOAT[384]) LIMIT 10;\")\n",
    "\n",
    "# Convert the result to a list of lists\n",
    "table_data = result.fetchall()\n",
    "\n",
    "# Get column names\n",
    "headers = [desc[0] for desc in result.description]\n",
    "\n",
    "# Pretty print the table\n",
    "print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first two rows of the text output to better understand similarity search results\n",
    "\n",
    "first_two = table_data[2:4]\n",
    "\n",
    "print(\"Text representation of the first two values (not including headers):\")\n",
    "for i, row in enumerate(first_two, 1):\n",
    "    print(f\"\\n{i}. doc_id: {row[0]}\")\n",
    "    print(f\"   Text representation:\\n   {row[1]}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now initialize a vector store on DuckDB using the LangChain integration. We perform similarity search using the integration here\n",
    "from langchain_community.vectorstores.duckdb import DuckDB\n",
    "vector_store = DuckDB(connection=in_memory_db, embedding=embedder, id_key= \"doc_id\", text_key=\"text_representation\", vector_key=\"embeddings\", table_name=\"in_memory_table\")\n",
    "result = vector_store.similarity_search('traffic')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI  \n",
    "from langchain.chains import RetrievalQAWithSourcesChain  \n",
    "import os\n",
    "# Finally, we initialize a RAG agent and ask the model a question about the data\n",
    "llm = ChatOpenAI(  \n",
    "    openai_api_key=os.environ.get(\"OPENAI_API_KEY\"),  \n",
    "    model_name='gpt-3.5-turbo',  \n",
    "    temperature=0.8  \n",
    ")  \n",
    "qa = RetrievalQAWithSourcesChain.from_chain_type(  \n",
    "    llm=llm,  \n",
    "    chain_type=\"stuff\",  \n",
    "    retriever=vector_store.as_retriever() , verbose=True\n",
    ")  \n",
    "qa.invoke({\"question\": \"How many accidents happened?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also see the performance of the DuckDB ANN query and check the index is being used by using the in-built visualization below \n",
    "q = \"What automobile type is the most accident prone?\"\n",
    "explain_result = in_memory_db.sql(f\"EXPLAIN SELECT doc_id, text_representation, metadata FROM in_memory_table ORDER BY array_distance(embeddings, {embedder.embed_query(q)}::FLOAT[384]) LIMIT 10\")\n",
    "\n",
    "# Convert the result to a list of lists\n",
    "explain_data = explain_result.fetchall()\n",
    "\n",
    "# Get column names\n",
    "headers = [desc[0] for desc in explain_result.description]\n",
    "\n",
    "# Pretty print the EXPLAIN output\n",
    "print(\"Expanded EXPLAIN output:\")\n",
    "print(tabulate(explain_data, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To experiment with other distance metrics, one can set the 'metric' value as done below\n",
    "# The default is Euclidean Distance ('l2sq'), with Cosine similarity ('cosine', 'array_cosine_simarlity') \n",
    "# and Inner Product ('ip', 'array_inner_product') also supported\n",
    "in_memory_db.execute(\"CREATE INDEX in_memory_ip_index ON in_memory_table USING HNSW(embeddings) WITH (metric = 'ip')\")\n",
    "in_memory_db.sql(f\"SELECT doc_id, text_representation, metadata FROM in_memory_table ORDER BY array_inner_product(embeddings, {embedder.embed_query(q)}::FLOAT[384]) LIMIT 10;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: If you would like to remove your database after running the code above, please run this code cell\n",
    "try:\n",
    "    os.unlink(persistent_db)\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting {persistent_db}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
