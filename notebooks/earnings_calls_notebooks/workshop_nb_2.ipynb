{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b16fb706-6d55-45ec-9410-0916f3e4c989",
   "metadata": {},
   "source": [
    "# Workshop Notebook 2 - DocSets and Document Processing\n",
    "\n",
    "In this notebook, we will scale from one document to two, using sycamore to apply various forms of processing to each of them, in order to write them to a database\n",
    "and be able to answer questions like:\n",
    "\n",
    "0. In the Broadcom earnings call, what details did the CEO, Hock Tan, discuss about the VMware acqusition?\n",
    "\n",
    "## Sycamore basics\n",
    "\n",
    "By now you have a basic sense of the data model - a Document is made up of Elements which represent logical chunks of the Document, and contain additional metadata about themselves.\n",
    "The next step is to scale this past one document to many, and this is where Sycamore comes in. Sycamore adds a data structure called a DocSet, which is a set of Documents.\n",
    "Each Document in the DocSet contains the list of Elements that it comprises, and a bunch of metadata as well (for instance, the name of the file the document came from).\n",
    "\n",
    "Now you'll likely want to apply a series of transformations to the Documents before you write them to a database. You can imagine writing a big for loop over all the documents and\n",
    "calling a series of functions on them in order. Maybe you throw `multiprocessing` at it to parallelize it. Maybe you run nested loops to do some sort of batching. You have to do a \n",
    "lot of work to optimize it, and you still probably aren't using memory as efficiently as you could be. \n",
    "\n",
    "DocSets make processing large amounts of documents easy. DocSet methods are mostly processing steps to be applied to every document in the DocSet - so instead of writing\n",
    "```python\n",
    "# without docsets\n",
    "processed_documents = []\n",
    "for document in list_of_documents:\n",
    "    processed_documents.append(foo(document))\n",
    "```\n",
    "You can write\n",
    "```python\n",
    "# with docsets\n",
    "processed_docset = docset.map(foo)\n",
    "```\n",
    "\n",
    "### Execution modes\n",
    "\n",
    "Each docset is bound to a Sycamore Context, which is the execution engine that actually executes\n",
    "the processing steps. We've implemented 2 execution modes, `LOCAL` and `RAY`. `RAY` mode executes \n",
    "the DocSet on a [ray](https://www.ray.io/) cluster, creating one locally if it does not find an \n",
    "existing ray cluster. This mode scales well, running transforms on Documents in parallel across \n",
    "processes (and nodes if you've set it up), but it can be tricky to debug - distributed stack traces \n",
    "are notoriously unwieldy. `LOCAL` mode runs in single-threaded python in the process and is generally\n",
    "better for debugging, but you lose the distributed/parallel aspect. For the beginning of the workshop,\n",
    "we will run in `LOCAL` mode, and then transition to `RAY` when we have ironed out the DocSet plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a708cb9c-04b1-4bcb-a733-7144888875e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a patch to allow sycamore to make asynchronous llm calls\n",
    "# in local mode within a jupyter notebook.\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a44bf0d-02b7-4f86-b902-1754af0925c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sycamore\n",
    "from sycamore import ExecMode\n",
    "\n",
    "context = sycamore.init(exec_mode = ExecMode.LOCAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7175b1-deef-4cc9-872b-f1a15f00d2e4",
   "metadata": {},
   "source": [
    "To create the DocSet, we need to tell sycamore how to read in the initial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002de04a-fa2a-4780-a75e-76bfc44dc560",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "repo_root = Path.cwd()\n",
    "pdf_dir = repo_root / \"files\" / \"earnings_calls\"\n",
    "two_pdfs = [str(pdf_dir / \"broadcom-avgo-q1-2024-earnings-call-transcript.pdf\"), str(pdf_dir / \"mongodb-mdb-q1-2024-earnings-call-transcript.pdf\")]\n",
    "\n",
    "pdf_docset = context.read.binary(paths=two_pdfs, binary_format=\"pdf\")\n",
    "\n",
    "# Let's see what that gave us\n",
    "pdf_docset.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50243b3f-53b7-4545-9be7-36fe02340ec9",
   "metadata": {},
   "source": [
    "Our docset has two Documents in it, with a 'properties' dict containing some metadata, an 'elements' list containing an empty list of elements, a doc_id, lineage_id, type, and binary_representation, which contains the binary of the original PDF.\n",
    "To get the elements as before, we'll want to run the `partition` transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2be01ac-5898-42a0-b911-f3a0895c08a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sycamore.transforms.partition import ArynPartitioner\n",
    "\n",
    "# If you did not see the error message about API keys, ignore this comment.\n",
    "# You might need to add aryn_api_key=\"<YOUR KEY>\" if the environment didn't pick it up correctly. \n",
    "partitioned_docset = pdf_docset.partition(ArynPartitioner())\n",
    "\n",
    "# We'll limit the number of elements to show because otherwise this produces an obnoxiously large output cell\n",
    "partitioned_docset.show(num_elements=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dfe94d-5a21-403e-8017-54125dac278e",
   "metadata": {},
   "source": [
    "We can visualize bounding boxes in much the same way that we did with aryn_sdk, with sycamore. Note that this will re-partition the documents. This is an intentional design choice within sycamore, as \n",
    "trying to hold an entire docset in memory at once doesn't necessarily scale; so the alternative is \n",
    "'lazy execution' - re-executing all the processing jobs. We'll show you how to optimize this in a few\n",
    "cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b49c688-4c5d-4050-907f-e5952a4515a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sycamore.utils.pdf_utils import show_pages\n",
    "\n",
    "show_pages(partitioned_docset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbeee31-e40e-4372-bbdc-7c2675bb0386",
   "metadata": {},
   "source": [
    "Wait a second.\n",
    "\n",
    "Running `showPages` and `show` ran the whole program all over again! This could get really cumbersome to work with, especially as we add additional transforms to our processing\n",
    "pipeline in development. I have a solution for you: `materialize`. But first, a diversion on lazy execution.\n",
    "\n",
    "DocSets are evaluated lazily, which means that as you're developing, the only thing held in memory in the DocSet object itself is an execution plan. To get the data in the DocSet,\n",
    "you have to 'execute' it - i.e. tell sycamore to run all the steps in the execution plan, from reading in the data to each transform. This allows sycamore to apply these sorts of \n",
    "parallelization/batch/streaming optimizations without you having to think about them. However, it comes with a drawback - accessing the documents themselves for ad-hoc inspection\n",
    "can be a little bit difficult. For example, DocSets do not provide random access to data.\n",
    "\n",
    "I often find it easier to think about a DocSet as a program than as a data structure.\n",
    "\n",
    "In order to execute a DocSet, there are a couple of methods that do that. \n",
    "\n",
    "- `docset.execute()` executes the docset and does nothing with the resulting Documents. Most production pipelines use this to run.\n",
    "- `docset.take_all()` (and its friend `docset.take(n)`) executes the docset and returns the Documents in a plain python list. This is useful for debugging and development, when datasets are still small.\n",
    "- `docset.count()` executes the docset and returns the number of Documents in it. This is most useful when debugging filters (map transforms don't change the size of the docset).\n",
    "- `docset.show()` executes and prints the first couple Documents - good for development\n",
    "- `docset.write.<some_target>()` executes the docset and writes the documents out to some target - could be a database like opensearch, or just the filesystem. Most of these writers have an `execute` flag that determines whether to execute the write (and return nothing) or just return a DocSet with the write in the plan.\n",
    "\n",
    "### Materialize & Memoization\n",
    "\n",
    "There's a technique for optimizing recursive functions called memoization - essentially, the first time\n",
    "you call the function with a given set of parameters, compute the result and cache it. Then, in all \n",
    "subsequent calls, simply look up the pre-computed result. Sycamore can do a similar thing with \n",
    "`docset.materialize()`, using the disk as a cache.\n",
    "\n",
    "When sycamore compiles a DocSet into an execution plan, it starts from the end and works toward the\n",
    "beginning. When it sees a `materialize`, it looks in the location where the `materialize` thinks its\n",
    "cache lives, and if it finds data, it finishes compiling and reads the data in from the cache location, \n",
    "essentially truncating the docset program to only the stuff after the `materialize`. However if it does\n",
    "not find data in its cache, it adds a step to the program to write data _to_ the cache and continues \n",
    "compiling.\n",
    "\n",
    "Code-wise, the `materialize` method takes two parameters: a path to the cache, which can be in the local\n",
    "filesystem or S3, and a `MaterializeSourceMode`, which is an enum with 2 values: `RECOMPUTE` and \n",
    "`USE_STORED`. `RECOMPUTE` tells the materialize not to act as a cache, but to always write out the data.\n",
    "This is more useful for debugging. `USE_STORED` tells materialize to act as a cache and do the memoize\n",
    "thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c2ecd9-e148-4dbb-97e4-ae4e2d90bab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sycamore.materialize import MaterializeSourceMode\n",
    "\n",
    "materialize_dir = repo_root / \"materialize\"\n",
    "\n",
    "materialized_ds = partitioned_docset.materialize(path = materialize_dir / \"twodocs-partitioned\", source_mode = MaterializeSourceMode.USE_STORED)\n",
    "\n",
    "materialized_ds.execute()\n",
    "print(\"Finished executing the first time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7807d9-4436-4d13-8779-4d1d15995a93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Note that the second time this is fast\n",
    "materialized_ds.execute()\n",
    "print(\"Finished executing the second time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544ac101-5169-4a0c-b27f-122e80be0141",
   "metadata": {},
   "source": [
    "## Sycamore UDFs\n",
    "\n",
    "Since we downloaded our documents for free from the internet, we've ended up with some advertisments\n",
    "in them. Inspecting the elements and their types we can clean them up mostly by throwing out images.\n",
    "For other workloads this probably doesn't apply, but here it provides a lovely opportunity to demonstrate one of the four most useful docset udf-transforms, `filter_elements`. Here's the list of udf transforms:\n",
    "\n",
    "- `docset.map(f)`: Applies a function (`Document` -> `Document`) to every Document in the DocSet\n",
    "- `docset.map_elements(f)`: Applies a function (`Element` -> `Element`) to every Element in every Document in the DocSet\n",
    "- `docset.filter(f)`: Applies a predicate function (`Document` -> `bool`) to every Document, keeping only those Documents for which f(Document) is True\n",
    "- `docset.filter_elements(f)`: Applies a predicate function (`Element` -> `bool`) to every Element in every Document, keeping only Elements for which f(Element) is True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173dfbdf-48c2-4004-bbcd-ae8097ac870e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sycamore.data import Element\n",
    "\n",
    "def kill_images(elt: Element) -> bool:\n",
    "    return elt.type != \"Image\"\n",
    "\n",
    "# docset.filter_elements takes a predicate function that maps Elements to bools. \n",
    "# For each element in a document, keep the element only if predicate(element) is True.\n",
    "filtered_ds = materialized_ds.filter_elements(kill_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea954b6-62f9-4d1e-8fd6-b27e26a28579",
   "metadata": {},
   "source": [
    "Sometimes, you'll want to redo a step that's been materialized. The simplest option is to remove the directory with all the cached data, e.g. `rm -rd materialize/twodocs-partitioned`\n",
    "\n",
    "### Debugging\n",
    "\n",
    "Debugging distributed systems can be tricky, but it's possible with a little bit of creativity. First-\n",
    "off, the execution-forcing methods above are useful - particularly `take` and `take_all` since they \n",
    "give you back the Documents. Materializing a docset lets the documents persist, which can allow\n",
    "dedicated debugging scripts and even sharing. Printing data can be a little hard to find as the log \n",
    "streams tend to get fairly polluted with stuff, so I will sometimes simply write a function that writes\n",
    "a piece of data for every Document/Element to a file and apply it with a `map` or `map_elements` like so:\n",
    "\n",
    "```python\n",
    "def debug_doc(doc):\n",
    "    with open(\"debug.txt\", \"a\") as f:\n",
    "        f.write(f\"Document {doc.doc_id}\\n\")\n",
    "        f.write(json.dumps(doc.elements, indent=2))\n",
    "        f.write(\"\\n\" + '-' * 80 + \"\\n\")\n",
    "    return doc\n",
    "\n",
    "docset.map(debug_doc)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c0f69d-6bfe-46aa-b283-f2d60ddb9d18",
   "metadata": {},
   "source": [
    "## Schema Extraction\n",
    "\n",
    "Ok, let's go back to the question we were trying to answer:  \"In the Broadcom earnings call, what details did the CEO, Hock Tan, discuss about the VMware acquisition?\" Now notice that we have two documents loaded into our DocSet, a Broadcom earnings call and a MongoDB earnings call. To successfully answer this question we'll have to perform the following steps:\n",
    "\n",
    "1. Identify the Broadcom document\n",
    "2. Identify the elements where Hock Tan is speaking\n",
    "3. Identify the element where he mentions VMWare. \n",
    "\n",
    "Now notice that the first step requires identifying the Broadcom document. A reasonable way to accomplish this is to extract the company from each document. Then we can filter the \n",
    "documents like we did the elements.\n",
    "\n",
    "One of sycamore's biggest benefits is its ability to interact with LLMs in this kind of data-flow-y way. LLMs are good at understanding unstructured data, so for processing unstructured\n",
    "documents, they're a very useful tool. They make it easy to extract common metadata properties from documents, and with sycamore we can very easily apply this to all documents in a docset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04bb621-40e7-4a27-8997-2cf697c72ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sycamore.llms.openai import OpenAI, OpenAIModels\n",
    "from sycamore.llms.llms import LLMMode\n",
    "from sycamore.transforms.extract_schema import LLMPropertyExtractor\n",
    "\n",
    "# You might need to explicitly set an api key here if it's not picked up from the environment variables\n",
    "# Add parameter: api_key = \"<key>\"\n",
    "gpt4o = OpenAI(OpenAIModels.GPT_4O)\n",
    "\n",
    "schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"quarter\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Quarter of the earnings call, it should be in the format of Q1, Q2, Q3, Q4\",\n",
    "        },\n",
    "        \"date\":{\"type\": \"string\", \"description\": \"The date of the earnings call\"}\n",
    "    },\n",
    "}\n",
    "\n",
    "# Quiz: As is, this property extraction will never run, even if I do something \n",
    "#       like `materialized_ds.execute()`. Why?\n",
    "#       Hint: Compare to how we're adding transforms to the docset in other places.\n",
    "filtered_ds.extract_properties(LLMPropertyExtractor(llm=gpt4o, schema=schema))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f2f735-64be-40de-a53a-ddf4e67d7423",
   "metadata": {},
   "source": [
    "Now see if you can add a `company_name` and `company_ticker` property to this schema and extract properties into a docset named `extracted_ds`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad19c277-27e5-48cc-b97d-0de81ec7a20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"quarter\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Quarter of the earnings call, it should be in the format of Q1, Q2, Q3, Q4\",\n",
    "        },\n",
    "        \"date\":{\"type\": \"string\", \"description\": \"The date of the earnings call\"},\n",
    "\n",
    "\n",
    "#        ... # Fill in the rest!\n",
    "\n",
    "extracted_ds = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41a05fa-e102-4b1d-9849-3b537facd925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that the schema is right. We'll reference these properties later.\n",
    "for doc in extracted_ds.take(1):\n",
    "    print(doc.properties)\n",
    "    assert 'entity' in doc.properties\n",
    "    ec = doc.properties['entity']\n",
    "    assert 'date' in ec\n",
    "    assert 'quarter' in ec\n",
    "    assert 'company_name' in ec\n",
    "    assert 'company_ticker' in ec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f7379f-03e6-4c35-bedf-0a96b8e03f16",
   "metadata": {},
   "source": [
    "Great! Now is there any optimization you can add to memoize the results of the LLM calls so that future docset executions can skip it?\n",
    "\n",
    "## Chunking\n",
    "\n",
    "Now, for our question answering system to be able to detect that this is element where Hock Tan discusses the VMWare acquistion, we'll need a way to associate the \"speaker element\" that is a few paragraphs above it, with this last element. The way to do that is through chunking. Sycamore implements a number of chunking strategies (documentation [here](https://sycamore.readthedocs.io/en/stable/sycamore/APIs/low_level_transforms/merge_elements.html)). \n",
    "For this workshop we will use the `MarkedMerger` as it is the most customizable.\n",
    "\n",
    "So, to be able to answer questions like the one about Hock Tan we'll chunk such that for each speaker 'block' we  get a chunk. In our partitioning we have split the text into paragraphs, but we'd like to squish all those paragraphs together, breaking the blocks wherever there's a new speaker. With a little bit of effort we can detect the lines that introduce speakers with regexes - one for external speakers and one for internal speakers, as the formatting is very consistent (this applies across all the documents in the dataset, don't worry):\n",
    "\n",
    "```python\n",
    "external_re = '([^ ]*[^\\S\\n\\t]){1,4}--[^\\S\\n\\t].*--' # A name (1-4 words long) followed by -- followed by anything followed by --\n",
    "internal_re = '([^ ]*[^\\S\\n\\t]){1,4}--.*'            # A name (1-4 words long) followed by -- followed by anything\n",
    "```\n",
    "\n",
    "We'll also add a condition to that the 'speaker' chunks be one line: occasionally we get a paragraph \n",
    "where the speaker kinda stutters the beginning of their speech which gets transcribed as a '--' and can\n",
    "trip up the regex.\n",
    "\n",
    "The `MarkedMerger` is set up perfectly to work with this. It will step through the elements, merging them together one by one, unless it sees one of two 'marks' in the data:\n",
    "\n",
    "- on a \"_drop\" mark it drops the element and continues merging\n",
    "- on a \"_break\" mark it finalizes the merged element and uses this one to start merging up a new element\n",
    "\n",
    "In the following cell, the first case (when the speaker is \"Operator\") has been left as an exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1137a8c-936a-4748-96d3-a84d548e4478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sycamore.transforms.merge_elements import MarkedMerger\n",
    "\n",
    "def mark_speakers(elt: Element) -> Element:\n",
    "    if not elt.text_representation:\n",
    "        return elt\n",
    "\n",
    "    external_speaker = re.match('([^ ]*[^\\S\\n\\t]){1,4}--[^\\S\\n\\t].*--', elt.text_representation)\n",
    "    internal_speaker = re.match('([^ ]*[^\\S\\n\\t]){1,4}--.*', elt.text_representation)\n",
    "    is_one_line = elt.text_representation.count(\"\\n\") <= 1\n",
    "    if elt.text_representation.strip() == 'Operator':\n",
    "        # The operator is also a speaker! In this case, we should set\n",
    "        # the 'speaker' property to True and the 'speaker_role' and \n",
    "        # 'speaker_name' properties to the string 'Operator'. We should \n",
    "        # also tell the MarkedMerger to break.\n",
    "        raise NotImplementedError(\"I thought operators were an algebra thing!\")\n",
    "    elif external_speaker and is_one_line:\n",
    "        parts = [p.strip() for p in elt.text_representation.split(\"--\")]\n",
    "        elt.properties['speaker_name'] = parts[0]\n",
    "        elt.properties['speaker_external_org'] = parts[1]\n",
    "        elt.properties['speaker_role'] = parts[2]\n",
    "        elt.properties['speaker'] = True\n",
    "        elt.data[\"_break\"] = True\n",
    "    elif internal_speaker and is_one_line:\n",
    "        location = elt.text_representation.find('--')\n",
    "        parts = [p.strip() for p in elt.text_representation.split(\"--\")]\n",
    "        elt.properties['speaker_name'] = parts[0]\n",
    "        elt.properties['speaker_role'] = parts[1]\n",
    "        elt.properties['speaker'] = True\n",
    "        elt.data[\"_break\"] = True\n",
    "    return elt\n",
    "\n",
    "# Also here's a nice way of writing chained pipelines\n",
    "merged_ds = (\n",
    "    extracted_ds\n",
    "    .map_elements(mark_speakers)\n",
    "    .merge(MarkedMerger())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf77c2d-140b-459c-86a0-40be0f5c39e8",
   "metadata": {},
   "source": [
    "## Initial Question Answering\n",
    "\n",
    "Now we should be able to get the data requisite to answer our first question, even without a database \n",
    "behind it. With just a bunch of filters we are able to narrow down the docset to exectly the one \n",
    "document that answers:\n",
    "\n",
    "0. In the Broadcom earnings call, what details did the CEO, Hock Tan, discuss about the VMware acqusition?\n",
    "\n",
    "To answer this question we can do the following:\n",
    "\n",
    "1. Identify the Broadcom document\n",
    "2. Identify the elements where Hock Tan is speaking\n",
    "3. Identify the element where he mentions VMWare.\n",
    "\n",
    "We can translate this into a series of sycamore filters like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c070a3-724d-4245-937e-19bcb041d07c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "broadcom_qads = (\n",
    "    merged_ds\n",
    "    .filter(lambda doc: doc.properties['entity']['company_ticker'] == 'AVGO')\n",
    "    .filter_elements(lambda elt: elt.properties.get('speaker_name') == 'Hock Tan')\n",
    "    .filter_elements(lambda elt: \"vmware\" in elt.text_representation.lower())\n",
    ")\n",
    "\n",
    "documents = broadcom_qads.take_all()\n",
    "# I happen to know that there is only one broadcom document (of the two documents in the docset)\n",
    "assert len(documents) == 1\n",
    "doc = documents[0]\n",
    "print(doc.properties)\n",
    "for e in doc.elements:\n",
    "    print(e.properties)\n",
    "    print(e.text_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6670d135-327c-48b7-bd4c-992caff863e3",
   "metadata": {},
   "source": [
    "We can be confident that these are all the places where VMWare came up in a Broadcom earnings call by Hock Tan. If we wanted a more concise answer we would probably just ask chatGPT to summarize the\n",
    "results.\n",
    "\n",
    "Now let's try to answer another question on mongodb in a similar way.\n",
    "\n",
    "2. What did the MongoDB president mention about their competitor Amazon DynamoDB?\n",
    "\n",
    "We'll use a similar sort of plan:\n",
    "\n",
    "1. Filter to the mongodb document (stock ticker = \"MDB\")\n",
    "2. Filter to elements where the speaker role contains \"President\"\n",
    "3. Filter to an element containing 'DynamoDB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0293d5a-0268-4255-9922-2b1c146a7a61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mongodb_qads = (\n",
    "    merged_ds\n",
    "    # Fill it out yo'self\n",
    ")\n",
    "\n",
    "documents = mongodb_qads.take_all()\n",
    "# I happen to know that there is only one MDB document (of the two documents in the docset)\n",
    "doc = documents[0]\n",
    "print(doc.properties)\n",
    "for e in doc.elements:\n",
    "    print(e.properties)\n",
    "    print(e.text_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494e9e0f-7f7d-43ba-90af-5580a6bb71a1",
   "metadata": {},
   "source": [
    "I'll admit that this may look stupid. Why wouldn't we just write all the documents to a database and then do the search that way?\n",
    "Well, yes, we'll enable that next. But we'll come back to this docset-based strategy for question-answering, as it allows you to \n",
    "answer almost arbitrarily complex questions that a search database may not support.\n",
    "\n",
    "Now let's enable approximate search:\n",
    "\n",
    "## Embedding\n",
    "\n",
    "In order to do that, we'll need to write our docset to a database, and embed the text of our elements to use k-nearest-neighbor vector \n",
    "search to retrieve relevant chunks for an LLM to summarize.\n",
    "\n",
    "Embedding data with sycamore is fairly simple, so I'm going to give you all the information you need to do it and let you write it out.\n",
    "There is a method on DocSets called `embed()`. It takes an `Embedder` as its parameter. We'll use the `OpenAIEmbedder`, which you can import from `sycamore.transforms.embed`. It takes a `model_name` parameter\n",
    "but we'll use the default. This will embed the text_representation of all elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bffcfe-6a17-46f2-b55b-5515d88c9257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "from ... import ...\n",
    "\n",
    "embedded_ds = merged_ds..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d50874-867c-4256-aa16-22acac62c8eb",
   "metadata": {},
   "source": [
    "## Ingestion\n",
    "\n",
    "We'll be writing our data to Aryn (because what kind of workshop would this be if we didn't stand behind our own data warehouse). Sycamore can\n",
    "also write to a number of other systems, such as OpenSearch, ElasticSearch, Weaviate, etc. \n",
    "\n",
    "The unit of storage in Aryn equivalent to an index in OpenSearch or a table in a SQL DB is a 'DocSet.' While a Sycamore DocSet is usually best \n",
    "understood as a program, an Aryn DocSet is actually a container. We can create one using aryn_sdk, and then write our (sycamore) docset to it.\n",
    "\n",
    "First I'll add in a `spread_properties` transform, which copies properties from every Document to each of its Elements, so that all the elements have\n",
    "the `entity` and `path` metadata associated with the document. This will help my queries work properly: Aryn stores elements in OpenSearch under the hood, so when filtering on a particular property,\n",
    "the elements all need to have that property in them, so I'll be able to filter by `entity.company_ticker = AVGO` and get the elements back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b60b36-498e-40d3-a6c0-89dab05203f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spread_ds = embedded_ds.spread_properties(['path', 'entity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a290188d-da80-4642-9b41-056b7509e4fe",
   "metadata": {},
   "source": [
    "Now let's create our docset target (give it a name) and write to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3831d1-f191-4ad4-970e-91c557cb7360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aryn_sdk.client.client import Client\n",
    "\n",
    "# You may need to specify aryn_api_key=\"<YOUR KEY>\" here\n",
    "aryn_client = Client()\n",
    "\n",
    "docset_name = \"yo\"\n",
    "aryn_docset = aryn_client.create_docset(name = docset_name)\n",
    "\n",
    "print(aryn_docset.value.docset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5438ca2a-a544-41c9-8037-bdc34b74e828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may need to specify aryn_api_key=\"<YOUR KEY>\" here too.\n",
    "spread_ds.write.aryn(docset_id=aryn_docset.value.docset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6332e92b-26d3-4542-8220-b8c9a2cfafff",
   "metadata": {},
   "source": [
    "Awesome! Now if you navigate to the [Aryn console](https://app.aryn.ai/storage) you should see your docset and the documents inside it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb609f2-eb4c-4dbd-9bec-a6ece305e879",
   "metadata": {},
   "source": [
    "## Full ingestion script\n",
    "\n",
    "Now let's scale our script. For consistency purposes, I've gone and written out a canonical form for \n",
    "it, and condensed it a little. I've also partitioned the documents ahead of time, as this can take\n",
    "a little while (our DOS protection is to limit the concurrent requests in an account) - so this script \n",
    "will get them from a materialize. This is one of the things you downloaded with `make downloads`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a109d05-89d1-45f0-af92-b2737ff3a7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I guess this is sort of a cheat sheet for the rest of the notebook. That's ok.\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "import sycamore\n",
    "from sycamore import MaterializeSourceMode\n",
    "from sycamore.data import Element\n",
    "from sycamore.llms.openai import OpenAI, OpenAIModels\n",
    "from sycamore.transforms.partition import ArynPartitioner\n",
    "from sycamore.transforms.extract_schema import LLMPropertyExtractor\n",
    "from sycamore.transforms.merge_elements import MarkedMerger\n",
    "from sycamore.transforms.embed import OpenAIEmbedder\n",
    "from aryn_sdk.client.client import Client\n",
    "\n",
    "repo_root = Path.cwd()\n",
    "pdf_dir = repo_root / \"files\" / \"earnings_calls\"\n",
    "materialize_dir = repo_root / \"materialize\"\n",
    "\n",
    "gpt4o = OpenAI(OpenAIModels.GPT_4O)\n",
    "\n",
    "schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"quarter\": {\"type\": \"string\", \"description\": \"Quarter of the earnings call, it should be in the format of Q1, Q2, Q3, Q4\"},\n",
    "        \"date\":{\"type\": \"string\", \"description\": \"The date of the earnings call\"},\n",
    "        \"company_name\": {\"type\": \"string\", \"description\": \"The name of the company in the earnings call\"},\n",
    "        \"company_ticker\": {\"type\": \"string\", \"description\": \"The stock ticker of the company in the earnings call\"},\n",
    "    }\n",
    "}\n",
    "\n",
    "def mark_speakers(elt: Element) -> Element:\n",
    "    if not elt.text_representation:\n",
    "        return elt\n",
    "\n",
    "    external_speaker = re.match('([^ ]*[^\\S\\n\\t]){1,4}--[^\\S\\n\\t].*--', elt.text_representation)\n",
    "    internal_speaker = re.match('([^ ]*[^\\S\\n\\t]){1,4}--.*', elt.text_representation)\n",
    "    is_one_line = elt.text_representation.count(\"\\n\") <= 1\n",
    "    if elt.text_representation.strip() == 'Operator':\n",
    "        elt.properties[\"speaker_name\"] = \"Operator\"\n",
    "        elt.properties[\"speaker_role\"] = \"Operator\"\n",
    "        elt.properties[\"speaker\"] = True\n",
    "        elt.data[\"_break\"] = True\n",
    "    elif external_speaker and is_one_line:\n",
    "        parts = [p.strip() for p in elt.text_representation.split(\"--\")]\n",
    "        elt.properties['speaker_name'] = parts[0]\n",
    "        elt.properties['speaker_external_org'] = parts[1]\n",
    "        elt.properties['speaker_role'] = parts[2]\n",
    "        elt.properties['speaker'] = True\n",
    "        elt.data[\"_break\"] = True\n",
    "    elif internal_speaker and is_one_line:\n",
    "        location = elt.text_representation.find('--')\n",
    "        parts = [p.strip() for p in elt.text_representation.split(\"--\")]\n",
    "        elt.properties['speaker_name'] = parts[0]\n",
    "        elt.properties['speaker_role'] = parts[1]\n",
    "        elt.properties['speaker'] = True\n",
    "        elt.data[\"_break\"] = True\n",
    "    return elt\n",
    "\n",
    "aryn_client = Client()\n",
    "aryn_docset = aryn_client.create_docset(name = \"haystack-workshop-all\")\n",
    "docset_id = aryn_docset.value.docset_id\n",
    "\n",
    "# Now we'll use ray (which is the default exec_mode)\n",
    "ctx = sycamore.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c883c588-9cf0-459e-8995-f39fd760fd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    ctx.read.binary(paths = str(pdf_dir), binary_format = \"pdf\")\n",
    "    .partition(ArynPartitioner())\n",
    "    .materialize(path=materialize_dir / \"alldocs-partitioned\", source_mode=MaterializeSourceMode.USE_STORED)\n",
    "    .filter_elements(lambda e: e.type != \"Image\")\n",
    "    .extract_properties(LLMPropertyExtractor(llm=gpt4o, schema=schema))\n",
    "    .map_elements(mark_speakers)\n",
    "    .merge(MarkedMerger())\n",
    "    .embed(OpenAIEmbedder())\n",
    "    .spread_properties(['path', 'entity'])\n",
    "    .write.aryn(docset_id=docset_id)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18668868-8df8-4661-af65-ad553ea6a1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the docset id to a file to pick up in the next notebooks:\n",
    "with open(\"docset_id\", \"w\") as f:\n",
    "    f.write(docset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ce1b39-5ee5-4edf-b8e9-a770cf832d33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
