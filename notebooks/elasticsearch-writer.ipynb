{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Aryn Partitioner in this job is configured to use the Aryn Partitioning Service to provide fast, GPU-powered performance. Go to [aryn.ai/sign-up ](aryn.ai/sign-up) to get a free API key for the service. This is the recommended configuration.\n",
    "\n",
    "You can also run the Aryn Partitioner locally by changing `use_partitioning_service` to `False`. Though you can use CPU to run the Aryn Partitioner, it is recommended to use an NVIDIA GPU for good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.fs\n",
    "from ray.data import ActorPoolStrategy\n",
    "import sycamore\n",
    "from elasticsearch import Elasticsearch\n",
    "from sycamore.functions.tokenizer import HuggingFaceTokenizer\n",
    "from sycamore.llms import OpenAIModels, OpenAI\n",
    "from sycamore.transforms import COALESCE_WHITESPACE\n",
    "from sycamore.transforms.merge_elements import MarkedMerger\n",
    "from sycamore.transforms.partition import ArynPartitioner\n",
    "from sycamore.transforms.extract_schema import OpenAISchemaExtractor, OpenAIPropertyExtractor\n",
    "from sycamore.transforms.embed import SentenceTransformerEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function used for cleaning and formatting later on in the notebook\n",
    "\n",
    "from sycamore.data.document import Document\n",
    "from dateutil import parser\n",
    "def convert_timestamp(doc: Document) -> Document:\n",
    "    if \"dateAndTime\" not in doc.properties['entity'] and \"dateTime\" not in doc.properties['entity']:\n",
    "        return doc\n",
    "    raw_date: str = doc.properties['entity'].get('dateAndTime') or doc.properties['entity'].get('dateTime')\n",
    "    raw_date = raw_date.replace(\"Local\", \"\")\n",
    "    parsed_date = parser.parse(raw_date, fuzzy=True)\n",
    "    extracted_date = parsed_date.date()\n",
    "    doc.properties['entity']['day'] = extracted_date.day\n",
    "    doc.properties['entity']['month'] = extracted_date.month\n",
    "    doc.properties['entity']['year'] = extracted_date.year\n",
    "    if parsed_date.utcoffset():\n",
    "        doc.properties['entity']['dateTime'] = parsed_date.isoformat()\n",
    "    else:\n",
    "        doc.properties['entity']['dateTime'] = parsed_date.isoformat() + \"Z\"\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "aryn_api_key = os.environ.get(\"ARYN_API_KEY\") ## REPLACE WITH YOUR-ARYN-API-KEY\n",
    "assert aryn_api_key is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"s3://aryn-public/NTSB/\"]\n",
    "fsys = pyarrow.fs.S3FileSystem(region=\"us-east-1\", anonymous=True)\n",
    "\n",
    "llm = OpenAI(OpenAIModels.GPT_3_5_TURBO.value)\n",
    "tokenizer = HuggingFaceTokenizer(\"thenlper/gte-small\")\n",
    "\n",
    "ctx = sycamore.init()\n",
    "\n",
    "ds = (\n",
    "    ctx.read.binary(paths, binary_format=\"pdf\", filesystem=fsys)\n",
    "    # Parition with the Aryn partitioner remotely, pulling out tables and images.\n",
    "    .partition(partitioner=ArynPartitioner(aryn_api_key=aryn_api_key, use_partitioning_service=True, use_cache=False, extract_images=True,  extract_table_structure=True))\n",
    "    # Get rid of spurious whitespace charaters\n",
    "    .regex_replace(COALESCE_WHITESPACE)\n",
    "    # Automatically determine a schema of additional metadata to extract from Documents\n",
    "    .extract_batch_schema(schema_extractor=OpenAISchemaExtractor(\"FlightAccidentReport\", llm=llm, num_of_elements=35))\n",
    "    # Extract the metadata specified by that schema\n",
    "    .extract_properties(property_extractor=OpenAIPropertyExtractor(llm=llm, num_of_elements=35))\n",
    "    # Merge elements into larger chunks\n",
    "    .mark_bbox_preset(tokenizer=tokenizer)\n",
    "    .merge(merger=MarkedMerger())\n",
    "    # Convert extracted timestamps to better-structured form using the function above\n",
    "    .map(convert_timestamp)\n",
    "    # Copy document properties to each Document's sub-elements\n",
    "    .spread_properties([\"path\", \"entity\"])\n",
    "    # Split elements that are too big to embed\n",
    "    .split_elements(tokenizer=tokenizer, max_tokens=512)\n",
    "    # Convert all Elements to Documents\n",
    "    .explode()\n",
    "    # Generate a series of hashes to represent each Document. For use with near-duplicate detection\n",
    "    .sketch()\n",
    "    # Embed each Document\n",
    "    .embed(embedder=SentenceTransformerEmbedder(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", batch_size=100))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-16 10:44:57,897\tWARNING util.py:560 -- The argument ``compute`` is deprecated in Ray 2.9. Please specify argument ``concurrency`` instead. For more information, see https://docs.ray.io/en/master/data/transforming-data.html#stateful-transforms.\n",
      "2024-07-16 10:44:57,898\tWARNING util.py:560 -- The argument ``compute`` is deprecated in Ray 2.9. Please specify argument ``concurrency`` instead. For more information, see https://docs.ray.io/en/master/data/transforming-data.html#stateful-transforms.\n",
      "2024-07-16 10:44:57,907\tINFO streaming_executor.py:112 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2024-07-15_17-37-44_746435_11753/logs/ray-data\n",
      "2024-07-16 10:44:57,908\tINFO streaming_executor.py:113 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadBinary] -> ActorPoolMapOperator[Map(BinaryScan._to_document)->MapBatches(BaseMapTransformCallable___wrap)] -> ActorPoolMapOperator[MapBatches(regex_replace)->MapBatches(BaseMapTransformCustom__Extract)] -> TaskPoolMapOperator[MapBatches(extract_properties)->MapBatches(sort_by_page_bbox)->MapBatches(mark_drop_tiny)->MapBatches(mark_drop_header_and_footer)->MapBatches(mark_break_page)->MapBatches(mark_break_by_column)->MapBatches(mark_break_by_tokens)->MapBatches(merge_elements)->MapBatches(convert_timestamp)->MapBatches(spread_properties)->MapBatches(split_doc)->MapBatches(explode)->MapBatches(sketcher)->MapBatches(SentenceTransformerEmbedder)->MapBatches(elastic_document_writer)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "839f46b69ac543128ef7c12d15d7b258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- ReadBinary->SplitBlocks(5) 1:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a873e41358344564806c64b8cee0a609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Map(BinaryScan._to_document)->MapBatches(BaseMapTransformCallable___wrap) 2:   0%|          | 0/5 [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a19e3140ab64271b76f5f89e54d58f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(regex_replace)->MapBatches(BaseMapTransformCustom__Extract) 3:   0%|          | 0/5 [00:00<?, ?it…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0b5bba4a2184160af79deb634b0cab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(extract_properties)->MapBatches(sort_by_page_bbox)->MapBatches(mark_drop_tiny)->MapBatches(mark_d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1b90881c3bc4d9aa4ba4a05f2a284fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Write to a persistent Elasticsearch Index. Note: You must have a specified elasticsearch instance running for this to work. For more information on how to set\n",
    "# one up, refer to https://www.elastic.co/guide/en/elasticsearch/reference/current/install-elasticsearch.html\n",
    "# - into a specific database (as specified by url) \n",
    "# - into a specific index (as specified by t)\n",
    "url = \"http://localhost:9201\"\n",
    "index_name = \"test_index\"\n",
    "ds.write.elasticsearch(url=url, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': '3b979cc72e9d', 'cluster_name': 'docker-cluster', 'cluster_uuid': 'rNXKhq5STYelYaU1xEN_rA', 'version': {'number': '8.14.2', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '2afe7caceec8a26ff53817e5ed88235e90592a1b', 'build_date': '2024-07-01T22:06:58.515911606Z', 'build_snapshot': False, 'lucene_version': '9.10.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'}\n"
     ]
    }
   ],
   "source": [
    "# Let's initialize the Elasticsearch client and connect it to the database to perform queries\n",
    "client = Elasticsearch(\n",
    "    url\n",
    ")\n",
    "\n",
    "print(client.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Airport: Pegasus McMurdo, Antarctica NZPG Runway Surface Type: Ice Airport Elevation: 18 ft msl Runway Used: 33 Runway Surface Condition: Ice IFR Approach: None Runway Length/Width: 10000 ft / 200 ft VFR Approach/Landing: Go around;Traffic pattern\n",
      "Wreckage and Impact Information\n",
      "\n",
      "--------------------------------------------------\n",
      "Meteorological Information and Flight Plan\n",
      "Conditions at Accident Site:\n",
      "Visual (VMC)\n",
      "Condition of Light:\n",
      "Day\n",
      "Observation Facility, Elevation:\n",
      "Observation Time:\n",
      "Distance from Accident Site:\n",
      "Direction from Accident Site:\n",
      "Lowest Cloud Condition:\n",
      "Visibility\n",
      "Lowest Ceiling:\n",
      "Visibility (RVR):\n",
      "Wind Speed/Gusts:\n",
      "9 knots /\n",
      "Turbulence Type Forecast/Actual:\n",
      "Wind Direction:\n",
      "360°\n",
      "Turbulence Severity Forecast/Actual:\n",
      "Altimeter Setting:\n",
      "Temperature/Dew Point:\n",
      "Precipitation and Obscuration:\n",
      "Departure Point:\n",
      "McMurdo (NZPG)\n",
      "Type of Flight Plan Filed:\n",
      "None\n",
      "Destination:\n",
      "McMurdo (NZPG)\n",
      "Type of Clearance:\n",
      "None\n",
      "Departure Time:\n",
      "Type of Airspace:\n",
      "Unknown\n",
      "Airport Information\n",
      "Airport:\n",
      "Pegasus McMurdo, Antarctica NZPG\n",
      "Runway Surface Type:\n",
      "Ice\n",
      "Airport Elevation:\n",
      "18 ft msl\n",
      "Runway Surface Condition:\n",
      "Ice\n",
      "Runway Used:\n",
      "33\n",
      "IFR Approach:\n",
      "None\n",
      "Runway Length/Width:\n",
      "10000 ft / 200 ft\n",
      "VFR Approach/Landing:\n",
      "Go around;Traffic pattern\n",
      "Wreckage and Impact Information\n",
      "Crew Injuries:\n",
      "Aircraft Damage:\n",
      "Substantial\n",
      "Passenger Injuries:\n",
      "Aircraft Fire:\n",
      "None\n",
      "Ground Injuries:\n",
      "N/A\n",
      "Aircraft Explosion:\n",
      "None\n",
      "Total Injuries:\n",
      "N/A\n",
      "Latitude, Longitude:\n",
      "77,166(est)\n",
      "\n",
      "--------------------------------------------------\n",
      "Meteorological Information and Flight Plan\n",
      "Conditions at Accident Site:\n",
      "Visual (VMC)\n",
      "Condition of Light:\n",
      "Day\n",
      "Observation Facility, Elevation:\n",
      "Observation Time:\n",
      "Distance from Accident Site:\n",
      "Direction from Accident Site:\n",
      "Lowest Cloud Condition:\n",
      "Visibility\n",
      "Lowest Ceiling:\n",
      "Visibility (RVR):\n",
      "Wind Speed/Gusts:\n",
      "9 knots /\n",
      "Turbulence Type Forecast/Actual:\n",
      "Wind Direction:\n",
      "360°\n",
      "Turbulence Severity Forecast/Actual:\n",
      "Altimeter Setting:\n",
      "Temperature/Dew Point:\n",
      "Precipitation and Obscuration:\n",
      "Departure Point:\n",
      "McMurdo (NZPG)\n",
      "Type of Flight Plan Filed:\n",
      "None\n",
      "Destination:\n",
      "McMurdo (NZPG)\n",
      "Type of Clearance:\n",
      "None\n",
      "Departure Time:\n",
      "Type of Airspace:\n",
      "Unknown\n",
      "Airport Information\n",
      "Airport:\n",
      "Pegasus McMurdo, Antarctica NZPG\n",
      "Runway Surface Type:\n",
      "Ice\n",
      "Airport Elevation:\n",
      "18 ft msl\n",
      "Runway Surface Condition:\n",
      "Ice\n",
      "Runway Used:\n",
      "33\n",
      "IFR Approach:\n",
      "None\n",
      "Runway Length/Width:\n",
      "10000 ft / 200 ft\n",
      "VFR Approach/Landing:\n",
      "Go around;Traffic pattern\n",
      "Wreckage and Impact Information\n",
      "Crew Injuries:\n",
      "Aircraft Damage:\n",
      "Substantial\n",
      "Passenger Injuries:\n",
      "Aircraft Fire:\n",
      "None\n",
      "Ground Injuries:\n",
      "N/A\n",
      "Aircraft Explosion:\n",
      "None\n",
      "Total Injuries:\n",
      "N/A\n",
      "Latitude, Longitude:\n",
      "77,166(est)\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Check for text search within Elasticsearch working\n",
    "\n",
    "result = client.search(index=index_name, body={\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"properties.text_representation\": \"traffic\"\n",
    "        }\n",
    "    },\n",
    "    \"_source\": [\"properties.text_representation\"],\n",
    "    \"size\": 5\n",
    "})\n",
    "\n",
    "# Process and print the results\n",
    "for hit in result['hits']['hits']:\n",
    "    text = hit['_source']['properties']['text_representation']\n",
    "    print(text)\n",
    "    print(\"-\" * 50)  # Separator between results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karansampath/Library/Caches/pypoetry/virtualenvs/sycamore-ai--zTjaFUY-py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# For queries, let's define an embedding function for the question that helps us easily run NN search \n",
    "# by comparing the two vectors and generating optimal results\n",
    "from sentence_transformers import SentenceTransformer\n",
    "minilm = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "from langchain_core.embeddings import Embeddings\n",
    "\n",
    "class Embedder(Embeddings):\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "    \n",
    "    def embed_query(self, question):\n",
    "        v = self.llm.encode(question).tolist()\n",
    "        return v\n",
    "    \n",
    "    def embed_documents(self, documents):\n",
    "        ans_list = [self.llm.encode(doc).tolist() for doc in documents]\n",
    "        return ans_list\n",
    "\n",
    "embedder = Embedder(minilm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ID: 65af5392-ca39-4da2-bb55-ef0ebc303989\n",
      "Summary: The National Transportation Safety Board determines the probable cause(s) of this accident to be:\n",
      "Page 1 of 5\n",
      "\n",
      "Score: 0.70915794\n",
      "\n",
      "ID: f98c5018-86ad-426e-9f57-22acb6aef49a\n",
      "Summary: The National Transportation Safety Board determines the probable cause(s) of this accident to be:\n",
      "Page 1 of 5\n",
      "\n",
      "Score: 0.70878434\n",
      "\n",
      "ID: e9cb97c7-c3df-42e7-b2d0-1aab568182db\n",
      "Summary: The National Transportation Safety Board determines the probable cause(s) of this accident to be:\n",
      "Page 1 of 5\n",
      "\n",
      "Score: 0.70878434\n",
      "\n",
      "ID: ff18a012-e3b0-43a2-9ced-6676d3132a26\n",
      "Summary: The National Transportation Safety Board determines the probable cause(s) of this accident to be: THE PILOT'S CONTINUED FLIGHT INTO ADVERSE WEATHER CONDITIONS. FACTORS WERE THE ICING CONDITIONS PREVAILING AT THE DESTINATION AIRPORT, AND THE PILOT'S INABILITY TO MAINTAIN VISUAL LOOKOUT DUE TO WINDSHIELD ICING.\n",
      "Page 1 of 7\n",
      "\n",
      "Score: 0.68322325\n",
      "\n",
      "ID: 0031397f-88af-4a61-b940-70113ee2be63\n",
      "Summary: The National Transportation Safety Board determines the probable cause(s) of this accident to be: THE PILOT'S CONTINUED FLIGHT INTO ADVERSE WEATHER CONDITIONS. FACTORS WERE THE ICING CONDITIONS PREVAILING AT THE DESTINATION AIRPORT, AND THE PILOT'S INABILITY TO MAINTAIN VISUAL LOOKOUT DUE TO WINDSHIELD ICING.\n",
      "Page 1 of 7\n",
      "\n",
      "Score: 0.68199754\n",
      "\n",
      "ID: 2adc9b9c-7ba7-4bb6-aa1b-847f78fcab95\n",
      "Summary: The National Transportation Safety Board determines the probable cause(s) of this accident to be: THE PILOT'S CONTINUED FLIGHT INTO ADVERSE WEATHER CONDITIONS. FACTORS WERE THE ICING CONDITIONS PREVAILING AT THE DESTINATION AIRPORT, AND THE PILOT'S INABILITY TO MAINTAIN VISUAL LOOKOUT DUE TO WINDSHIELD ICING.\n",
      "Page 1 of 7\n",
      "\n",
      "Score: 0.68199754\n",
      "\n",
      "ID: 59a856d2-b794-4474-b8c8-3931bff63a03\n",
      "Summary: National Transportation Safety Board Aviation Accident Final Report\n",
      "\n",
      "Score: 0.67625785\n",
      "\n",
      "ID: 85565baf-1995-4687-9454-71368274abfd\n",
      "Summary: Wreckage and Impact Information\n",
      "Crew Injuries:\n",
      "1 Minor, 3 None\n",
      "Aircraft Damage:\n",
      "Substantial\n",
      "Passenger Injuries:\n",
      "6 None\n",
      "Aircraft Fire:\n",
      "None\n",
      "Ground Injuries:\n",
      "N/A\n",
      "Aircraft Explosion:\n",
      "None\n",
      "Total Injuries:\n",
      "1 Minor, 9 None\n",
      "Latitude, Longitude:\n",
      "78.016944,-154.966949(est)\n",
      "Administrative Information\n",
      "Investigator In Charge (IIC):\n",
      "Johnson, Clinton\n",
      "Additional Participating Persons:\n",
      "Jon Lee; Transport Safety Board (TSB); Edmonton, Alberta John E Mills; US Department of the Interior; Bosie , ID\n",
      "Original Publish Date:\n",
      "March 5, 2009\n",
      "Note:\n",
      "Investigation Docket:\n",
      "https://data.ntsb.gov/Docket?ProjectID=67394\n",
      "The National Transportation Safety Board (NTSB), established in 1967, is an independent federal agency mandated by Congress through the Independent Safety Board Act of 1974 to investigate transportation accidents, determine the probable causes of the accidents, issue safety recommendations, study transportation safety issues, and evaluate the safety effectiveness of government agencies involved in transportation. The NTSB makes public its actions and decisions through accident reports, safety studies, special investigation reports, safety recommendations, and statistical reviews.\n",
      "The Independent Safety Board Act, as codified at 49 U.S.C. Section 1154(b), precludes the admission into evidence or use of any part of an NTSB report related to an incident or accident in a civil action for damages resulting from a matter mentioned in the report. A factual report that may be admissible under 49 U.S.C. § 1154(b) is available here.\n",
      "\n",
      "Score: 0.6648934\n",
      "\n",
      "ID: 32b9b9f2-d4e4-4b77-afc1-29e72c781933\n",
      "Summary: Wreckage and Impact Information\n",
      "Crew Injuries:\n",
      "1 Minor, 3 None\n",
      "Aircraft Damage:\n",
      "Substantial\n",
      "Passenger Injuries:\n",
      "6 None\n",
      "Aircraft Fire:\n",
      "None\n",
      "Ground Injuries:\n",
      "N/A\n",
      "Aircraft Explosion:\n",
      "None\n",
      "Total Injuries:\n",
      "1 Minor, 9 None\n",
      "Latitude, Longitude:\n",
      "78.016944,-154.966949(est)\n",
      "Administrative Information\n",
      "Investigator In Charge (IIC):\n",
      "Johnson, Clinton\n",
      "Additional Participating Persons:\n",
      "Jon Lee; Transport Safety Board (TSB); Edmonton, Alberta John E Mills; US Department of the Interior; Bosie , ID\n",
      "Original Publish Date:\n",
      "March 5, 2009\n",
      "Note:\n",
      "Investigation Docket:\n",
      "https://data.ntsb.gov/Docket?ProjectID=67394\n",
      "The National Transportation Safety Board (NTSB), established in 1967, is an independent federal agency mandated by Congress through the Independent Safety Board Act of 1974 to investigate transportation accidents, determine the probable causes of the accidents, issue safety recommendations, study transportation safety issues, and evaluate the safety effectiveness of government agencies involved in transportation. The NTSB makes public its actions and decisions through accident reports, safety studies, special investigation reports, safety recommendations, and statistical reviews.\n",
      "The Independent Safety Board Act, as codified at 49 U.S.C. Section 1154(b), precludes the admission into evidence or use of any part of an NTSB report related to an incident or accident in a civil action for damages resulting from a matter mentioned in the report. A factual report that may be admissible under 49 U.S.C. § 1154(b) is available here.\n",
      "\n",
      "Score: 0.6648934\n",
      "\n",
      "ID: e7e5c25f-bed1-4ab5-9ced-466f22ad2250\n",
      "Summary: The National Transportation Safety Board (NTSB), established in 1967, is an independent federal agency mandated by Congress through the Independent Safety Board Act of 1974 to investigate transportation accidents, determine the probable causes of the accidents, issue safety recommendations, study transportation safety issues, and evaluate the safety effectiveness of government agencies involved in transportation. The NTSB makes public its actions and decisions through accident reports, safety studies, special investigation reports, safety recommendations, and statistical reviews.\n",
      "The Independent Safety Board Act, as codified at 49 U.S.C. Section 1154(b), precludes the admission into evidence or use of any part of an NTSB report related to an incident or accident in a civil action for damages resulting from a matter mentioned in the report.\n",
      "FTW95FA129\n",
      "Page 7 of 7\n",
      "\n",
      "Score: 0.6611712\n"
     ]
    }
   ],
   "source": [
    "# Let us now run NN queries using Elasticsearch. First, we define a small function to pretty print the results and define an embedding function \n",
    "# for the question that helps us easily run NN search by comparing the two vectors and generating optimal results\n",
    "\n",
    "def pretty_response(response):\n",
    "    for hit in response['hits']['hits']:\n",
    "        id = hit['_id']\n",
    "        score = hit['_score']\n",
    "        text = hit['_source']['properties']['text_representation']\n",
    "        pretty_output = (f\"\\nID: {id}\\nSummary: {text}\\nScore: {score}\")\n",
    "        print(pretty_output)\n",
    "\n",
    "response = client.search(\n",
    "  index = index_name,\n",
    "  knn={\n",
    "      \"field\": \"embeddings\",\n",
    "      \"query_vector\": embedder.embed_query(\"How do I prevent accidents?\"),\n",
    "      \"k\": 10,\n",
    "      \"num_candidates\": 10\n",
    "    }\n",
    ")\n",
    "pretty_response(response)\n",
    "\n",
    "top_hit_summary = response['hits']['hits'][0]['_source']['properties']['text_representation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Before trying to begin RAG on this vector store with Langchain, we must add a source field in the metadata of each document.\n",
    "\n",
    "# Define the update script\n",
    "script = {\n",
    "    \"source\": \"\"\"\n",
    "    if (ctx._source.metadata == null) {\n",
    "        ctx._source.metadata = new HashMap();\n",
    "    }\n",
    "    ctx._source.metadata.source = 'default';\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "# Perform the update\n",
    "response = client.update_by_query(\n",
    "    index=index_name,\n",
    "    body={\n",
    "        \"query\": {\n",
    "            \"match_all\": {}\n",
    "        },\n",
    "        \"script\": script\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True})"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The Langchain Elasticsearch does not allow for nested text fields currently, so we bring the text field out of the properties dictionary.\n",
    "\n",
    "client.indices.put_mapping(\n",
    "    index=index_name,\n",
    "    body={\n",
    "        \"properties\": {\n",
    "            \"text_representation\": {\n",
    "                \"type\": \"text\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "response = client.update_by_query(\n",
    "    index=index_name,\n",
    "    body={\n",
    "        \"script\": {\n",
    "            \"source\": \"ctx._source.text_representation = ctx._source.properties.text_representation\",\n",
    "            \"lang\": \"painless\"\n",
    "        },\n",
    "        \"query\": {\n",
    "            \"exists\": {\n",
    "                \"field\": \"properties.text_representation\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "> Entering new RetrievalQAWithSourcesChain chain...\n",
      "\n",
      "> Finished chain.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'How many accidents happened?',\n",
       " 'answer': 'There was a total of 94 injuries, none of which were fatal.\\n',\n",
       " 'sources': 'default'}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, we initialize a RAG agent and ask the model a question about the data\n",
    "\n",
    "from langchain_elasticsearch import ElasticsearchStore\n",
    "from langchain_openai import ChatOpenAI  \n",
    "from langchain.chains import RetrievalQAWithSourcesChain  \n",
    "import os\n",
    "\n",
    "vector_store = ElasticsearchStore(index_name=index_name, es_connection=client, vector_query_field=\"embeddings\", \n",
    "                                  query_field=\"text_representation\", embedding=embedder)\n",
    "\n",
    "llm = ChatOpenAI(  \n",
    "    openai_api_key=os.environ.get(\"OPENAI_API_KEY\"),  \n",
    "    model_name='gpt-3.5-turbo',  \n",
    "    temperature=0.9\n",
    ")  \n",
    "qa = RetrievalQAWithSourcesChain.from_chain_type(  \n",
    "    llm=llm,  \n",
    "    chain_type=\"stuff\",  \n",
    "    retriever=vector_store.as_retriever() , verbose=True\n",
    ")  \n",
    "qa.invoke({\"question\": \"How many accidents happened?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sycamore-ai--zTjaFUY-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
