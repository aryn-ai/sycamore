{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Aryn Partitioner in this job is configured to use the Aryn Partitioning Service to provide fast, GPU-powered performance. Go to [aryn.ai/sign-up ](aryn.ai/sign-up) to get a free API key for the service. This is the recommended configuration.\n",
    "\n",
    "You can also run the Aryn Partitioner locally by setting `use_partitioning_service` to `False`. Though you can use CPU to run the Aryn Partitioner, it is recommended to use an NVIDIA GPU for good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.fs\n",
    "import sycamore\n",
    "from elasticsearch import Elasticsearch\n",
    "from sycamore.functions.tokenizer import HuggingFaceTokenizer\n",
    "from sycamore.llms import OpenAIModels, OpenAI\n",
    "from sycamore.transforms import COALESCE_WHITESPACE\n",
    "from sycamore.transforms.merge_elements import MarkedMerger\n",
    "from sycamore.transforms.partition import ArynPartitioner\n",
    "from sycamore.transforms.extract_schema import OpenAISchemaExtractor, OpenAIPropertyExtractor\n",
    "from sycamore.transforms.embed import SentenceTransformerEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sycamore.utils.aryn_config import ArynConfig, _DEFAULT_PATH\n",
    "assert ArynConfig.get_aryn_api_key() != \"\", f\"Unable to find aryn API key.  Looked in {_DEFAULT_PATH}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the above assertion fails, you can either set the environment variable ARYN_API_KEY and restart jupyter\n",
    "or make a yaml file at the specified path in the assertion error that looks like:\n",
    "\n",
    "```\n",
    "aryn_token: \"YOUR-ARYN-API-KEY\"\n",
    "```\n",
    "\n",
    "It is unsafe, but if neither of those options work, you can put it in this notebook with\n",
    "```\n",
    "import os\n",
    "os.environ[\"ARYN_API_KEY\"] = \"UNSAFE-ARYN-API-KEY-LOCATION\" \n",
    "```\n",
    "\n",
    "but beware that it is easy to accidentally commit the notebook file and have it include your key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function used for cleaning and formatting later on in the notebook\n",
    "\n",
    "from sycamore.data.document import Document\n",
    "from dateutil import parser\n",
    "def convert_timestamp(doc: Document) -> Document:\n",
    "    if \"dateAndTime\" not in doc.properties['entity'] and \"dateTime\" not in doc.properties['entity']:\n",
    "        return doc\n",
    "    raw_date: str = doc.properties['entity'].get('dateAndTime') or doc.properties['entity'].get('dateTime')\n",
    "    raw_date = raw_date.replace(\"Local\", \"\")\n",
    "    parsed_date = parser.parse(raw_date, fuzzy=True)\n",
    "    extracted_date = parsed_date.date()\n",
    "    doc.properties['entity']['day'] = extracted_date.day\n",
    "    doc.properties['entity']['month'] = extracted_date.month\n",
    "    doc.properties['entity']['year'] = extracted_date.year\n",
    "    if parsed_date.utcoffset():\n",
    "        doc.properties['entity']['dateTime'] = parsed_date.isoformat()\n",
    "    else:\n",
    "        doc.properties['entity']['dateTime'] = parsed_date.isoformat() + \"Z\"\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aryn_api_key = \"YOUR-ARYN-API-KEY\" ## REPLACE WITH YOUR-ARYN-API-KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"s3://aryn-public/NTSB/\"]\n",
    "fsys = pyarrow.fs.S3FileSystem(region=\"us-east-1\", anonymous=True)\n",
    "\n",
    "llm = OpenAI(OpenAIModels.GPT_3_5_TURBO.value)\n",
    "tokenizer = HuggingFaceTokenizer(\"thenlper/gte-small\")\n",
    "\n",
    "ctx = sycamore.init()\n",
    "\n",
    "ds = (\n",
    "    ctx.read.binary(paths, binary_format=\"pdf\", filesystem=fsys)\n",
    "    # Partition with the Aryn partitioner remotely, pulling out tables and images.\n",
    "    .partition(partitioner=ArynPartitioner(extract_images=True,  extract_table_structure=True))\n",
    "    # Get rid of spurious whitespace charaters\n",
    "    .regex_replace(COALESCE_WHITESPACE)\n",
    "    # Automatically determine a schema of additional metadata to extract from Documents\n",
    "    .extract_batch_schema(schema_extractor=OpenAISchemaExtractor(\"FlightAccidentReport\", llm=llm, num_of_elements=35))\n",
    "    # Extract the metadata specified by that schema\n",
    "    .extract_properties(property_extractor=OpenAIPropertyExtractor(llm=llm, num_of_elements=35))\n",
    "    # Merge elements into larger chunks\n",
    "    .mark_bbox_preset(tokenizer=tokenizer)\n",
    "    .merge(merger=MarkedMerger())\n",
    "    # Convert extracted timestamps to better-structured form using the function above\n",
    "    .map(convert_timestamp)\n",
    "    # Copy document properties to each Document's sub-elements\n",
    "    .spread_properties([\"path\", \"entity\"])\n",
    "    # Split elements that are too big to embed\n",
    "    .split_elements(tokenizer=tokenizer, max_tokens=512)\n",
    "    # Convert all Elements to Documents\n",
    "    .explode()\n",
    "    # Generate a series of hashes to represent each Document. For use with near-duplicate detection\n",
    "    .sketch()\n",
    "    # Embed each Document\n",
    "    .embed(embedder=SentenceTransformerEmbedder(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", batch_size=100))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to a persistent Elasticsearch Index. Note: You must have a specified elasticsearch instance running for this to work. For more information on how to set\n",
    "# one up, refer to https://www.elastic.co/guide/en/elasticsearch/reference/current/install-elasticsearch.html\n",
    "# - into a specific database (as specified by url) \n",
    "# - into a specific index (as specified by t)\n",
    "url = \"http://localhost:9200\"\n",
    "index_name = \"test_index\"\n",
    "ds.write.elasticsearch(url=url, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's initialize the Elasticsearch client and connect it to the database to perform queries\n",
    "client = Elasticsearch(\n",
    "    url\n",
    ")\n",
    "\n",
    "print(client.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for text search within Elasticsearch working\n",
    "\n",
    "result = client.search(index=index_name, body={\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"properties.text_representation\": \"traffic\"\n",
    "        }\n",
    "    },\n",
    "    \"_source\": [\"properties.text_representation\"],\n",
    "    \"size\": 5\n",
    "})\n",
    "\n",
    "# Process and print the results\n",
    "for hit in result['hits']['hits']:\n",
    "    text = hit['_source']['properties']['text_representation']\n",
    "    print(text)\n",
    "    print(\"-\" * 50)  # Separator between results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For queries, let's define an embedding function for the question that helps us easily run NN search \n",
    "# by comparing the two vectors and generating optimal results\n",
    "from sentence_transformers import SentenceTransformer\n",
    "minilm = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "from langchain_core.embeddings import Embeddings\n",
    "\n",
    "class Embedder(Embeddings):\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "    \n",
    "    def embed_query(self, question):\n",
    "        v = self.llm.encode(question).tolist()\n",
    "        return v\n",
    "    \n",
    "    def embed_documents(self, documents):\n",
    "        ans_list = [self.llm.encode(doc).tolist() for doc in documents]\n",
    "        return ans_list\n",
    "\n",
    "embedder = Embedder(minilm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us now run NN queries using Elasticsearch. First, we define a small function to pretty print the results and define an embedding function \n",
    "# for the question that helps us easily run NN search by comparing the two vectors and generating optimal results\n",
    "\n",
    "def pretty_response(response):\n",
    "    for hit in response['hits']['hits']:\n",
    "        id = hit['_id']\n",
    "        score = hit['_score']\n",
    "        text = hit['_source']['properties']['text_representation']\n",
    "        pretty_output = (f\"\\nID: {id}\\nSummary: {text}\\nScore: {score}\")\n",
    "        print(pretty_output)\n",
    "\n",
    "response = client.search(\n",
    "  index = index_name,\n",
    "  knn={\n",
    "      \"field\": \"embeddings\",\n",
    "      \"query_vector\": embedder.embed_query(\"How do I prevent accidents?\"),\n",
    "      \"k\": 10,\n",
    "      \"num_candidates\": 10\n",
    "    }\n",
    ")\n",
    "pretty_response(response)\n",
    "\n",
    "top_hit_summary = response['hits']['hits'][0]['_source']['properties']['text_representation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Before trying to begin RAG on this vector store with Langchain, we must add a source field in the metadata of each document.\n",
    "\n",
    "# Define the update script\n",
    "script = {\n",
    "    \"source\": \"\"\"\n",
    "    if (ctx._source.metadata == null) {\n",
    "        ctx._source.metadata = new HashMap();\n",
    "    }\n",
    "    ctx._source.metadata.source = 'default';\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "# Perform the update\n",
    "response = client.update_by_query(\n",
    "    index=index_name,\n",
    "    body={\n",
    "        \"query\": {\n",
    "            \"match_all\": {}\n",
    "        },\n",
    "        \"script\": script\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The Langchain Elasticsearch integration does not allow for nested text fields currently, so we bring the text field out of the properties \n",
    "# dictionary as independent column.\n",
    "\n",
    "client.indices.put_mapping(\n",
    "    index=index_name,\n",
    "    body={\n",
    "        \"properties\": {\n",
    "            \"text_representation\": {\n",
    "                \"type\": \"text\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "response = client.update_by_query(\n",
    "    index=index_name,\n",
    "    body={\n",
    "        \"script\": {\n",
    "            \"source\": \"ctx._source.text_representation = ctx._source.properties.text_representation\",\n",
    "            \"lang\": \"painless\"\n",
    "        },\n",
    "        \"query\": {\n",
    "            \"exists\": {\n",
    "                \"field\": \"properties.text_representation\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we initialize a RAG agent and ask the model a question about the data\n",
    "\n",
    "from langchain_elasticsearch import ElasticsearchStore\n",
    "from langchain_openai import ChatOpenAI  \n",
    "from langchain.chains import RetrievalQAWithSourcesChain  \n",
    "import os\n",
    "\n",
    "vector_store = ElasticsearchStore(index_name=index_name, es_connection=client, vector_query_field=\"embeddings\", \n",
    "                                  query_field=\"text_representation\", embedding=embedder)\n",
    "\n",
    "llm = ChatOpenAI(  \n",
    "    openai_api_key=os.environ.get(\"OPENAI_API_KEY\"),  \n",
    "    model_name='gpt-3.5-turbo',  \n",
    "    temperature=0.9\n",
    ")  \n",
    "qa = RetrievalQAWithSourcesChain.from_chain_type(  \n",
    "    llm=llm,  \n",
    "    chain_type=\"stuff\",  \n",
    "    retriever=vector_store.as_retriever() , verbose=True\n",
    ")  \n",
    "qa.invoke({\"question\": \"How many accidents happened?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
