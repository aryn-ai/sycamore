{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.fs\n",
    "from ray.data import ActorPoolStrategy\n",
    "import sycamore\n",
    "from elasticsearch import Elasticsearch\n",
    "from sycamore.functions.tokenizer import HuggingFaceTokenizer\n",
    "from sycamore.llms import OpenAIModels, OpenAI\n",
    "from sycamore.transforms import COALESCE_WHITESPACE\n",
    "from sycamore.transforms.merge_elements import MarkedMerger\n",
    "from sycamore.transforms.partition import UnstructuredPdfPartitioner\n",
    "from sycamore.transforms.extract_schema import OpenAISchemaExtractor, OpenAIPropertyExtractor\n",
    "from sycamore.transforms.embed import SentenceTransformerEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function used for cleaning and formatting later on in the notebook\n",
    "\n",
    "from sycamore.data.document import Document\n",
    "from dateutil import parser\n",
    "def convert_timestamp(doc: Document) -> Document:\n",
    "    if \"dateAndTime\" not in doc.properties['entity'] and \"dateTime\" not in doc.properties['entity']:\n",
    "        return doc\n",
    "    raw_date: str = doc.properties['entity'].get('dateAndTime') or doc.properties['entity'].get('dateTime')\n",
    "    raw_date = raw_date.replace(\"Local\", \"\")\n",
    "    parsed_date = parser.parse(raw_date, fuzzy=True)\n",
    "    extracted_date = parsed_date.date()\n",
    "    doc.properties['entity']['day'] = extracted_date.day\n",
    "    doc.properties['entity']['month'] = extracted_date.month\n",
    "    doc.properties['entity']['year'] = extracted_date.year\n",
    "    if parsed_date.utcoffset():\n",
    "        doc.properties['entity']['dateTime'] = parsed_date.isoformat()\n",
    "    else:\n",
    "        doc.properties['entity']['dateTime'] = parsed_date.isoformat() + \"Z\"\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"s3://aryn-public/NTSB/\"]\n",
    "fsys = pyarrow.fs.S3FileSystem(region=\"us-east-1\", anonymous=True)\n",
    "\n",
    "llm = OpenAI(OpenAIModels.GPT_3_5_TURBO.value)\n",
    "tokenizer = HuggingFaceTokenizer(\"thenlper/gte-small\")\n",
    "\n",
    "ctx = sycamore.init()\n",
    "\n",
    "ds = (\n",
    "    ctx.read.binary(paths, binary_format=\"pdf\", filesystem=fsys)\n",
    "    # Parition with the Aryn partitioner, pulling out tables and images. Set an appropriate compute to gain the maximum parallelism and performance\n",
    "    # from the machine, ActorPoolStrategy(size=4) worked best for our case here\n",
    "    .partition(partitioner=UnstructuredPdfPartitioner(), compute=ActorPoolStrategy(size=4))\n",
    "    # Get rid of spurious whitespace charaters\n",
    "    .regex_replace(COALESCE_WHITESPACE)\n",
    "    # Automatically determine a schema of additional metadata to extract from Documents\n",
    "    .extract_batch_schema(schema_extractor=OpenAISchemaExtractor(\"FlightAccidentReport\", llm=llm, num_of_elements=35))\n",
    "    # Extract the metadata specified by that schema\n",
    "    .extract_properties(property_extractor=OpenAIPropertyExtractor(llm=llm, num_of_elements=35))\n",
    "    # Merge elements into larger chunks\n",
    "    .mark_bbox_preset(tokenizer=tokenizer)\n",
    "    .merge(merger=MarkedMerger())\n",
    "    # Convert extracted timestamps to better-structured form using the function above\n",
    "    .map(convert_timestamp)\n",
    "    # Copy document properties to each Document's sub-elements\n",
    "    .spread_properties([\"path\", \"entity\"])\n",
    "    # Split elements that are too big to embed\n",
    "    .split_elements(tokenizer=tokenizer, max_tokens=512)\n",
    "    # Convert all Elements to Documents\n",
    "    .explode()\n",
    "    # Generate a series of hashes to represent each Document. For use with near-duplicate detection\n",
    "    .sketch()\n",
    "    # Embed each Document\n",
    "    .embed(embedder=SentenceTransformerEmbedder(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", batch_size=100))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-15 17:59:15,524\tWARNING util.py:560 -- The argument ``compute`` is deprecated in Ray 2.9. Please specify argument ``concurrency`` instead. For more information, see https://docs.ray.io/en/master/data/transforming-data.html#stateful-transforms.\n",
      "2024-07-15 17:59:15,526\tWARNING util.py:560 -- The argument ``compute`` is deprecated in Ray 2.9. Please specify argument ``concurrency`` instead. For more information, see https://docs.ray.io/en/master/data/transforming-data.html#stateful-transforms.\n",
      "2024-07-15 17:59:15,539\tINFO streaming_executor.py:112 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2024-07-15_17-37-44_746435_11753/logs/ray-data\n",
      "2024-07-15 17:59:15,539\tINFO streaming_executor.py:113 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadBinary] -> ActorPoolMapOperator[Map(BinaryScan._to_document)->MapBatches(BaseMapTransformCallable___wrap)] -> ActorPoolMapOperator[MapBatches(regex_replace)->MapBatches(BaseMapTransformCustom__Extract)] -> TaskPoolMapOperator[MapBatches(extract_properties)->MapBatches(sort_by_page_bbox)->MapBatches(mark_drop_tiny)->MapBatches(mark_drop_header_and_footer)->MapBatches(mark_break_page)->MapBatches(mark_break_by_column)->MapBatches(mark_break_by_tokens)->MapBatches(merge_elements)->MapBatches(convert_timestamp)->MapBatches(spread_properties)->MapBatches(split_doc)->MapBatches(explode)->MapBatches(sketcher)->MapBatches(SentenceTransformerEmbedder)->MapBatches(elastic_document_writer)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe2be797c03142a48c783e37bac5f6b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- ReadBinary->SplitBlocks(5) 1:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6606bd04d1c54bb09a5c2f0dc78676ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Map(BinaryScan._to_document)->MapBatches(BaseMapTransformCallable___wrap) 2:   0%|          | 0/5 [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "760eca32470d447f89de8955bf1bb50e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(regex_replace)->MapBatches(BaseMapTransformCustom__Extract) 3:   0%|          | 0/5 [00:00<?, ?it…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "142654eeae9842ea92c6b8e830e2a01e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(extract_properties)->MapBatches(sort_by_page_bbox)->MapBatches(mark_drop_tiny)->MapBatches(mark_d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f933ae9fa8674faebe31c50274ceaf8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>e)->MapBatches(BaseMapTransformCustom__Extract)) pid=21457) \n",
      "<IPython.core.display.HTML object>e)->MapBatches(BaseMapTransformCustom__Extract)) pid=21457) \n",
      "<IPython.core.display.HTML object>e)->MapBatches(BaseMapTransformCustom__Extract)) pid=21457) \n",
      "<IPython.core.display.HTML object>e)->MapBatches(BaseMapTransformCustom__Extract)) pid=21457) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(MapBatches(extract_properties)->MapBatches(sort_by_page_bbox)->MapBatches(mark_drop_tiny)->MapBatches(mark_drop_header_and_footer)->MapBatches(mark_break_page)->MapBatches(mark_break_by_column)->MapBatches(mark_break_by_tokens)->MapBatches(merge_elements)->MapBatches(convert_timestamp)->MapBatches(spread_properties)->MapBatches(split_doc)->MapBatches(explode)->MapBatches(sketcher)->MapBatches(SentenceTransformerEmbedder)->MapBatches(elastic_document_writer) pid=12115) /Users/karansampath/Library/Caches/pypoetry/virtualenvs/sycamore-ai--zTjaFUY-py3.10/lib/python3.10/site-packages/dateutil/parser/_parser.py:1207: UnknownTimezoneWarning: tzname CST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "(MapBatches(extract_properties)->MapBatches(sort_by_page_bbox)->MapBatches(mark_drop_tiny)->MapBatches(mark_drop_header_and_footer)->MapBatches(mark_break_page)->MapBatches(mark_break_by_column)->MapBatches(mark_break_by_tokens)->MapBatches(merge_elements)->MapBatches(convert_timestamp)->MapBatches(spread_properties)->MapBatches(split_doc)->MapBatches(explode)->MapBatches(sketcher)->MapBatches(SentenceTransformerEmbedder)->MapBatches(elastic_document_writer) pid=12115)   warnings.warn(\"tzname {tzname} identified but not understood.  \"\n"
     ]
    }
   ],
   "source": [
    "# Write to a persistent Elasticsearch Index. Note: You must have a specified elasticsearch instance running for this to work. For more information on how to set\n",
    "# one up, refer to https://www.elastic.co/guide/en/elasticsearch/reference/current/install-elasticsearch.html\n",
    "# - into a specific database (as specified by url) \n",
    "# - into a specific index (as specified by t)\n",
    "url = \"http://localhost:9201\"\n",
    "index_name = \"test_index\"\n",
    "ds.write.elasticsearch(url=url, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': '3b979cc72e9d', 'cluster_name': 'docker-cluster', 'cluster_uuid': 'rNXKhq5STYelYaU1xEN_rA', 'version': {'number': '8.14.2', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '2afe7caceec8a26ff53817e5ed88235e90592a1b', 'build_date': '2024-07-01T22:06:58.515911606Z', 'build_snapshot': False, 'lucene_version': '9.10.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'}\n"
     ]
    }
   ],
   "source": [
    "# Let's initialize the Elasticsearch client and connect it to the database to perform queries\n",
    "client = Elasticsearch(\n",
    "    url\n",
    ")\n",
    "\n",
    "print(client.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meteorological Information and Flight Plan\n",
      "Conditions at Accident Site:\n",
      "Visual (VMC)\n",
      "Condition of Light:\n",
      "Day\n",
      "Observation Facility, Elevation:\n",
      "Observation Time:\n",
      "Distance from Accident Site:\n",
      "Direction from Accident Site:\n",
      "Lowest Cloud Condition:\n",
      "Visibility\n",
      "Lowest Ceiling:\n",
      "Visibility (RVR):\n",
      "Wind Speed/Gusts:\n",
      "9 knots /\n",
      "Turbulence Type Forecast/Actual:\n",
      "Wind Direction:\n",
      "360°\n",
      "Turbulence Severity Forecast/Actual:\n",
      "Altimeter Setting:\n",
      "Temperature/Dew Point:\n",
      "Precipitation and Obscuration:\n",
      "Departure Point:\n",
      "McMurdo (NZPG)\n",
      "Type of Flight Plan Filed:\n",
      "None\n",
      "Destination:\n",
      "McMurdo (NZPG)\n",
      "Type of Clearance:\n",
      "None\n",
      "Departure Time:\n",
      "Type of Airspace:\n",
      "Unknown\n",
      "Airport Information\n",
      "Airport:\n",
      "Pegasus McMurdo, Antarctica NZPG\n",
      "Runway Surface Type:\n",
      "Ice\n",
      "Airport Elevation:\n",
      "18 ft msl\n",
      "Runway Surface Condition:\n",
      "Ice\n",
      "Runway Used:\n",
      "33\n",
      "IFR Approach:\n",
      "None\n",
      "Runway Length/Width:\n",
      "10000 ft / 200 ft\n",
      "VFR Approach/Landing:\n",
      "Go around;Traffic pattern\n",
      "Wreckage and Impact Information\n",
      "Crew Injuries:\n",
      "Aircraft Damage:\n",
      "Substantial\n",
      "Passenger Injuries:\n",
      "Aircraft Fire:\n",
      "None\n",
      "Ground Injuries:\n",
      "N/A\n",
      "Aircraft Explosion:\n",
      "None\n",
      "Total Injuries:\n",
      "N/A\n",
      "Latitude, Longitude:\n",
      "77,166(est)\n",
      "\n",
      "--------------------------------------------------\n",
      "Meteorological Information and Flight Plan\n",
      "Conditions at Accident Site:\n",
      "Visual (VMC)\n",
      "Condition of Light:\n",
      "Day\n",
      "Observation Facility, Elevation:\n",
      "Observation Time:\n",
      "Distance from Accident Site:\n",
      "Direction from Accident Site:\n",
      "Lowest Cloud Condition:\n",
      "Visibility\n",
      "Lowest Ceiling:\n",
      "Visibility (RVR):\n",
      "Wind Speed/Gusts:\n",
      "9 knots /\n",
      "Turbulence Type Forecast/Actual:\n",
      "Wind Direction:\n",
      "360°\n",
      "Turbulence Severity Forecast/Actual:\n",
      "Altimeter Setting:\n",
      "Temperature/Dew Point:\n",
      "Precipitation and Obscuration:\n",
      "Departure Point:\n",
      "McMurdo (NZPG)\n",
      "Type of Flight Plan Filed:\n",
      "None\n",
      "Destination:\n",
      "McMurdo (NZPG)\n",
      "Type of Clearance:\n",
      "None\n",
      "Departure Time:\n",
      "Type of Airspace:\n",
      "Unknown\n",
      "Airport Information\n",
      "Airport:\n",
      "Pegasus McMurdo, Antarctica NZPG\n",
      "Runway Surface Type:\n",
      "Ice\n",
      "Airport Elevation:\n",
      "18 ft msl\n",
      "Runway Surface Condition:\n",
      "Ice\n",
      "Runway Used:\n",
      "33\n",
      "IFR Approach:\n",
      "None\n",
      "Runway Length/Width:\n",
      "10000 ft / 200 ft\n",
      "VFR Approach/Landing:\n",
      "Go around;Traffic pattern\n",
      "Wreckage and Impact Information\n",
      "Crew Injuries:\n",
      "Aircraft Damage:\n",
      "Substantial\n",
      "Passenger Injuries:\n",
      "Aircraft Fire:\n",
      "None\n",
      "Ground Injuries:\n",
      "N/A\n",
      "Aircraft Explosion:\n",
      "None\n",
      "Total Injuries:\n",
      "N/A\n",
      "Latitude, Longitude:\n",
      "77,166(est)\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Check for text search within Elasticsearch working\n",
    "\n",
    "result = client.search(index=index_name, body={\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"properties.text_representation\": \"traffic\"\n",
    "        }\n",
    "    },\n",
    "    \"_source\": [\"properties.text_representation\"],\n",
    "    \"size\": 5\n",
    "})\n",
    "\n",
    "# Process and print the results\n",
    "for hit in result['hits']['hits']:\n",
    "    text = hit['_source']['properties']['text_representation']\n",
    "    print(text)\n",
    "    print(\"-\" * 50)  # Separator between results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karansampath/Library/Caches/pypoetry/virtualenvs/sycamore-ai--zTjaFUY-py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# For queries, let's define an embedding function for the question that helps us easily run NN search \n",
    "# by comparing the two vectors and generating optimal results\n",
    "from sentence_transformers import SentenceTransformer\n",
    "minilm = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "class Embedder():\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "    \n",
    "    def embed_query(self, question):\n",
    "        v = self.llm.encode(question).tolist()\n",
    "        return v\n",
    "\n",
    "embedder = Embedder(minilm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ID: f98c5018-86ad-426e-9f57-22acb6aef49a\n",
      "Summary: The National Transportation Safety Board determines the probable cause(s) of this accident to be:\n",
      "Page 1 of 5\n",
      "\n",
      "Score: 0.70878434\n",
      "\n",
      "ID: e9cb97c7-c3df-42e7-b2d0-1aab568182db\n",
      "Summary: The National Transportation Safety Board determines the probable cause(s) of this accident to be:\n",
      "Page 1 of 5\n",
      "\n",
      "Score: 0.70878434\n",
      "\n",
      "ID: 0031397f-88af-4a61-b940-70113ee2be63\n",
      "Summary: The National Transportation Safety Board determines the probable cause(s) of this accident to be: THE PILOT'S CONTINUED FLIGHT INTO ADVERSE WEATHER CONDITIONS. FACTORS WERE THE ICING CONDITIONS PREVAILING AT THE DESTINATION AIRPORT, AND THE PILOT'S INABILITY TO MAINTAIN VISUAL LOOKOUT DUE TO WINDSHIELD ICING.\n",
      "Page 1 of 7\n",
      "\n",
      "Score: 0.68199754\n",
      "\n",
      "ID: 2adc9b9c-7ba7-4bb6-aa1b-847f78fcab95\n",
      "Summary: The National Transportation Safety Board determines the probable cause(s) of this accident to be: THE PILOT'S CONTINUED FLIGHT INTO ADVERSE WEATHER CONDITIONS. FACTORS WERE THE ICING CONDITIONS PREVAILING AT THE DESTINATION AIRPORT, AND THE PILOT'S INABILITY TO MAINTAIN VISUAL LOOKOUT DUE TO WINDSHIELD ICING.\n",
      "Page 1 of 7\n",
      "\n",
      "Score: 0.68199754\n",
      "\n",
      "ID: 85565baf-1995-4687-9454-71368274abfd\n",
      "Summary: Wreckage and Impact Information\n",
      "Crew Injuries:\n",
      "1 Minor, 3 None\n",
      "Aircraft Damage:\n",
      "Substantial\n",
      "Passenger Injuries:\n",
      "6 None\n",
      "Aircraft Fire:\n",
      "None\n",
      "Ground Injuries:\n",
      "N/A\n",
      "Aircraft Explosion:\n",
      "None\n",
      "Total Injuries:\n",
      "1 Minor, 9 None\n",
      "Latitude, Longitude:\n",
      "78.016944,-154.966949(est)\n",
      "Administrative Information\n",
      "Investigator In Charge (IIC):\n",
      "Johnson, Clinton\n",
      "Additional Participating Persons:\n",
      "Jon Lee; Transport Safety Board (TSB); Edmonton, Alberta John E Mills; US Department of the Interior; Bosie , ID\n",
      "Original Publish Date:\n",
      "March 5, 2009\n",
      "Note:\n",
      "Investigation Docket:\n",
      "https://data.ntsb.gov/Docket?ProjectID=67394\n",
      "The National Transportation Safety Board (NTSB), established in 1967, is an independent federal agency mandated by Congress through the Independent Safety Board Act of 1974 to investigate transportation accidents, determine the probable causes of the accidents, issue safety recommendations, study transportation safety issues, and evaluate the safety effectiveness of government agencies involved in transportation. The NTSB makes public its actions and decisions through accident reports, safety studies, special investigation reports, safety recommendations, and statistical reviews.\n",
      "The Independent Safety Board Act, as codified at 49 U.S.C. Section 1154(b), precludes the admission into evidence or use of any part of an NTSB report related to an incident or accident in a civil action for damages resulting from a matter mentioned in the report. A factual report that may be admissible under 49 U.S.C. § 1154(b) is available here.\n",
      "\n",
      "Score: 0.6648934\n",
      "\n",
      "ID: 32b9b9f2-d4e4-4b77-afc1-29e72c781933\n",
      "Summary: Wreckage and Impact Information\n",
      "Crew Injuries:\n",
      "1 Minor, 3 None\n",
      "Aircraft Damage:\n",
      "Substantial\n",
      "Passenger Injuries:\n",
      "6 None\n",
      "Aircraft Fire:\n",
      "None\n",
      "Ground Injuries:\n",
      "N/A\n",
      "Aircraft Explosion:\n",
      "None\n",
      "Total Injuries:\n",
      "1 Minor, 9 None\n",
      "Latitude, Longitude:\n",
      "78.016944,-154.966949(est)\n",
      "Administrative Information\n",
      "Investigator In Charge (IIC):\n",
      "Johnson, Clinton\n",
      "Additional Participating Persons:\n",
      "Jon Lee; Transport Safety Board (TSB); Edmonton, Alberta John E Mills; US Department of the Interior; Bosie , ID\n",
      "Original Publish Date:\n",
      "March 5, 2009\n",
      "Note:\n",
      "Investigation Docket:\n",
      "https://data.ntsb.gov/Docket?ProjectID=67394\n",
      "The National Transportation Safety Board (NTSB), established in 1967, is an independent federal agency mandated by Congress through the Independent Safety Board Act of 1974 to investigate transportation accidents, determine the probable causes of the accidents, issue safety recommendations, study transportation safety issues, and evaluate the safety effectiveness of government agencies involved in transportation. The NTSB makes public its actions and decisions through accident reports, safety studies, special investigation reports, safety recommendations, and statistical reviews.\n",
      "The Independent Safety Board Act, as codified at 49 U.S.C. Section 1154(b), precludes the admission into evidence or use of any part of an NTSB report related to an incident or accident in a civil action for damages resulting from a matter mentioned in the report. A factual report that may be admissible under 49 U.S.C. § 1154(b) is available here.\n",
      "\n",
      "Score: 0.6648934\n",
      "\n",
      "ID: 377b1d5b-305b-47e8-81dd-b883666855d3\n",
      "Summary: The National Transportation Safety Board (NTSB), established in 1967, is an independent federal agency mandated by Congress through the Independent Safety Board Act of 1974 to investigate transportation accidents, determine the probable causes of the accidents, issue safety recommendations, study transportation safety issues, and evaluate the safety effectiveness of government agencies involved in transportation. The NTSB makes public its actions and decisions through accident reports, safety studies, special investigation reports, safety recommendations, and statistical reviews.\n",
      "The Independent Safety Board Act, as codified at 49 U.S.C. Section 1154(b), precludes the admission into evidence or use of any part of an NTSB report related to an incident or accident in a civil action for damages resulting from a matter mentioned in the report.\n",
      "Page 7 of 7\n",
      "FTW95FA129\n",
      "\n",
      "Score: 0.6609762\n",
      "\n",
      "ID: 85212b3a-ddef-428b-9a49-fda0aeba9bfd\n",
      "Summary: The National Transportation Safety Board (NTSB), established in 1967, is an independent federal agency mandated by Congress through the Independent Safety Board Act of 1974 to investigate transportation accidents, determine the probable causes of the accidents, issue safety recommendations, study transportation safety issues, and evaluate the safety effectiveness of government agencies involved in transportation. The NTSB makes public its actions and decisions through accident reports, safety studies, special investigation reports, safety recommendations, and statistical reviews.\n",
      "The Independent Safety Board Act, as codified at 49 U.S.C. Section 1154(b), precludes the admission into evidence or use of any part of an NTSB report related to an incident or accident in a civil action for damages resulting from a matter mentioned in the report.\n",
      "Page 7 of 7\n",
      "FTW95FA129\n",
      "\n",
      "Score: 0.6609762\n",
      "\n",
      "ID: 5cc87ebb-9a60-46c1-845b-3e8640426282\n",
      "Summary: The National Transportation Safety Board determines the probable cause(s) of this accident to be: an aerodynamic stall induced by an inadvertent autopilot Home command. The Home command was entered following a loss of the direct radio link due to improperly set failsafe settings, and an unintentional latching of Home mode from an earlier functionality test.\n",
      "Findings\n",
      "Aircraft\n",
      "Angle of attack - Capability exceeded\n",
      "Aircraft\n",
      "Autopilot system - Incorrect use/operation\n",
      "Aircraft\n",
      "Autopilot system - Unintentional use/operation\n",
      "Organizational issues\n",
      "Equip certification/testing - Operator\n",
      "\n",
      "Score: 0.6470318\n",
      "\n",
      "ID: 2e038b99-8a2b-4fef-836f-c33e3f9918f3\n",
      "Summary: The National Transportation Safety Board determines the probable cause(s) of this accident to be: an aerodynamic stall induced by an inadvertent autopilot Home command. The Home command was entered following a loss of the direct radio link due to improperly set failsafe settings, and an unintentional latching of Home mode from an earlier functionality test.\n",
      "Findings\n",
      "Aircraft\n",
      "Angle of attack - Capability exceeded\n",
      "Aircraft\n",
      "Autopilot system - Incorrect use/operation\n",
      "Aircraft\n",
      "Autopilot system - Unintentional use/operation\n",
      "Organizational issues\n",
      "Equip certification/testing - Operator\n",
      "\n",
      "Score: 0.6470318\n"
     ]
    }
   ],
   "source": [
    "# Let us now run NN queries using Elasticsearch. First, we define a small function to pretty print the results and define an embedding function \n",
    "# for the question that helps us easily run NN search by comparing the two vectors and generating optimal results\n",
    "\n",
    "def pretty_response(response):\n",
    "    for hit in response['hits']['hits']:\n",
    "        id = hit['_id']\n",
    "        score = hit['_score']\n",
    "        text = hit['_source']['properties']['text_representation']\n",
    "        pretty_output = (f\"\\nID: {id}\\nSummary: {text}\\nScore: {score}\")\n",
    "        print(pretty_output)\n",
    "\n",
    "response = client.search(\n",
    "  index = index_name,\n",
    "  knn={\n",
    "      \"field\": \"embeddings\",\n",
    "      \"query_vector\": embedder.embed_query(\"How do I prevent accidents?\"),\n",
    "      \"k\": 10,\n",
    "      \"num_candidates\": 10\n",
    "    }\n",
    ")\n",
    "pretty_response(response)\n",
    "\n",
    "top_hit_summary = response['hits']['hits'][0]['_source']['properties']['text_representation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Before trying to begin RAG on this vector store, we must add a source field in the metadata of each document. We use the script below to do so\n",
    "\n",
    "# Define the update script\n",
    "script = {\n",
    "    \"source\": \"\"\"\n",
    "    if (ctx._source.metadata == null) {\n",
    "        ctx._source.metadata = new HashMap();\n",
    "    }\n",
    "    ctx._source.metadata.source = 'default';\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "# Perform the update\n",
    "response = client.update_by_query(\n",
    "    index=index_name,\n",
    "    body={\n",
    "        \"query\": {\n",
    "            \"match_all\": {}\n",
    "        },\n",
    "        \"script\": script\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "> Entering new RetrievalQAWithSourcesChain chain...\n",
      "\n",
      "> Finished chain.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What are some probable causes of accidents?',\n",
       " 'answer': \"I don't know.\\n\",\n",
       " 'sources': ''}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, we initialize a RAG agent and ask the model a question about the data\n",
    "\n",
    "from langchain_elasticsearch import ElasticsearchStore\n",
    "from langchain_openai import ChatOpenAI  \n",
    "from langchain.chains import RetrievalQAWithSourcesChain  \n",
    "import os\n",
    "\n",
    "vector_store = ElasticsearchStore(index_name=index_name, es_connection=client, vector_query_field=\"embeddings\", query_field=\"properties.text_representation\",\n",
    "                                   embedding=embedder)\n",
    "\n",
    "llm = ChatOpenAI(  \n",
    "    openai_api_key=os.environ.get(\"OPENAI_API_KEY\"),  \n",
    "    model_name='gpt-3.5-turbo',  \n",
    "    temperature=0.2\n",
    ")  \n",
    "qa = RetrievalQAWithSourcesChain.from_chain_type(  \n",
    "    llm=llm,  \n",
    "    chain_type=\"stuff\",  \n",
    "    retriever=vector_store.as_retriever() , verbose=True\n",
    ")  \n",
    "qa.invoke({\"question\": \"What are some probable causes of accidents?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sycamore-ai--zTjaFUY-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
