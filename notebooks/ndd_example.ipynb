{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed9dd0a2-6b12-436f-b6c3-02e0072115e3",
   "metadata": {},
   "source": [
    "This notebook shows the results of query-time near-duplicate detection (NDD).  It shows the same query with and without the duplicates.  The content here is inspired by this [blog post](https://www.aryn.ai/post/near-duplicate-detection-in-sycamore-what-is-it-good-for).\n",
    "\n",
    "To use this notebook:\n",
    "1. Follow [these instructions](https://sycamore.readthedocs.io/en/stable/welcome_to_sycamore/get_started.html) and start the Sycamore containers using `docker compose up`.\n",
    "2. It's best to start with a clean slate by running `docker compose run reset`.\n",
    "3. Ingest the college credit card marketing agreements data.  The documents come from [data.gov](https://catalog.data.gov/dataset/college-credit-card-marketing-agreements-data), but we have made them accessible via a public S3 bucket.  There are two ingestion methods to choose from, depending on how much time is available:\n",
    "\n",
    "    - JSON: (minutes) ingest pre-processed data represented as JSON into OpenSearch\n",
    "    - PDF: (hours) fully process all ~2000 PDFs and ingest them into OpenSearch\n",
    "\n",
    "Set `use_json` below accordingly.  Also set `save_resources` as desired.\n",
    "\n",
    "The results should be the same for both methods, although there may be variations due to platform differences and OpenAI variation.\n",
    "\n",
    "More information about NDD can be found [here](https://sycamore.readthedocs.io/en/stable/querying_data/dedup.html).  Join our [Slack channel](https://join.slack.com/t/sycamore-ulj8912/shared_invite/zt-23sv0yhgy-MywV5dkVQ~F98Aoejo48Jg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbf93d45-bee3-4202-8d5a-88497196db06",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import warnings\n",
    "import urllib3\n",
    "import multiprocessing\n",
    "import pyarrow.fs\n",
    "import sycamore\n",
    "from sycamore.functions.tokenizer import HuggingFaceTokenizer\n",
    "from sycamore.transforms import COALESCE_WHITESPACE\n",
    "from sycamore.transforms.merge_elements import MarkedMerger\n",
    "from sycamore.transforms.partition import UnstructuredPdfPartitioner\n",
    "from sycamore.transforms.embed import SentenceTransformerEmbedder\n",
    "\n",
    "warnings.filterwarnings('ignore', category=urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a467ebe-fbdd-40a8-8075-48c200f0b788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to False to ingest the PDFs from scratch, which takes an hour or more\n",
    "use_json = True\n",
    "\n",
    "# Set to False to use all available CPU and memory\n",
    "save_resources = True\n",
    "\n",
    "# Different hostnames inside and outside Docker compose environment\n",
    "opensearch_host = 'opensearch' if os.path.exists('/.dockerenv') else 'localhost'\n",
    "\n",
    "index_name = 'demoindex0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9291082e-cee5-465c-a15f-5cfdcc15a1be",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "osrch_args = {\n",
    "    'hosts': [{'host': opensearch_host, 'port': 9200}],\n",
    "    'http_compress': True,\n",
    "    'http_auth': ('admin', 'admin'),\n",
    "    'use_ssl': True,\n",
    "    'verify_certs': False,\n",
    "    'ssl_assert_hostname': False,\n",
    "    'ssl_show_warn': False,\n",
    "    'timeout': 120,\n",
    "}\n",
    "\n",
    "idx_settings = {\n",
    "    'body': {\n",
    "        'settings': {\n",
    "            'index.knn': True,\n",
    "        },\n",
    "        'mappings': {\n",
    "            'properties': {\n",
    "                'embedding': {\n",
    "                    'type': 'knn_vector',\n",
    "                    'dimension': 384,\n",
    "                    'method': {'name': 'hnsw', 'engine': 'faiss'},\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b890ef9-bfd9-46fe-b05b-13066ed525e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-11 04:41:45,809\tWARNING services.py:1996 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=9.86gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2024-04-11 04:41:48,206\tWARNING read_api.py:2850 -- The argument ``parallelism`` is deprecated in Ray 2.10. Please specify argument ``override_num_blocks`` instead.\n",
      "2024-04-11 04:41:57,097\tINFO streaming_executor.py:115 -- Starting execution of Dataset. Full log is in /tmp/ray/session_2024-04-11_04-41-44_300665_12230/logs/ray-data.log\n",
      "2024-04-11 04:41:57,097\tINFO streaming_executor.py:116 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadJSON->FlatMap(json_as_document)->Write]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- ReadJSON->FlatMap(json_as_document)->Write 1:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1f3fe132e6e4463bb9d92c4e7333656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parallelism = max(2, multiprocessing.cpu_count() // 2) if save_resources else -1\n",
    "\n",
    "tokenizer = HuggingFaceTokenizer('thenlper/gte-small')\n",
    "embedder = SentenceTransformerEmbedder(model_name='sentence-transformers/all-MiniLM-L6-v2', batch_size=100)\n",
    "\n",
    "fsys = pyarrow.fs.S3FileSystem(anonymous=True, region='us-east-1')\n",
    "ctx = sycamore.init()\n",
    "\n",
    "if use_json:\n",
    "    # Fast way: pre-processed DocSet as JSON...\n",
    "    path = 's3://aryn-public/cccmad-json'\n",
    "    ds = ctx.read.json_document(path, filesystem=fsys, parallelism=parallelism)\n",
    "else:\n",
    "    # Slow way: process PDF documents via Sycamore pipeline...\n",
    "    path = 's3://aryn-public/cccmad'\n",
    "    ds = (\n",
    "        ctx.read.binary(path, binary_format='pdf', filesystem=fsys, parallelism=parallelism)\n",
    "        .partition(partitioner=UnstructuredPdfPartitioner())\n",
    "        .regex_replace(COALESCE_WHITESPACE)\n",
    "        .mark_bbox_preset(tokenizer=tokenizer)\n",
    "        .merge(merger=MarkedMerger())\n",
    "        .spread_properties(['path'])\n",
    "        .split_elements(tokenizer=tokenizer, max_tokens=512)\n",
    "        .explode()\n",
    "        .sketch()\n",
    "        .embed(embedder=embedder)\n",
    "    )\n",
    "\n",
    "ds.write.opensearch(\n",
    "    os_client_args=osrch_args,\n",
    "    index_name=index_name,\n",
    "    index_settings=idx_settings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e0eb6c-2585-4c00-97ea-f7f7aa316ed5",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "The code below exists to retrieve the embedding model ID from OpenSearch.  This ID is different every time OpenSearch is set up.  We need to supply the ID in our query.  So, we will fetch it every time in order to be sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06e9a476-134f-4901-a89e-3c91bdb8881c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_id():\n",
    "    query = {\n",
    "        'query': {\n",
    "            'bool': {\n",
    "                'must': [\n",
    "                    {\n",
    "                        'match': {'name': 'all-MiniLM-L6-v2'},\n",
    "                    },\n",
    "                    {\n",
    "                        'term': {'model_config.model_type': 'bert'},\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "    with requests.get(f'https://{opensearch_host}:9200/_plugins/_ml/models/_search', json=query, verify=False) as resp:\n",
    "        res = json.loads(resp.text)\n",
    "        return res['hits']['hits'][0]['_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94d8d48-a8b6-4704-a709-5cd022ed7343",
   "metadata": {},
   "source": [
    "<br><hr>\n",
    "This next function performs the given query and prints out both the top ten retrieved chunks and the AI-generated answer.  For clarity, the text chunks are truncated at 80 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a68af4e-843e-4c4e-b295-1a4e9ad55881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_query(query_dict):\n",
    "    url = f'https://{opensearch_host}:9200/{index_name}/_search?search_pipeline=hybrid_rag_pipeline'\n",
    "    with requests.post(url, json=query, verify=False) as resp:\n",
    "        res = json.loads(resp.text)\n",
    "        hits = res['hits']['hits']\n",
    "        for i in range(10):\n",
    "            text = hits[i]['_source']['text_representation']\n",
    "            text = text.replace('\\n', ' ')[:80]\n",
    "            print(f'[{i+1}] {text}')\n",
    "        answer = res['ext']['retrieval_augmented_generation']['answer']\n",
    "        print(f'[ANSWER]\\n{answer}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a71039-0a85-4e14-b42a-0969d04d979a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "First, we run the query without near-duplicate-detection.  We do this by not asking for `shingles` in `_source`.  In OpenSearch queries, the `_source` is where we list the fields that we want to retrieve for each hit.\n",
    "\n",
    "If everything is set up and running properly, the numbered results will contain many repeated lines.  There is only one document in the top 10 (the RAG context).  The resulting generated answer starts by saying no information was found and then goes on to summarize the single source.  The answer doesn't reflect the breadth of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02b45b71-75ba-4a88-b3d0-41563dba91a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] The Party affected by the Event of Force Majeure shall make all reasonable effor\n",
      "[2] The Party affected by the Event of Force Majeure shall make all reasonable effor\n",
      "[3] The Party affected by the Event of Force Majeure shall make all reasonable effor\n",
      "[4] The Party affected by the Event of Force Majeure shall make all reasonable effor\n",
      "[5] The Party affected by the Event of Force Majeure shall make all reasonable effor\n",
      "[6] The Party affected by the Event of Force Majeure shall make all reasonable effor\n",
      "[7] The Party affected by the Event of Force Majeure shall make all reasonable effor\n",
      "[8] The Party affected by the Event of Force Majeure shall make all reasonable effor\n",
      "[9] The Party affected by the Event of Force Majeure shall make all reasonable effor\n",
      "[10] The Party affected by the Event of Force Majeure shall make all reasonable effor\n",
      "[ANSWER]\n",
      "The search results do not provide specific information on how force majeure affects assets and insolvency. However, they indicate that in the event of force majeure, the affected party should try to mitigate the impact and resume its obligations as soon as possible. If the party cannot fulfill its obligations for more than eight weeks due to the force majeure event, the other party may terminate the agreement without liability [1].\n"
     ]
    }
   ],
   "source": [
    "query_str = 'how does force majeure affect assets and insolvency'\n",
    "query = {\n",
    "    '_source': [\n",
    "        'text_representation',\n",
    "    ],\n",
    "    'query': {\n",
    "        'hybrid': {\n",
    "            'queries': [\n",
    "                {\n",
    "                    'match': {'text_representation': query_str},\n",
    "                },\n",
    "                {\n",
    "                    'neural': {\n",
    "                        'embedding': {\n",
    "                            'query_text': query_str,\n",
    "                            'k': 100,\n",
    "                            'model_id': get_model_id(),\n",
    "                        },\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    },\n",
    "    'ext': {\n",
    "        'generative_qa_parameters': {\n",
    "            'llm_question': query_str,\n",
    "            'context_size': 10,\n",
    "            'llm_model': 'gpt-4',\n",
    "        },\n",
    "    },\n",
    "    'size': 100,\n",
    "}\n",
    "do_query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ba7b19-4c11-4a00-bada-fe367bb212fd",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "For the next query, we re-use the previous query data structure, but we modify it slightly.  We append `shingles` to the list of fields to be retrieved.  This enables NDD processing; without `shingles` it can't detect near-duplicates.  Now, when we run the query there is much more diversity in the retrieved chunks.  There appear to be four unique chunks after NDD.  Looking at the generated answer, there are more cited sources and the explanation is richer.  It specifically addresses insolvency, which was part of the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d63234b4-fb51-4f07-8033-4e54f634d7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] The Party affected by the Event of Force Majeure shall make all reasonable effor\n",
      "[2] (b) assets, or it is unable to meet or it has ceased paying its obligation as th\n",
      "[3] (vi) becomes insolvent in that its liabilities exceed its assets, then the other\n",
      "[4] exceed its assets, or is adjudicated insolvent, or takes advantage of or is subj\n",
      "[5] (b) unable to meet or it has ceased paying its obligations as they generally bec\n",
      "[6] exceed its assets, is adjudicated insolvent, takes advantage of or is subject to\n",
      "[7] (b) its assets or it is unable to meet or it has ceased paying its obligations a\n",
      "[8] (b) assets, or it is unable to meet or it has ceased paying its obligations as t\n",
      "[9] (b) assets, or is adjudicated insolvent, or takes advantage of or is subject to \n",
      "[10] (b) assets, or is adjudicated insolvent, or takes advantage of or is subject to \n",
      "[ANSWER]\n",
      "Force majeure doesn't directly affect assets and insolvency. However, if a party is unable to fulfill its obligations due to a force majeure event for more than eight weeks, the other party may terminate the agreement [1]. If a party becomes insolvent, meaning its liabilities exceed its assets, or it can't meet its obligations, the other party may also terminate the agreement [2][3][4]. These terminations could potentially lead to insolvency proceedings, receivership, conservatorship, or liquidation [4][5][6].\n"
     ]
    }
   ],
   "source": [
    "query['_source'].append('shingles')\n",
    "do_query(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
