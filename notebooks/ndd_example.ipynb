{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed9dd0a2-6b12-436f-b6c3-02e0072115e3",
   "metadata": {},
   "source": [
    "##### This notebook shows the results of query-time near-duplicate detection (NDD).  It shows the same query with and without the duplicates.  The content here is inspired by this [blog post](https://www.aryn.ai/post/near-duplicate-detection-in-sycamore-what-is-it-good-for).\n",
    "\n",
    "\n",
    "##### The Aryn Partitioner in this job is configured to use the Aryn Partitioning Service to provide fast, GPU-powered performance. Go to [aryn.ai/sign-up ](aryn.ai/sign-up) to get a free API key for the service. This is the recommended configuration.\n",
    "\n",
    "##### You can also run the Aryn Partitioner locally by setting `use_partitioning_service` to `False`. Though you can use CPU to run the Aryn Partitioner, it is recommended to use an NVIDIA GPU for good performance.\n",
    "\n",
    "\n",
    "To use this notebook:\n",
    "1. Follow [these instructions](https://sycamore.readthedocs.io/en/stable/welcome_to_sycamore/get_started.html) and start the Sycamore containers using `docker compose up`.\n",
    "2. It's best to start with a clean slate by running `docker compose run reset`.\n",
    "3. Ingest the college credit card marketing agreements data.  The documents come from [data.gov](https://catalog.data.gov/dataset/college-credit-card-marketing-agreements-data), but we have made them accessible via a public S3 bucket.  There are two ingestion methods to choose from, depending on how much time is available:\n",
    "\n",
    "    - JSON: (minutes) ingest pre-processed data represented as JSON into OpenSearch\n",
    "    - PDF: (hours) fully process all ~2000 PDFs and ingest them into OpenSearch\n",
    "\n",
    "Set `use_json` below accordingly.  Also set `save_resources` as desired.\n",
    "\n",
    "The results should be the same for both methods, although there may be variations due to platform differences and OpenAI variation.\n",
    "\n",
    "More information about NDD can be found [here](https://sycamore.readthedocs.io/en/stable/querying_data/dedup.html).  Join our [Slack channel](https://join.slack.com/t/sycamore-ulj8912/shared_invite/zt-23sv0yhgy-MywV5dkVQ~F98Aoejo48Jg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf93d45-bee3-4202-8d5a-88497196db06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import warnings\n",
    "import urllib3\n",
    "import multiprocessing\n",
    "import pyarrow.fs\n",
    "import sycamore\n",
    "from sycamore.functions.tokenizer import HuggingFaceTokenizer\n",
    "from sycamore.transforms import COALESCE_WHITESPACE\n",
    "from sycamore.transforms.merge_elements import MarkedMerger\n",
    "from sycamore.transforms.partition import ArynPartitioner\n",
    "from sycamore.transforms.embed import SentenceTransformerEmbedder\n",
    "\n",
    "warnings.filterwarnings('ignore', category=urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c22b742-be10-421c-b98d-4cb45c57fc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sycamore.utils.aryn_config import ArynConfig, _DEFAULT_PATH\n",
    "assert ArynConfig.get_aryn_api_key() != \"\", f\"Unable to find aryn API key.  Looked in {_DEFAULT_PATH}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d9cc54-76cf-446b-a128-0836ba6e4d75",
   "metadata": {},
   "source": [
    "if the above assertion fails, you can either set the environment variable ARYN_API_KEY and restart jupyter\n",
    "or make a yaml file at the specified path in the assertion error that looks like:\n",
    "\n",
    "```\n",
    "aryn_token: \"YOUR-ARYN-API-KEY\"\n",
    "```\n",
    "\n",
    "It is unsafe, but if neither of those options work, you can put it in this notebook with\n",
    "```\n",
    "import os\n",
    "os.environ[\"ARYN_API_KEY\"] = \"UNSAFE-ARYN-API-KEY-LOCATION\" \n",
    "```\n",
    "\n",
    "but beware that it is easy to accidentally commit the notebook file and have it include your key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a467ebe-fbdd-40a8-8075-48c200f0b788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to False to ingest the PDFs from scratch, which takes an hour or more\n",
    "use_json = True\n",
    "\n",
    "# Set to False to use all available CPU and memory\n",
    "save_resources = True\n",
    "\n",
    "# Different hostnames inside and outside Docker compose environment\n",
    "opensearch_host = 'opensearch' if os.path.exists('/.dockerenv') else 'localhost'\n",
    "\n",
    "index_name = 'demoindex0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9291082e-cee5-465c-a15f-5cfdcc15a1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "osrch_args = {\n",
    "    'hosts': [{'host': opensearch_host, 'port': 9200}],\n",
    "    'http_compress': True,\n",
    "    'http_auth': ('admin', 'admin'),\n",
    "    'use_ssl': True,\n",
    "    'verify_certs': False,\n",
    "    'ssl_assert_hostname': False,\n",
    "    'ssl_show_warn': False,\n",
    "    'timeout': 120,\n",
    "}\n",
    "\n",
    "idx_settings = {\n",
    "    'body': {\n",
    "        'settings': {\n",
    "            'index.knn': True,\n",
    "        },\n",
    "        'mappings': {\n",
    "            'properties': {\n",
    "                'embedding': {\n",
    "                    'type': 'knn_vector',\n",
    "                    'dimension': 384,\n",
    "                    'method': {'name': 'hnsw', 'engine': 'faiss'},\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b890ef9-bfd9-46fe-b05b-13066ed525e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = HuggingFaceTokenizer('thenlper/gte-small')\n",
    "embedder = SentenceTransformerEmbedder(model_name='sentence-transformers/all-MiniLM-L6-v2', batch_size=100)\n",
    "\n",
    "fsys = pyarrow.fs.S3FileSystem(anonymous=True, region='us-east-1')\n",
    "ctx = sycamore.init()\n",
    "\n",
    "if use_json:\n",
    "    # Fast way: pre-processed DocSet as JSON...\n",
    "    path = 's3://aryn-public/cccmad-json'\n",
    "    ds = ctx.read.json_document(path, filesystem=fsys)\n",
    "else:\n",
    "    # Slow way: process PDF documents via Sycamore pipeline...\n",
    "    path = 's3://aryn-public/cccmad'\n",
    "    ds = (\n",
    "        ctx.read.binary(path, binary_format='pdf', filesystem=fsys)\n",
    "        .partition(partitioner=ArynPartitioner())\n",
    "        .regex_replace(COALESCE_WHITESPACE)\n",
    "        .mark_bbox_preset(tokenizer=tokenizer)\n",
    "        .merge(merger=MarkedMerger())\n",
    "        .spread_properties(['path'])\n",
    "        .split_elements(tokenizer=tokenizer, max_tokens=512)\n",
    "        .explode()\n",
    "        .sketch()\n",
    "        .embed(embedder=embedder)\n",
    "    )\n",
    "\n",
    "ds.write.opensearch(\n",
    "    os_client_args=osrch_args,\n",
    "    index_name=index_name,\n",
    "    index_settings=idx_settings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e0eb6c-2585-4c00-97ea-f7f7aa316ed5",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "The code below exists to retrieve the embedding model ID from OpenSearch.  This ID is different every time OpenSearch is set up.  We need to supply the ID in our query.  So, we will fetch it every time in order to be sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e9a476-134f-4901-a89e-3c91bdb8881c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_id():\n",
    "    query = {\n",
    "        'query': {\n",
    "            'bool': {\n",
    "                'must': [\n",
    "                    {\n",
    "                        'match': {'name': 'all-MiniLM-L6-v2'},\n",
    "                    },\n",
    "                    {\n",
    "                        'term': {'model_config.model_type': 'bert'},\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "    with requests.get(f'https://{opensearch_host}:9200/_plugins/_ml/models/_search', json=query, verify=False) as resp:\n",
    "        res = json.loads(resp.text)\n",
    "        return res['hits']['hits'][0]['_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94d8d48-a8b6-4704-a709-5cd022ed7343",
   "metadata": {},
   "source": [
    "<br><hr>\n",
    "This next function performs the given query and prints out both the top ten retrieved chunks and the AI-generated answer.  For clarity, the text chunks are truncated at 80 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a68af4e-843e-4c4e-b295-1a4e9ad55881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_query(query_dict):\n",
    "    url = f'https://{opensearch_host}:9200/{index_name}/_search?search_pipeline=hybrid_rag_pipeline'\n",
    "    with requests.post(url, json=query, verify=False) as resp:\n",
    "        res = json.loads(resp.text)\n",
    "        hits = res['hits']['hits']\n",
    "        for i in range(10):\n",
    "            text = hits[i]['_source']['text_representation']\n",
    "            text = text.replace('\\n', ' ')[:80]\n",
    "            print(f'[{i+1}] {text}')\n",
    "        answer = res['ext']['retrieval_augmented_generation']['answer']\n",
    "        print(f'[ANSWER]\\n{answer}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a71039-0a85-4e14-b42a-0969d04d979a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "First, we run the query without near-duplicate-detection.  We do this by not asking for `shingles` in `_source`.  In OpenSearch queries, the `_source` is where we list the fields that we want to retrieve for each hit.\n",
    "\n",
    "If everything is set up and running properly, the numbered results will contain many repeated lines.  There is only one document in the top 10 (the RAG context).  The resulting generated answer starts by saying no information was found and then goes on to summarize the single source.  The answer doesn't reflect the breadth of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b45b71-75ba-4a88-b3d0-41563dba91a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = 'how does force majeure affect assets and insolvency'\n",
    "query = {\n",
    "    '_source': [\n",
    "        'text_representation',\n",
    "    ],\n",
    "    'query': {\n",
    "        'hybrid': {\n",
    "            'queries': [\n",
    "                {\n",
    "                    'match': {'text_representation': query_str},\n",
    "                },\n",
    "                {\n",
    "                    'neural': {\n",
    "                        'embedding': {\n",
    "                            'query_text': query_str,\n",
    "                            'k': 100,\n",
    "                            'model_id': get_model_id(),\n",
    "                        },\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    },\n",
    "    'ext': {\n",
    "        'generative_qa_parameters': {\n",
    "            'llm_question': query_str,\n",
    "            'context_size': 10,\n",
    "            'llm_model': 'gpt-4',\n",
    "        },\n",
    "    },\n",
    "    'size': 100,\n",
    "}\n",
    "do_query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ba7b19-4c11-4a00-bada-fe367bb212fd",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "For the next query, we re-use the previous query data structure, but we modify it slightly.  We append `shingles` to the list of fields to be retrieved.  This enables NDD processing; without `shingles` it can't detect near-duplicates.  Now, when we run the query there is much more diversity in the retrieved chunks.  There appear to be four unique chunks after NDD.  Looking at the generated answer, there are more cited sources and the explanation is richer.  It specifically addresses insolvency, which was part of the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63234b4-fb51-4f07-8033-4e54f634d7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "query['_source'].append('shingles')\n",
    "do_query(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
