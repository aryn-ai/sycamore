{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "This notebook shows the results of query-time near-duplicate detection (NDD).  It shows the same query with and without the duplicates.  The content here is inspired by this [blog post](https://www.aryn.ai/post/near-duplicate-detection-in-sycamore-what-is-it-good-for).\n",
    "\n",
    "To use this notebook:\n",
    "1. Follow [these instructions](https://sycamore.readthedocs.io/en/stable/welcome_to_sycamore/get_started.html) and start the Sycamore containers using `docker compose up`.\n",
    "2. Make sure to start with a clean slate by running `docker compose run reset`.\n",
    "3. Ingest the college credit card marketing agreements data.  The documents come from [data.gov](https://catalog.data.gov/dataset/college-credit-card-marketing-agreements-data), but we have made them accessible via S3.  There are two ingestion commands to choose from, depending on how much time is available:\n",
    "\n",
    "    - 35 PDFs needed for demo (minutes): `docker compose run sycamore_crawler_s3 aryn-public cccmad-tiny`\n",
    "    - All ~2000 PDFs (4+ hours): `docker compose run sycamore_crawler_s3 aryn-public cccmad`\n",
    "\n",
    "The results below are from the 35-document dataset, which we think most users will choose.  The full dataset will provide different results.  There may be variations even in the small dataset due to platform differences and OpenAI variation.  It's not necessary for 100% of the documents to be ingested in order to run this example.  Once the lion's share completes, it's OK to proceed.\n",
    "\n",
    "More information about NDD can be found [here](https://sycamore.readthedocs.io/en/stable/querying_data/dedup.html).  Join our [Slack channel](https://join.slack.com/t/sycamore-ulj8912/shared_invite/zt-23sv0yhgy-MywV5dkVQ~F98Aoejo48Jg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import warnings\n",
    "import urllib3\n",
    "warnings.filterwarnings(\"ignore\", category=urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "The code below exists to retrieve the embedding model ID from OpenSearch.  This ID is different every time OpenSearch is set up.  We need to supply the ID in our query.  So, we need to fetch it every time in order to be sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_id():\n",
    "    query = {\n",
    "        'query': {\n",
    "            'bool': {\n",
    "                'must': [\n",
    "                    {\n",
    "                        'match': {'name': 'all-MiniLM-L6-v2'},\n",
    "                    },\n",
    "                    {\n",
    "                        'term': {'model_config.model_type': 'bert'},\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "    with requests.get('https://opensearch:9200/_plugins/_ml/models/_search', json=query, verify=False) as resp:\n",
    "        res = json.loads(resp.text)\n",
    "        return res['hits']['hits'][0]['_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "<br><hr>\n",
    "This next function performs the supplied query and prints out both the retrieved chunks and the AI-generated answer.  For clarity, the text chunks are truncated at 80 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_query(query_dict):\n",
    "    url = 'https://opensearch:9200/demoindex0/_search?search_pipeline=hybrid_rag_pipeline'\n",
    "    with requests.post(url, json=query, verify=False) as resp:\n",
    "        res = json.loads(resp.text)\n",
    "        hits = res['hits']['hits']\n",
    "        for i in range(10):\n",
    "            text = hits[i]['_source']['text_representation']\n",
    "            text = text.replace('\\n', ' ')[:80]\n",
    "            print(f'[{i+1}] {text}')\n",
    "        answer = res['ext']['retrieval_augmented_generation']['answer']\n",
    "        print(f'[ANSWER]\\n{answer}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "First, we run the query without near-duplicate-detection.  We do this by not asking for `shingles` in `_source`.  In OpenSearch, the `_source` is where we list the fields that we want to retrieve for each hit.\n",
    "\n",
    "If everything is set up and running properly, the numbered results will contain many repeated lines.  In the small dataset, there are only two documents in the top 5 (the RAG context).  The resulting generated answer starts by denying the premise of the question and then goes on to summarize one source.  The answer doesn't reflect the breadth of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = 'how are liabilities and assets affected by force majeure'\n",
    "query = {\n",
    "    '_source': [\n",
    "        'text_representation',\n",
    "    ],\n",
    "    'query': {\n",
    "        'hybrid': {\n",
    "            'queries': [\n",
    "                {\n",
    "                    'match': {'text_representation': query_str},\n",
    "                },\n",
    "                {\n",
    "                    'neural': {\n",
    "                        'embedding': {\n",
    "                            'query_text': query_str,\n",
    "                            'k': 100,\n",
    "                            'model_id': get_model_id(),\n",
    "                        },\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    },\n",
    "    'ext': {\n",
    "        'generative_qa_parameters': {\n",
    "            'llm_question': query_str,\n",
    "            'context_size': 5,\n",
    "            'llm_model': 'gpt-4',\n",
    "        },\n",
    "    },\n",
    "    'size': 100,\n",
    "}\n",
    "do_query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "For the next query, we re-use the previous query data structure, but we modify it slightly.  We append `shingles` to the list of fields to be retrieved.  This enables NDD processing; without `shingles` it can't detect near-duplicates.  Now, when we run the query there is much more diversity in the retrieved chunks.  When run on the small dataset, there appears to be just one duplicate that gets past NDD and it appears to be some sort of revision.  Looking at the generated answer, there are more cited sources and the explanation is richer.  It covers topics like modifying obligations and prior obligations, not included above.  Crucially, it mentions assets twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query['_source'].append('shingles')\n",
    "do_query(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
