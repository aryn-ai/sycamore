{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.fs\n",
    "from ray.data import ActorPoolStrategy\n",
    "import sycamore\n",
    "import json\n",
    "from sycamore.functions.tokenizer import HuggingFaceTokenizer\n",
    "from sycamore.llms import OpenAIModels, OpenAI\n",
    "from sycamore.transforms import COALESCE_WHITESPACE\n",
    "from sycamore.transforms.merge_elements import MarkedMerger\n",
    "from sycamore.transforms.partition import SycamorePartitioner\n",
    "from sycamore.transforms.extract_schema import OpenAISchemaExtractor, OpenAIPropertyExtractor\n",
    "from sycamore.transforms.embed import SentenceTransformerEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sycamore.data.document import Document\n",
    "from dateutil import parser\n",
    "def convert_timestamp(doc: Document) -> Document:\n",
    "    if \"dateAndTime\" not in doc.properties['entity'] and \"dateTime\" not in doc.properties['entity']:\n",
    "        return doc\n",
    "    raw_date: str = doc.properties['entity'].get('dateAndTime') or doc.properties['entity'].get('dateTime')\n",
    "    raw_date = raw_date.replace(\"Local\", \"\")\n",
    "    parsed_date = parser.parse(raw_date, fuzzy=True)\n",
    "    extracted_date = parsed_date.date()\n",
    "    doc.properties['entity']['day'] = extracted_date.day\n",
    "    doc.properties['entity']['month'] = extracted_date.month\n",
    "    doc.properties['entity']['year'] = extracted_date.year\n",
    "    if parsed_date.utcoffset():\n",
    "        doc.properties['entity']['dateTime'] = parsed_date.isoformat()\n",
    "    else:\n",
    "        doc.properties['entity']['dateTime'] = parsed_date.isoformat() + \"Z\"\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"s3://aryn-public/ntsb/\"]\n",
    "fsys = pyarrow.fs.S3FileSystem(region=\"us-east-1\", anonymous=True)\n",
    "\n",
    "llm = OpenAI(OpenAIModels.GPT_3_5_TURBO.value)\n",
    "tokenizer = HuggingFaceTokenizer(\"thenlper/gte-small\")\n",
    "\n",
    "ctx = sycamore.init()\n",
    "\n",
    "ds = (\n",
    "    ctx.read.binary(paths, binary_format=\"pdf\", filesystem=fsys)\n",
    "    # Parition with the sycamore partitioner, pulling out tables and images. ActorPoolStrategy(size=3) works best on my particular hardware\n",
    "    # but your mileage may vary depending on your RAM.\n",
    "    .partition(partitioner=SycamorePartitioner(extract_table_structure=True, extract_images=True, use_cache=False), compute=ActorPoolStrategy(size=3))\n",
    "    # Get rid of spurious whitespace charaters\n",
    "    .regex_replace(COALESCE_WHITESPACE)\n",
    "    # Automatically determine a schema of additional metadata to extract from documents\n",
    "    .extract_batch_schema(schema_extractor=OpenAISchemaExtractor(\"FlightAccidentReport\", llm=llm, num_of_elements=35))\n",
    "    # Extract the metadata specified by that schema\n",
    "    .extract_properties(property_extractor=OpenAIPropertyExtractor(llm=llm, num_of_elements=35))\n",
    "    # Merge elements into larger chunks\n",
    "    .mark_bbox_preset(tokenizer=tokenizer)\n",
    "    .merge(merger=MarkedMerger())\n",
    "    # Convert extracted timestamps to better-structured form using the function above\n",
    "    .map(convert_timestamp)\n",
    "    # Copy document properties to each document's sub-elements\n",
    "    .spread_properties([\"path\", \"entity\"])\n",
    "    # Split elements that are too big to embed\n",
    "    .split_elements(tokenizer=tokenizer, max_tokens=512)\n",
    "    # Convert all Elements to Documents\n",
    "    .explode()\n",
    "    # Generate a series of hashes to represent each document. For use with near-duplicate detection\n",
    "    .sketch()\n",
    "    # Embed each document\n",
    "    .embed(embedder=SentenceTransformerEmbedder(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", batch_size=100))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to a persistent duckdb database\n",
    "# - into a specific database (as specified by url) \n",
    "# - into a specific table (as specified by table name)\n",
    "persistent_db = \"demo.db\"\n",
    "persistent_table = \"demo_table\"\n",
    "ds.write.duckdb(\n",
    "    db_url=persistent_db,\n",
    "    table_name=persistent_table,\n",
    "    dimensions=384\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We connect to the DuckDB to perform operations\n",
    "import duckdb\n",
    "data_conn = duckdb.connect(\"demo.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DuckDB is not production ready yet for efficient Vector Similarity Search on persistent databases (https://duckdb.org/docs/extensions/vss.html), \n",
    "# so we load our data into an in-memory database for this demo. In future versions, once DuckDB productionizes this feature, there will be no more need for this code cell\n",
    "\n",
    "# Load from disk into Pandas Dataframe as an intermediate step\n",
    "df = data_conn.execute(\"SELECT * FROM demo_table\").fetchdf()\n",
    "df.dropna(subset=[\"embeddings\"],inplace=True)\n",
    "\n",
    "\n",
    "# Now, we load the data into an in-memory database. Notice that we specify beforehand since we need the embedding column (must be specified as a FLOAT[N] where N is specified) \n",
    "# for Vector Similarity Search using HNSW to work.\n",
    "in_memory_db = duckdb.connect(\":default:\")\n",
    "in_memory_db.install_extension(\"vss\")\n",
    "in_memory_db.load_extension(\"vss\")\n",
    "\n",
    "# To allow RAG to work properly on this database using Langchain (shown later), we must rename the properties column to metadata, \n",
    "# add a source key within it, and make it of type later. Note that the value 'default' can be modified to other options if needed\n",
    "new_key = 'source'\n",
    "new_value = 'default'\n",
    "schema = {\n",
    "            \"doc_id\": \"VARCHAR\",\n",
    "            \"embeddings\": \"FLOAT[384]\",\n",
    "            \"metadata\": \"VARCHAR\",\n",
    "            \"text_representation\": \"VARCHAR\",\n",
    "            \"bbox\": \"DOUBLE[]\",\n",
    "            \"shingles\": \"BIGINT[]\",\n",
    "            \"type\": \"VARCHAR\",\n",
    "        }\n",
    "df['properties'] = df['properties'].apply(lambda x: dict(zip(x['key'], x['value'])))\n",
    "df['properties'] = df['properties'].apply(lambda x: {**x, new_key: new_value}).apply(lambda x: json.dumps(x))\n",
    "df = df.rename(columns={'properties': 'metadata'})\n",
    "in_memory_db.execute(f\"\"\"CREATE TABLE in_memory_table (doc_id {schema.get('doc_id')},\n",
    "                      embeddings {schema.get('embeddings')}, metadata {schema.get('metadata')}, \n",
    "                      text_representation {schema.get('text_representation')}, bbox {schema.get('bbox')}, \n",
    "                      shingles {schema.get('shingles')}, type {schema.get('type')})\"\"\"\n",
    "    )\n",
    "in_memory_db.execute(\"\"\"INSERT INTO in_memory_table SELECT * FROM df; \n",
    "                      CREATE INDEX in_memory_table_index ON in_memory_table USING HNSW(embeddings)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For queries, let's define an embedding function for the question that helps us easily compare the two vectors and generate optimal results // Run NN search\n",
    "from sentence_transformers import SentenceTransformer\n",
    "minilm = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "class Embedder():\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "    \n",
    "    def embed_query(self, question):\n",
    "        v = self.llm.encode(question).tolist()\n",
    "        return v\n",
    "\n",
    "embedder = Embedder(minilm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use DuckDB's ANN HNSW semantic search for retrieval. We use Array Euclidean Distance as the nearest neighbor lookup\n",
    "q = \"What automobile type is the most accident prone?\"\n",
    "in_memory_db.sql(f\"SELECT doc_id, text_representation, metadata FROM in_memory_table ORDER BY array_distance(embeddings, {embedder.embed_query(q)}::FLOAT[384]) LIMIT 10;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty print tables\n",
    "from tabulate import tabulate\n",
    "\n",
    "q = \"What automobile type is the most accident prone?\"\n",
    "result = in_memory_db.sql(f\"SELECT doc_id, text_representation, metadata FROM in_memory_table ORDER BY array_distance(embeddings, {embedder.embed_query(q)}::FLOAT[384]) LIMIT 10;\")\n",
    "\n",
    "# Convert the result to a list of lists\n",
    "table_data = result.fetchall()\n",
    "\n",
    "# Get column names\n",
    "headers = [desc[0] for desc in result.description]\n",
    "\n",
    "# Pretty print the table\n",
    "print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first two rows of the text representation to visually confirm results \n",
    "\n",
    "first_two = table_data[2:4]\n",
    "\n",
    "print(\"Text representation of the first two values (not including headers):\")\n",
    "for i, row in enumerate(first_two, 1):\n",
    "    print(f\"\\n{i}. doc_id: {row[0]}\")\n",
    "    print(f\"   Text representation:\\n   {row[1]}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also see the performance of the DuckDB query and check the index is being used by using the in-built visualization below \n",
    "q = \"What automobile type is the most accident prone?\"\n",
    "explain_result = in_memory_db.sql(f\"EXPLAIN SELECT doc_id, text_representation, metadata FROM in_memory_table ORDER BY array_distance(embeddings, {embedder.embed_query(q)}::FLOAT[384]) LIMIT 10\")\n",
    "\n",
    "# Convert the result to a list of lists\n",
    "explain_data = explain_result.fetchall()\n",
    "\n",
    "# Get column names\n",
    "headers = [desc[0] for desc in explain_result.description]\n",
    "\n",
    "# Pretty print the EXPLAIN output\n",
    "print(\"Expanded EXPLAIN output:\")\n",
    "print(tabulate(explain_data, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now initialize a vector store on DuckDB using the LangChain integration. We perform similarity search using the integration here\n",
    "from langchain_community.vectorstores.duckdb import DuckDB\n",
    "vector_store = DuckDB(connection=in_memory_db, embedding=embedder, id_key= \"doc_id\", text_key=\"text_representation\", vector_key=\"embeddings\", table_name=\"in_memory_table\")\n",
    "result = vector_store.similarity_search('traffic')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI  \n",
    "from langchain.chains import RetrievalQAWithSourcesChain  \n",
    "import os\n",
    "# Finally, we initialize a RAG agent and ask a model question of the data\n",
    "llm = ChatOpenAI(  \n",
    "    openai_api_key=os.environ.get(\"OPENAI_API_KEY\"),  \n",
    "    model_name='gpt-3.5-turbo',  \n",
    "    temperature=0.8  \n",
    ")  \n",
    "qa = RetrievalQAWithSourcesChain.from_chain_type(  \n",
    "    llm=llm,  \n",
    "    chain_type=\"stuff\",  \n",
    "    retriever=vector_store.as_retriever() , verbose=True\n",
    ")  \n",
    "qa.invoke({\"question\": \"How many accidents happened?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally we can build an index with other distance metrics, the default is Euclidean Distance ('l2sq'), with Cosine similarity ('cosine', 'array_cosine_simarlity') \n",
    "# and Inner Product ('ip', 'array_inner_product') also supported\n",
    "in_memory_db.execute(\"CREATE INDEX in_memory_ip_index ON in_memory_table USING HNSW(embeddings) WITH (metric = 'ip')\")\n",
    "in_memory_db.sql(f\"SELECT doc_id, text_representation, metadata FROM in_memory_table ORDER BY array_inner_product(embeddings, {embedder.embed_query(q)}::FLOAT[384]) LIMIT 10;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: If you would like to remove your database after running the code above, please run this code cell\n",
    "try:\n",
    "    os.unlink(persistent_db)\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting {persistent_db}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sycamore-ai--zTjaFUY-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
