{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.fs\n",
    "from ray.data import ActorPoolStrategy\n",
    "import sycamore\n",
    "from sycamore.functions.tokenizer import HuggingFaceTokenizer\n",
    "from sycamore.llms import OpenAIModels, OpenAI\n",
    "from sycamore.transforms import COALESCE_WHITESPACE\n",
    "from sycamore.transforms.merge_elements import MarkedMerger\n",
    "from sycamore.transforms.partition import SycamorePartitioner\n",
    "from sycamore.transforms.extract_schema import OpenAISchemaExtractor, OpenAIPropertyExtractor\n",
    "from sycamore.transforms.embed import SentenceTransformerEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sycamore.data.document import Document\n",
    "from dateutil import parser\n",
    "def convert_timestamp(doc: Document) -> Document:\n",
    "    if \"dateAndTime\" not in doc.properties['entity'] and \"dateTime\" not in doc.properties['entity']:\n",
    "        return doc\n",
    "    raw_date: str = doc.properties['entity'].get('dateAndTime') or doc.properties['entity'].get('dateTime')\n",
    "    raw_date = raw_date.replace(\"Local\", \"\")\n",
    "    parsed_date = parser.parse(raw_date, fuzzy=True)\n",
    "    extracted_date = parsed_date.date()\n",
    "    doc.properties['entity']['day'] = extracted_date.day\n",
    "    doc.properties['entity']['month'] = extracted_date.month\n",
    "    doc.properties['entity']['year'] = extracted_date.year\n",
    "    if parsed_date.utcoffset():\n",
    "        doc.properties['entity']['dateTime'] = parsed_date.isoformat()\n",
    "    else:\n",
    "        doc.properties['entity']['dateTime'] = parsed_date.isoformat() + \"Z\"\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"s3://aryn-public/ntsb/\"]\n",
    "fsys = pyarrow.fs.S3FileSystem(region=\"us-east-1\", anonymous=True)\n",
    "\n",
    "llm = OpenAI(OpenAIModels.GPT_3_5_TURBO.value)\n",
    "tokenizer = HuggingFaceTokenizer(\"thenlper/gte-small\")\n",
    "\n",
    "ctx = sycamore.init()\n",
    "\n",
    "ds = (\n",
    "    ctx.read.binary(paths, binary_format=\"pdf\", filesystem=fsys)\n",
    "    # Parition with the sycamore partitioner, pulling out tables and images. ActorPoolStrategy(size=3) works best on my particular hardware\n",
    "    # but your mileage may vary depending on your RAM.\n",
    "    .partition(partitioner=SycamorePartitioner(extract_table_structure=True, extract_images=True), compute=ActorPoolStrategy(size=3))\n",
    "    # Get rid of spurious whitespace charaters\n",
    "    .regex_replace(COALESCE_WHITESPACE)\n",
    "    # Automatically determine a schema of additional metadata to extract from documents\n",
    "    .extract_batch_schema(schema_extractor=OpenAISchemaExtractor(\"FlightAccidentReport\", llm=llm, num_of_elements=35))\n",
    "    # Extract the metadata specified by that schema\n",
    "    .extract_properties(property_extractor=OpenAIPropertyExtractor(llm=llm, num_of_elements=35))\n",
    "    # Merge elements into larger chunks\n",
    "    .mark_bbox_preset(tokenizer=tokenizer)\n",
    "    .merge(merger=MarkedMerger())\n",
    "    # Convert extracted timestamps to better-structured form using the function above\n",
    "    .map(convert_timestamp)\n",
    "    # Copy document properties to each document's sub-elements\n",
    "    .spread_properties([\"path\", \"entity\"])\n",
    "    # Split elements that are too big to embed\n",
    "    .split_elements(tokenizer=tokenizer, max_tokens=512)\n",
    "    # Convert all Elements to Documents\n",
    "    .explode()\n",
    "    # Generate a series of hashes to represent each document. For use with near-duplicate detection\n",
    "    .sketch()\n",
    "    # Embed each document\n",
    "    .embed(embedder=SentenceTransformerEmbedder(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", batch_size=100))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-01 13:32:35,656\tWARNING util.py:560 -- The argument ``compute`` is deprecated in Ray 2.9. Please specify argument ``concurrency`` instead. For more information, see https://docs.ray.io/en/master/data/transforming-data.html#stateful-transforms.\n",
      "2024-07-01 13:32:35,658\tWARNING util.py:560 -- The argument ``compute`` is deprecated in Ray 2.9. Please specify argument ``concurrency`` instead. For more information, see https://docs.ray.io/en/master/data/transforming-data.html#stateful-transforms.\n",
      "2024-07-01 13:32:35,662\tWARNING util.py:560 -- The argument ``compute`` is deprecated in Ray 2.9. Please specify argument ``concurrency`` instead. For more information, see https://docs.ray.io/en/master/data/transforming-data.html#stateful-transforms.\n",
      "2024-07-01 13:32:35,667\tINFO streaming_executor.py:112 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2024-07-01_12-05-17_251742_53329/logs/ray-data\n",
      "2024-07-01 13:32:35,667\tINFO streaming_executor.py:113 -- Execution plan of Dataset: InputDataBuffer[Input] -> ActorPoolMapOperator[ReadBinary->Map(BinaryScan._to_document)->MapBatches(BaseMapTransformCallable___wrap)] -> ActorPoolMapOperator[MapBatches(regex_replace)->MapBatches(BaseMapTransformCustom__Extract)] -> ActorPoolMapOperator[MapBatches(extract_properties)->MapBatches(sort_by_page_bbox)->MapBatches(mark_drop_tiny)->MapBatches(mark_drop_header_and_footer)->MapBatches(mark_break_page)->MapBatches(mark_break_by_column)->MapBatches(mark_break_by_tokens)->MapBatches(merge_elements)->MapBatches(convert_timestamp)->MapBatches(spread_properties)->MapBatches(split_doc)->MapBatches(explode)->MapBatches(sketcher)->MapBatches(SentenceTransformerEmbedder)->MapBatches(BaseMapTransformCallable__duckdb_write_documents)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b51419744b4dd39f7611df774b97fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- ReadBinary->Map(BinaryScan._to_document)->MapBatches(BaseMapTransformCallable___wrap) 1:   0%|          | 0/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35be232621094ea490a27d1d350cf9d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(regex_replace)->MapBatches(BaseMapTransformCustom__Extract) 2:   0%|          | 0/65 [00:00<?, ?i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bbafdeb98f0424080a0c60f8558aa25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(extract_properties)->MapBatches(sort_by_page_bbox)->MapBatches(mark_drop_tiny)->MapBatches(mark_d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff9244f6fc674bbf892972e55c4cff11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(MapWorker(ReadBinary->Map(BinaryScan._to_document)->MapBatches(BaseMapTransformCallable___wrap)) pid=54460) The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>e)->MapBatches(BaseMapTransformCustom__Extract)) pid=54463) \n",
      "<IPython.core.display.HTML object>e)->MapBatches(BaseMapTransformCustom__Extract)) pid=54463) \n",
      "<IPython.core.display.HTML object>e)->MapBatches(BaseMapTransformCustom__Extract)) pid=54463) \n",
      "<IPython.core.display.HTML object>e)->MapBatches(BaseMapTransformCustom__Extract)) pid=54463) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(MapWorker(ReadBinary->Map(BinaryScan._to_document)->MapBatches(BaseMapTransformCallable___wrap)) pid=54462) The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n",
      "(MapWorker(ReadBinary->Map(BinaryScan._to_document)->MapBatches(BaseMapTransformCallable___wrap)) pid=54461) The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>erties)->MapBatches(sort_by_page_bbox)->MapBatches(mark_drop_tiny)->MapBatches(mark_drop_header_and_footer)->MapBatches(mark_break_page)->MapBatches(mark_break_by_column)->MapBatches(mark_break_by_tokens)->MapBatches(merge_elements)->MapBatches(convert_timestamp)->MapBatches(spread_properties)->MapBatches(split_doc)->MapBatches(explode)->MapBatches(sketcher)->MapBatches(SentenceTransformerEmbedder)->MapBatches(BaseMapTransformCallable__duckdb_write_documents)) pid=54464)  [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\n",
      "<IPython.core.display.HTML object>erties)->MapBatches(sort_by_page_bbox)->MapBatches(mark_drop_tiny)->MapBatches(mark_drop_header_and_footer)->MapBatches(mark_break_page)->MapBatches(mark_break_by_column)->MapBatches(mark_break_by_tokens)->MapBatches(merge_elements)->MapBatches(convert_timestamp)->MapBatches(spread_properties)->MapBatches(split_doc)->MapBatches(explode)->MapBatches(sketcher)->MapBatches(SentenceTransformerEmbedder)->MapBatches(BaseMapTransformCallable__duckdb_write_documents)) pid=54464)  [repeated 60x across cluster]\n",
      "<IPython.core.display.HTML object>erties)->MapBatches(sort_by_page_bbox)->MapBatches(mark_drop_tiny)->MapBatches(mark_drop_header_and_footer)->MapBatches(mark_break_page)->MapBatches(mark_break_by_column)->MapBatches(mark_break_by_tokens)->MapBatches(merge_elements)->MapBatches(convert_timestamp)->MapBatches(spread_properties)->MapBatches(split_doc)->MapBatches(explode)->MapBatches(sketcher)->MapBatches(SentenceTransformerEmbedder)->MapBatches(BaseMapTransformCallable__duckdb_write_documents)) pid=54464)  [repeated 53x across cluster]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(MapWorker(MapBatches(extract_properties)->MapBatches(sort_by_page_bbox)->MapBatches(mark_drop_tiny)->MapBatches(mark_drop_header_and_footer)->MapBatches(mark_break_page)->MapBatches(mark_break_by_column)->MapBatches(mark_break_by_tokens)->MapBatches(merge_elements)->MapBatches(convert_timestamp)->MapBatches(spread_properties)->MapBatches(split_doc)->MapBatches(explode)->MapBatches(sketcher)->MapBatches(SentenceTransformerEmbedder)->MapBatches(BaseMapTransformCallable__duckdb_write_documents)) pid=54464) /Users/karansampath/Library/Caches/pypoetry/virtualenvs/sycamore-ai--zTjaFUY-py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "(MapWorker(MapBatches(extract_properties)->MapBatches(sort_by_page_bbox)->MapBatches(mark_drop_tiny)->MapBatches(mark_drop_header_and_footer)->MapBatches(mark_break_page)->MapBatches(mark_break_by_column)->MapBatches(mark_break_by_tokens)->MapBatches(merge_elements)->MapBatches(convert_timestamp)->MapBatches(spread_properties)->MapBatches(split_doc)->MapBatches(explode)->MapBatches(sketcher)->MapBatches(SentenceTransformerEmbedder)->MapBatches(BaseMapTransformCallable__duckdb_write_documents)) pid=54464)   warnings.warn(\n",
      "(MapWorker(MapBatches(extract_properties)->MapBatches(sort_by_page_bbox)->MapBatches(mark_drop_tiny)->MapBatches(mark_drop_header_and_footer)->MapBatches(mark_break_page)->MapBatches(mark_break_by_column)->MapBatches(mark_break_by_tokens)->MapBatches(merge_elements)->MapBatches(convert_timestamp)->MapBatches(spread_properties)->MapBatches(split_doc)->MapBatches(explode)->MapBatches(sketcher)->MapBatches(SentenceTransformerEmbedder)->MapBatches(BaseMapTransformCallable__duckdb_write_documents)) pid=54464) huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "(MapWorker(MapBatches(extract_properties)->MapBatches(sort_by_page_bbox)->MapBatches(mark_drop_tiny)->MapBatches(mark_drop_header_and_footer)->MapBatches(mark_break_page)->MapBatches(mark_break_by_column)->MapBatches(mark_break_by_tokens)->MapBatches(merge_elements)->MapBatches(convert_timestamp)->MapBatches(spread_properties)->MapBatches(split_doc)->MapBatches(explode)->MapBatches(sketcher)->MapBatches(SentenceTransformerEmbedder)->MapBatches(BaseMapTransformCallable__duckdb_write_documents)) pid=54464) To disable this warning, you can either:\n",
      "(MapWorker(MapBatches(extract_properties)->MapBatches(sort_by_page_bbox)->MapBatches(mark_drop_tiny)->MapBatches(mark_drop_header_and_footer)->MapBatches(mark_break_page)->MapBatches(mark_break_by_column)->MapBatches(mark_break_by_tokens)->MapBatches(merge_elements)->MapBatches(convert_timestamp)->MapBatches(spread_properties)->MapBatches(split_doc)->MapBatches(explode)->MapBatches(sketcher)->MapBatches(SentenceTransformerEmbedder)->MapBatches(BaseMapTransformCallable__duckdb_write_documents)) pid=54464) \t- Avoid using `tokenizers` before the fork if possible\n",
      "(MapWorker(MapBatches(extract_properties)->MapBatches(sort_by_page_bbox)->MapBatches(mark_drop_tiny)->MapBatches(mark_drop_header_and_footer)->MapBatches(mark_break_page)->MapBatches(mark_break_by_column)->MapBatches(mark_break_by_tokens)->MapBatches(merge_elements)->MapBatches(convert_timestamp)->MapBatches(spread_properties)->MapBatches(split_doc)->MapBatches(explode)->MapBatches(sketcher)->MapBatches(SentenceTransformerEmbedder)->MapBatches(BaseMapTransformCallable__duckdb_write_documents)) pid=54464) \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>erties)->MapBatches(sort_by_page_bbox)->MapBatches(mark_drop_tiny)->MapBatches(mark_drop_header_and_footer)->MapBatches(mark_break_page)->MapBatches(mark_break_by_column)->MapBatches(mark_break_by_tokens)->MapBatches(merge_elements)->MapBatches(convert_timestamp)->MapBatches(spread_properties)->MapBatches(split_doc)->MapBatches(explode)->MapBatches(sketcher)->MapBatches(SentenceTransformerEmbedder)->MapBatches(BaseMapTransformCallable__duckdb_write_documents)) pid=54464)  [repeated 12x across cluster]\n",
      "<IPython.core.display.HTML object>erties)->MapBatches(sort_by_page_bbox)->MapBatches(mark_drop_tiny)->MapBatches(mark_drop_header_and_footer)->MapBatches(mark_break_page)->MapBatches(mark_break_by_column)->MapBatches(mark_break_by_tokens)->MapBatches(merge_elements)->MapBatches(convert_timestamp)->MapBatches(spread_properties)->MapBatches(split_doc)->MapBatches(explode)->MapBatches(sketcher)->MapBatches(SentenceTransformerEmbedder)->MapBatches(BaseMapTransformCallable__duckdb_write_documents)) pid=54464)  [repeated 51x across cluster]\n",
      "(MapWorker(MapBatches(extract_properties)->MapBatches(sort_by_page_bbox)->MapBatches(mark_drop_tiny)->MapBatches(mark_drop_header_and_footer)->MapBatches(mark_break_page)->MapBatches(mark_break_by_column)->MapBatches(mark_break_by_tokens)->MapBatches(merge_elements)->MapBatches(convert_timestamp)->MapBatches(spread_properties)->MapBatches(split_doc)->MapBatches(explode)->MapBatches(sketcher)->MapBatches(SentenceTransformerEmbedder)->MapBatches(BaseMapTransformCallable__duckdb_write_documents)) pid=54464) Error creating table demo_table in database demo.db: Catalog Error: Table with name \"demo_table\" already exists!\n",
      "<IPython.core.display.HTML object>erties)->MapBatches(sort_by_page_bbox)->MapBatches(mark_drop_tiny)->MapBatches(mark_drop_header_and_footer)->MapBatches(mark_break_page)->MapBatches(mark_break_by_column)->MapBatches(mark_break_by_tokens)->MapBatches(merge_elements)->MapBatches(convert_timestamp)->MapBatches(spread_properties)->MapBatches(split_doc)->MapBatches(explode)->MapBatches(sketcher)->MapBatches(SentenceTransformerEmbedder)->MapBatches(BaseMapTransformCallable__duckdb_write_documents)) pid=54464)  [repeated 50x across cluster]\n",
      "<IPython.core.display.HTML object>erties)->MapBatches(sort_by_page_bbox)->MapBatches(mark_drop_tiny)->MapBatches(mark_drop_header_and_footer)->MapBatches(mark_break_page)->MapBatches(mark_break_by_column)->MapBatches(mark_break_by_tokens)->MapBatches(merge_elements)->MapBatches(convert_timestamp)->MapBatches(spread_properties)->MapBatches(split_doc)->MapBatches(explode)->MapBatches(sketcher)->MapBatches(SentenceTransformerEmbedder)->MapBatches(BaseMapTransformCallable__duckdb_write_documents)) pid=54464)  [repeated 3x across cluster]\n",
      "(MapWorker(MapBatches(extract_properties)->MapBatches(sort_by_page_bbox)->MapBatches(mark_drop_tiny)->MapBatches(mark_drop_header_and_footer)->MapBatches(mark_break_page)->MapBatches(mark_break_by_column)->MapBatches(mark_break_by_tokens)->MapBatches(merge_elements)->MapBatches(convert_timestamp)->MapBatches(spread_properties)->MapBatches(split_doc)->MapBatches(explode)->MapBatches(sketcher)->MapBatches(SentenceTransformerEmbedder)->MapBatches(BaseMapTransformCallable__duckdb_write_documents)) pid=54464) Error creating table demo_table in database demo.db: Catalog Error: Table with name \"demo_table\" already exists!\n",
      "<IPython.core.display.HTML object>erties)->MapBatches(sort_by_page_bbox)->MapBatches(mark_drop_tiny)->MapBatches(mark_drop_header_and_footer)->MapBatches(mark_break_page)->MapBatches(mark_break_by_column)->MapBatches(mark_break_by_tokens)->MapBatches(merge_elements)->MapBatches(convert_timestamp)->MapBatches(spread_properties)->MapBatches(split_doc)->MapBatches(explode)->MapBatches(sketcher)->MapBatches(SentenceTransformerEmbedder)->MapBatches(BaseMapTransformCallable__duckdb_write_documents)) pid=54464)  [repeated 66x across cluster]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(MapWorker(MapBatches(extract_properties)->MapBatches(sort_by_page_bbox)->MapBatches(mark_drop_tiny)->MapBatches(mark_drop_header_and_footer)->MapBatches(mark_break_page)->MapBatches(mark_break_by_column)->MapBatches(mark_break_by_tokens)->MapBatches(merge_elements)->MapBatches(convert_timestamp)->MapBatches(spread_properties)->MapBatches(split_doc)->MapBatches(explode)->MapBatches(sketcher)->MapBatches(SentenceTransformerEmbedder)->MapBatches(BaseMapTransformCallable__duckdb_write_documents)) pid=54464) /Users/karansampath/miniforge3/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "(MapWorker(MapBatches(extract_properties)->MapBatches(sort_by_page_bbox)->MapBatches(mark_drop_tiny)->MapBatches(mark_drop_header_and_footer)->MapBatches(mark_break_page)->MapBatches(mark_break_by_column)->MapBatches(mark_break_by_tokens)->MapBatches(merge_elements)->MapBatches(convert_timestamp)->MapBatches(spread_properties)->MapBatches(split_doc)->MapBatches(explode)->MapBatches(sketcher)->MapBatches(SentenceTransformerEmbedder)->MapBatches(BaseMapTransformCallable__duckdb_write_documents)) pid=54464)   warnings.warn('resource_tracker: There appear to be %d '\n"
     ]
    }
   ],
   "source": [
    "# Write to a persistent duckdb database\n",
    "# - into a specific database (as specified by url) \n",
    "# - into a specific table (as specified by table name)\n",
    "persistent_db = \"demo.db\"\n",
    "persistent_table = \"demo_table\"\n",
    "ds.write.duckdb(\n",
    "    db_url=persistent_db,\n",
    "    table_name=persistent_table\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We connect to the DuckDB to perform operations\n",
    "import duckdb\n",
    "data_conn = duckdb.connect(\"demo.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionException",
     "evalue": "Connection Error: Connection has already been closed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionException\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m in_memory_db \u001b[38;5;241m=\u001b[39m duckdb\u001b[38;5;241m.\u001b[39mconnect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:default:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# in_memory_db.install_extension(\"vss\")\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# in_memory_db.load_extension(\"vss\")\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[43min_memory_db\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[38;5;124;43mCREATE TABLE in_memory_table (doc_id \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdoc_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;43m                      embeddings \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43membeddings\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m, properties \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mproperties\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;43m                      text_representation \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext_representation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m, bbox \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbbox\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;43m                      shingles \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mshingles\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m, type \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m in_memory_db\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mINSERT INTO in_memory_table SELECT * FROM df; \u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124m                      CREATE INDEX in_memory_table_index ON in_memory_table USING HNSW(embeddings)\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n",
      "\u001b[0;31mConnectionException\u001b[0m: Connection Error: Connection has already been closed"
     ]
    }
   ],
   "source": [
    "# DuckDB is not production ready yet for efficient Vector Similarity Search on persistent databases (https://duckdb.org/docs/extensions/vss.html), \n",
    "# so we load our data into an in-memory database for this demo. In future versions, once DuckDB productionizes this feature, there will be no more need for this code cell\n",
    "\n",
    "# Load from disk into Pandas Dataframe as an intermediate step\n",
    "df = data_conn.execute(\"SELECT * FROM demo_table\").fetchdf()\n",
    "df.dropna(subset=[\"embeddings\"],inplace=True)\n",
    "\n",
    "# Now, we load the data into an in-memory database. Notice that we specify beforehand since we need the embedding column (must be specified as a FLOAT[N] where N is specified) \n",
    "# for Vector Similarity Search using HNSW to work\n",
    "schema = {\n",
    "            \"doc_id\": \"VARCHAR\",\n",
    "            \"embeddings\": \"FLOAT[384]\",\n",
    "            \"properties\": \"MAP(VARCHAR, VARCHAR)\",\n",
    "            \"text_representation\": \"VARCHAR\",\n",
    "            \"bbox\": \"DOUBLE[]\",\n",
    "            \"shingles\": \"BIGINT[]\",\n",
    "            \"type\": \"VARCHAR\",\n",
    "        }\n",
    "in_memory_db = duckdb.connect(\":default:\")\n",
    "# in_memory_db.install_extension(\"vss\")\n",
    "# in_memory_db.load_extension(\"vss\")\n",
    "in_memory_db.execute(f\"\"\"CREATE TABLE in_memory_table (doc_id {schema.get('doc_id')},\n",
    "                      embeddings {schema.get('embeddings')}, properties {schema.get('properties')}, \n",
    "                      text_representation {schema.get('text_representation')}, bbox {schema.get('bbox')}, \n",
    "                      shingles {schema.get('shingles')}, type {schema.get('type')})\"\"\"\n",
    "    )\n",
    "in_memory_db.execute(\"\"\"INSERT INTO in_memory_table SELECT * FROM df; \n",
    "                      CREATE INDEX in_memory_table_index ON in_memory_table USING HNSW(embeddings)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karansampath/Library/Caches/pypoetry/virtualenvs/sycamore-ai--zTjaFUY-py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# For queries, let's define an embedding function for the question that helps us easily compare the two vectors and generate optimal results // Run NN search\n",
    "from sentence_transformers import SentenceTransformer\n",
    "minilm = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "class Embedder():\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "    \n",
    "    def embed_query(self, question):\n",
    "        v = self.llm.encode(question).tolist()\n",
    "        return v\n",
    "\n",
    "embedder = Embedder(minilm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌──────────────────────┬──────────────────────┬────────────────────────────────────────────────────────────────────────┐\n",
      "│        doc_id        │ text_representation  │                               properties                               │\n",
      "│       varchar        │       varchar        │                         map(varchar, varchar)                          │\n",
      "├──────────────────────┼──────────────────────┼────────────────────────────────────────────────────────────────────────┤\n",
      "│ 73f87ba9-ff4e-4d75…  │ The National Trans…  │ {score=0.8915606737136841, page_numbers=[1], page_number=1, path=s3:…  │\n",
      "│ 06b31736-b0ed-4c57…  │ The National Trans…  │ {score=0.8774372339248657, page_numbers=[1], page_number=1, path=s3:…  │\n",
      "│ d701d8a7-b5fa-4a8f…  │ The National Trans…  │ {score=0.7743222713470459, page_numbers=[1], page_number=1, path=s3:…  │\n",
      "│ 36e85e87-85b7-4261…  │ The National Trans…  │ {score=0.7248827219009399, page_numbers=[1], page_number=1, path=s3:…  │\n",
      "│ 0918cf95-4cbb-452c…  │ The National Trans…  │ {score=0.7322630286216736, page_numbers=[1], page_number=1, path=s3:…  │\n",
      "│ cd5d8027-b01b-49ab…  │ The National Trans…  │ {score=0.7365314364433289, page_numbers=[1], page_number=1, path=s3:…  │\n",
      "│ 7a30f2ad-ebfe-457e…  │ The National Trans…  │ {score=0.6194686889648438, page_numbers=[1], page_number=1, path=s3:…  │\n",
      "│ 028ac479-d388-4320…  │ The National Trans…  │ {score=0.8677337765693665, page_numbers=[1], page_number=1, path=s3:…  │\n",
      "│ 2988b1ab-2eb1-4e64…  │ The National Trans…  │ {score=0.8568803071975708, page_numbers=[1], page_number=1, path=s3:…  │\n",
      "│ 3ffb4b09-71ad-43b3…  │ The National Trans…  │ {score=0.8405874371528625, page_numbers=[1], page_number=1, path=s3:…  │\n",
      "├──────────────────────┴──────────────────────┴────────────────────────────────────────────────────────────────────────┤\n",
      "│ 10 rows                                                                                                    3 columns │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let us now see the performance of the ANN HNSW search using DuckDB // array_distance is the nn lookup\n",
    "q = \"What automobile type is the most accident prone?\"\n",
    "in_memory_db.sql(f\"SELECT doc_id, text_representation, properties FROM in_memory_table ORDER BY array_distance(embeddings, {embed(q)}::FLOAT[384]) LIMIT 10;\")\n",
    "# Pretty print tables\n",
    "# text representation --> print one cell out (of the top result). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌──────────────────────┬──────────────────────┬────────────────────────────────────────────────────────────────────────┐\n",
       "│        doc_id        │ text_representation  │                               properties                               │\n",
       "│       varchar        │       varchar        │                         map(varchar, varchar)                          │\n",
       "├──────────────────────┼──────────────────────┼────────────────────────────────────────────────────────────────────────┤\n",
       "│ 3c82fabc-9f99-4dfe…  │ Airport Informatio…  │ {score=0.7310920357704163, page_numbers=[5], page_number=5, path=s3:…  │\n",
       "│ 8f3d0214-9de3-48c3…  │ Airport Informatio…  │ {score=0.702387809753418, page_numbers=[5], page_number=5, path=s3:/…  │\n",
       "│ 1c6083df-00a8-484e…  │ Airport Informatio…  │ {score=0.6993858218193054, page_numbers=[5], page_number=5, path=s3:…  │\n",
       "│ b10638a0-2ea2-4d23…  │ Airport Informatio…  │ {score=0.7263772487640381, page_numbers=[5], page_number=5, path=s3:…  │\n",
       "│ c0594fed-b67a-44b9…  │ Airport Informatio…  │ {score=0.7013193368911743, page_numbers=[5], page_number=5, path=s3:…  │\n",
       "│ b1896c98-e854-41cd…  │ Airport Informatio…  │ {score=0.7164381146430969, page_numbers=[5], page_number=5, path=s3:…  │\n",
       "│ 4da07493-8808-4b92…  │ Airport Informatio…  │ {score=0.72945636510849, page_numbers=[5], page_number=5, path=s3://…  │\n",
       "│ f2a4f9f8-fa4a-46e1…  │ Airport Informatio…  │ {score=0.7050329446792603, page_numbers=[5], page_number=5, path=s3:…  │\n",
       "│ 8a2faffe-4059-460d…  │ Airport Informatio…  │ {score=0.7286437153816223, page_numbers=[5], page_number=5, path=s3:…  │\n",
       "│ d0437fc8-0da0-4611…  │ Airport Informatio…  │ {score=0.7291507720947266, page_numbers=[5], page_number=5, path=s3:…  │\n",
       "├──────────────────────┴──────────────────────┴────────────────────────────────────────────────────────────────────────┤\n",
       "│ 10 rows                                                                                                    3 columns │\n",
       "└──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us now see the performance of the ANN HNSW search using DuckDB \n",
    "q = \"Traffic Statistics\"\n",
    "in_memory_db.sql(f\"SELECT doc_id, text_representation, properties FROM in_memory_table ORDER BY array_distance(embeddings, {embed(q)}::FLOAT[384]) LIMIT 10;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌───────────────┬──────────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
       "│  explain_key  │                                            explain_value                                             │\n",
       "│    varchar    │                                               varchar                                                │\n",
       "├───────────────┼──────────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
       "│ physical_plan │ ┌───────────────────────────┐\\n│         PROJECTION        │\\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\\n│    …  │\n",
       "└───────────────┴──────────────────────────────────────────────────────────────────────────────────────────────────────┘"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also see the performance of the DuckDB query and check the index is being used by using the in-built visualization below \n",
    "q = \"What automobile type is the most accident prone?\"\n",
    "in_memory_db.sql(f\"EXPLAIN SELECT doc_id, text_representation, properties FROM in_memory_table ORDER BY array_distance(embeddings, {embed(q)}::FLOAT[384]) LIMIT 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌──────────────────────┬──────────────────────┬────────────────────────────────────────────────────────────────────────┐\n",
       "│        doc_id        │ text_representation  │                               properties                               │\n",
       "│       varchar        │       varchar        │                         map(varchar, varchar)                          │\n",
       "├──────────────────────┼──────────────────────┼────────────────────────────────────────────────────────────────────────┤\n",
       "│ 73f87ba9-ff4e-4d75…  │ The National Trans…  │ {score=0.8915606737136841, page_numbers=[1], page_number=1, path=s3:…  │\n",
       "│ 06b31736-b0ed-4c57…  │ The National Trans…  │ {score=0.8774372339248657, page_numbers=[1], page_number=1, path=s3:…  │\n",
       "│ d701d8a7-b5fa-4a8f…  │ The National Trans…  │ {score=0.7743222713470459, page_numbers=[1], page_number=1, path=s3:…  │\n",
       "│ 36e85e87-85b7-4261…  │ The National Trans…  │ {score=0.7248827219009399, page_numbers=[1], page_number=1, path=s3:…  │\n",
       "│ 0918cf95-4cbb-452c…  │ The National Trans…  │ {score=0.7322630286216736, page_numbers=[1], page_number=1, path=s3:…  │\n",
       "│ cd5d8027-b01b-49ab…  │ The National Trans…  │ {score=0.7365314364433289, page_numbers=[1], page_number=1, path=s3:…  │\n",
       "│ 7a30f2ad-ebfe-457e…  │ The National Trans…  │ {score=0.6194686889648438, page_numbers=[1], page_number=1, path=s3:…  │\n",
       "│ 028ac479-d388-4320…  │ The National Trans…  │ {score=0.8677337765693665, page_numbers=[1], page_number=1, path=s3:…  │\n",
       "│ 2988b1ab-2eb1-4e64…  │ The National Trans…  │ {score=0.8568803071975708, page_numbers=[1], page_number=1, path=s3:…  │\n",
       "│ 3ffb4b09-71ad-43b3…  │ The National Trans…  │ {score=0.8405874371528625, page_numbers=[1], page_number=1, path=s3:…  │\n",
       "├──────────────────────┴──────────────────────┴────────────────────────────────────────────────────────────────────────┤\n",
       "│ 10 rows                                                                                                    3 columns │\n",
       "└──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally we can build an index with other distance metrics, the default is Euclidean Distance ('l2sq'), with Cosine similarity ('cosine', 'array_cosine_simarlity') \n",
    "# and Inner Product ('ip', 'array_inner_product') also supported\n",
    "in_memory_db.execute(\"CREATE INDEX in_memory_ip_index ON in_memory_table USING HNSW(embeddings) WITH (metric = 'ip')\")\n",
    "in_memory_db.sql(f\"SELECT doc_id, text_representation, properties FROM in_memory_table ORDER BY array_inner_product(embeddings, {embed(q)}::FLOAT[384]) LIMIT 10;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "CatalogException",
     "evalue": "Catalog Error: Column with name metadata already exists!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCatalogException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43min_memory_db\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mALTER TABLE in_memory_table ADD COLUMN metadata VARCHAR[]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mCatalogException\u001b[0m: Catalog Error: Column with name metadata already exists!"
     ]
    }
   ],
   "source": [
    "in_memory_db.sql(f\"ALTER TABLE in_memory_table ADD COLUMN metadata VARCHAR[]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "the JSON object must be str, bytes or bytearray, not float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m DuckDB(connection\u001b[38;5;241m=\u001b[39min_memory_db, embedding\u001b[38;5;241m=\u001b[39membedder, id_key\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoc_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, text_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_representation\u001b[39m\u001b[38;5;124m\"\u001b[39m, vector_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m, table_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_memory_table\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# vector_store.add_texts(['text1', 'text2'])\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mvector_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraffic\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sycamore-ai--zTjaFUY-py3.10/lib/python3.10/site-packages/langchain_community/vectorstores/duckdb.py:228\u001b[0m, in \u001b[0;36mDuckDB.similarity_search\u001b[0;34m(self, query, k, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m list_cosine_similarity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mduckdb\u001b[38;5;241m.\u001b[39mFunctionExpression(\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlist_cosine_similarity\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mduckdb\u001b[38;5;241m.\u001b[39mColumnExpression(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector_key),\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mduckdb\u001b[38;5;241m.\u001b[39mConstantExpression(embedding),\n\u001b[1;32m    216\u001b[0m )\n\u001b[1;32m    217\u001b[0m docs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_table\u001b[38;5;241m.\u001b[39mselect(\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;241m*\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;241m.\u001b[39mfetchdf()\n\u001b[1;32m    227\u001b[0m )\n\u001b[0;32m--> 228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    229\u001b[0m     Document(\n\u001b[1;32m    230\u001b[0m         page_content\u001b[38;5;241m=\u001b[39mdocs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_text_key][idx],\n\u001b[1;32m    231\u001b[0m         metadata\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    232\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mjson\u001b[38;5;241m.\u001b[39mloads(docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m][idx]),\n\u001b[1;32m    233\u001b[0m             \u001b[38;5;66;03m# using underscore prefix to avoid conflicts with user metadata keys\u001b[39;00m\n\u001b[1;32m    234\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSIMILARITY_ALIAS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m: docs[SIMILARITY_ALIAS][idx],\n\u001b[1;32m    235\u001b[0m         }\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m][idx]\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m {},\n\u001b[1;32m    238\u001b[0m     )\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(docs))\n\u001b[1;32m    240\u001b[0m ]\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sycamore-ai--zTjaFUY-py3.10/lib/python3.10/site-packages/langchain_community/vectorstores/duckdb.py:232\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    212\u001b[0m list_cosine_similarity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mduckdb\u001b[38;5;241m.\u001b[39mFunctionExpression(\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlist_cosine_similarity\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mduckdb\u001b[38;5;241m.\u001b[39mColumnExpression(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector_key),\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mduckdb\u001b[38;5;241m.\u001b[39mConstantExpression(embedding),\n\u001b[1;32m    216\u001b[0m )\n\u001b[1;32m    217\u001b[0m docs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_table\u001b[38;5;241m.\u001b[39mselect(\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;241m*\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;241m.\u001b[39mfetchdf()\n\u001b[1;32m    227\u001b[0m )\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    229\u001b[0m     Document(\n\u001b[1;32m    230\u001b[0m         page_content\u001b[38;5;241m=\u001b[39mdocs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_text_key][idx],\n\u001b[1;32m    231\u001b[0m         metadata\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m--> 232\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    233\u001b[0m             \u001b[38;5;66;03m# using underscore prefix to avoid conflicts with user metadata keys\u001b[39;00m\n\u001b[1;32m    234\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSIMILARITY_ALIAS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m: docs[SIMILARITY_ALIAS][idx],\n\u001b[1;32m    235\u001b[0m         }\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m][idx]\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m {},\n\u001b[1;32m    238\u001b[0m     )\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(docs))\n\u001b[1;32m    240\u001b[0m ]\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/json/__init__.py:339\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(s, (\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mbytearray\u001b[39m)):\n\u001b[0;32m--> 339\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe JSON object must be str, bytes or bytearray, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    340\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n",
      "\u001b[0;31mTypeError\u001b[0m: the JSON object must be str, bytes or bytearray, not float"
     ]
    }
   ],
   "source": [
    "# conn = duckdb.connect(database=':memory:',\n",
    "#     config={\n",
    "#         # Sample configuration to restrict some DuckDB capabilities\n",
    "#         # List is not exhaustive. Please review DuckDB documentation.\n",
    "#             \"enable_external_access\": \"false\",\n",
    "#             \"autoinstall_known_extensions\": \"false\",\n",
    "#             \"autoload_known_extensions\": \"false\"\n",
    "#         }\n",
    "# )\n",
    "from langchain_community.vectorstores.duckdb import DuckDB\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "vector_store = DuckDB(connection=in_memory_db, embedding=embedder, id_key= \"doc_id\", text_key=\"text_representation\", vector_key=\"embeddings\", table_name=\"in_memory_table\")\n",
    "# vector_store.add_texts(['text1', 'text2'])\n",
    "result = vector_store.similarity_search('traffic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karansampath/Library/Caches/pypoetry/virtualenvs/sycamore-ai--zTjaFUY-py3.10/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`run` not supported when there is not exactly one output key. Got ['answer', 'sources'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 15\u001b[0m\n\u001b[1;32m      5\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatOpenAI(  \n\u001b[1;32m      6\u001b[0m     openai_api_key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m),  \n\u001b[1;32m      7\u001b[0m     model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m'\u001b[39m,  \n\u001b[1;32m      8\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m  \n\u001b[1;32m      9\u001b[0m )  \n\u001b[1;32m     10\u001b[0m qa \u001b[38;5;241m=\u001b[39m RetrievalQAWithSourcesChain\u001b[38;5;241m.\u001b[39mfrom_chain_type(  \n\u001b[1;32m     11\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,  \n\u001b[1;32m     12\u001b[0m     chain_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstuff\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \n\u001b[1;32m     13\u001b[0m     retriever\u001b[38;5;241m=\u001b[39mvector_store\u001b[38;5;241m.\u001b[39mas_retriever()  \n\u001b[1;32m     14\u001b[0m )  \n\u001b[0;32m---> 15\u001b[0m \u001b[43mqa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sycamore-ai--zTjaFUY-py3.10/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:168\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     emit_warning()\n\u001b[0;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sycamore-ai--zTjaFUY-py3.10/lib/python3.10/site-packages/langchain/chains/base.py:595\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Convenience method for executing chain.\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \n\u001b[1;32m    560\u001b[0m \u001b[38;5;124;03mThe main difference between this method and `Chain.__call__` is that this\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;124;03m        # -> \"The temperature in Boise is...\"\u001b[39;00m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;66;03m# Run at start to make sure this is possible/defined\u001b[39;00m\n\u001b[0;32m--> 595\u001b[0m _output_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_output_key\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs:\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sycamore-ai--zTjaFUY-py3.10/lib/python3.10/site-packages/langchain/chains/base.py:543\u001b[0m, in \u001b[0;36mChain._run_output_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_output_key\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 543\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    544\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` not supported when there is not exactly \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mone output key. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         )\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: `run` not supported when there is not exactly one output key. Got ['answer', 'sources']."
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI  \n",
    "from langchain.chains import RetrievalQAWithSourcesChain  \n",
    "import os\n",
    "# completion llm  \n",
    "llm = ChatOpenAI(  \n",
    "    openai_api_key=os.environ.get(\"OPENAI_API_KEY\"),  \n",
    "    model_name='gpt-3.5-turbo',  \n",
    "    temperature=0.0  \n",
    ")  \n",
    "qa = RetrievalQAWithSourcesChain.from_chain_type(  \n",
    "    llm=llm,  \n",
    "    chain_type=\"stuff\",  \n",
    "    retriever=vector_store.as_retriever()  \n",
    ")  \n",
    "qa.run(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: If you would like to remove your database after running the code above\n",
    "try:\n",
    "    os.unlink(persistent_db)\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting {persistent_db}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sycamore-ai--zTjaFUY-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
