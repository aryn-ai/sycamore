{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First some imports\n",
    "import pyarrow.fs\n",
    "from ray.data import ActorPoolStrategy\n",
    "import sycamore\n",
    "from sycamore.functions.tokenizer import HuggingFaceTokenizer\n",
    "from sycamore.llms import OpenAIModels, OpenAI\n",
    "from sycamore.transforms import COALESCE_WHITESPACE\n",
    "from sycamore.transforms.merge_elements import MarkedMerger\n",
    "from sycamore.transforms.partition import SycamorePartitioner\n",
    "from sycamore.transforms.extract_schema import OpenAISchemaExtractor, OpenAIPropertyExtractor\n",
    "from sycamore.transforms.embed import SentenceTransformerEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a function to convert llm-generated date/time strings into well-structured datetime strings\n",
    "from sycamore.data.document import Document\n",
    "from dateutil import parser\n",
    "def convert_timestamp(doc: Document) -> Document:\n",
    "    if \"dateAndTime\" not in doc.properties['entity'] and \"dateTime\" not in doc.properties['entity']:\n",
    "        return doc\n",
    "    raw_date: str = doc.properties['entity'].get('dateAndTime') or doc.properties['entity'].get('dateTime')\n",
    "    raw_date = raw_date.replace(\"Local\", \"\")\n",
    "    parsed_date = parser.parse(raw_date, fuzzy=True)\n",
    "    extracted_date = parsed_date.date()\n",
    "    doc.properties['entity']['day'] = extracted_date.isoformat()\n",
    "    if parsed_date.utcoffset():\n",
    "        doc.properties['entity']['dateTime'] = parsed_date.isoformat()\n",
    "    else:\n",
    "        doc.properties['entity']['dateTime'] = parsed_date.isoformat() + \"Z\"\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"s3://aryn-public/ntsb/\"]\n",
    "fsys = pyarrow.fs.S3FileSystem(region=\"us-east-1\", anonymous=True)\n",
    "\n",
    "llm = OpenAI(OpenAIModels.GPT_3_5_TURBO.value)\n",
    "tokenizer = HuggingFaceTokenizer(\"thenlper/gte-small\")\n",
    "\n",
    "ctx = sycamore.init()\n",
    "\n",
    "# Main ingest pipeline. Note the use of `.term_frequency()`, which will enable hybrid search in pinecone\n",
    "ds = (\n",
    "    ctx.read.binary(paths, binary_format=\"pdf\", filesystem=fsys)\n",
    "    .partition(partitioner=SycamorePartitioner(extract_table_structure=True, extract_images=True), compute=ActorPoolStrategy(size=4))\n",
    "    .regex_replace(COALESCE_WHITESPACE)\n",
    "    .extract_batch_schema(schema_extractor=OpenAISchemaExtractor(\"FlightAccidentReport\", llm=llm, num_of_elements=35))\n",
    "    .extract_properties(property_extractor=OpenAIPropertyExtractor(llm=llm, num_of_elements=35))\n",
    "    .mark_bbox_preset(tokenizer=tokenizer)\n",
    "    .merge(merger=MarkedMerger())\n",
    "    .map(convert_timestamp)\n",
    "    .spread_properties([\"path\", \"entity\"])\n",
    "    .split_elements(tokenizer=tokenizer, max_tokens=512)\n",
    "    .explode()\n",
    "    .sketch()\n",
    "    .embed(embedder=SentenceTransformerEmbedder(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", batch_size=100))\n",
    "    .term_frequency(tokenizer=tokenizer, with_token_ids=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to pinecone\n",
    "import pinecone\n",
    "ds.write.pinecone(\n",
    "    index_name=\"ntsb\",\n",
    "    index_spec=pinecone.ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    dimensions=384,\n",
    "    distance_metric=\"dotproduct\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = pinecone.Pinecone()\n",
    "pc.describe_index(\"ntsb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntsb = pc.Index(name=\"ntsb\")\n",
    "ntsb.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pinecone needs us to generate our own query vectors\n",
    "from sentence_transformers import SentenceTransformer\n",
    "minilm = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def q_vec(question):\n",
    "    v = minilm.encode(question).tolist()\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the ntsb index with only dense semantic search\n",
    "def pure_semantic_query(question):\n",
    "    results = ntsb.query(\n",
    "        top_k=5,\n",
    "        vector=q_vec(question),\n",
    "        include_metadata=True,\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pure_semantic_query(\"plane crashes in california\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty printing\n",
    "def print_results_pretty(results):\n",
    "    hits = results.get(\"matches\")\n",
    "    metadata = [h.get(\"metadata\") for h in hits]\n",
    "    for m in metadata:\n",
    "        print(f\"{m.get('properties.entity.accidentNumber', 'UNKNOWN')} {'='*80}\")\n",
    "        print(f\"Aircraft: {m.get('properties.entity.aircraft', 'UNKNOWN')}\")\n",
    "        print(f\"Location: {m.get('properties.entity.location', 'UNKNOWN')}\")\n",
    "        print(f\"Date:     {m.get('properties.entity.day', 'UNKNOWN')}\")\n",
    "        print(f\"Damage:   {m.get('properties.entity.aircraftDamage', 'UNKNOWN')}\")\n",
    "        print(f\"Text:     {m.get('text_representation')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results_pretty(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also have to generate our own sparse vectors. Note that the pinecone implementation of\n",
    "# sparse vector search makes it difficult to perform BM-25 (TF/IDF) search. Instead we use \n",
    "# pure term frequency, which will cause some common words to be inordinately 'meaningful'.\n",
    "from collections import Counter\n",
    "\n",
    "def s_vec(question):\n",
    "    tokens = tokenizer.tokenize(question, as_ints=True)\n",
    "    table = dict(Counter(tokens))\n",
    "    indices = list(table.keys())\n",
    "    values = [float(v) for v in table.values()]\n",
    "    return {\"indices\": indices, \"values\": values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_vec(\"Mary had a little little lamb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to combine sparse (TF) and dense (embedding) vectors into a \n",
    "# query. Alpha is the weight to give to the dense vector as opposed to\n",
    "# the sparse vector... alpha=1 means only care about the dense vector \n",
    "# and alpha=0 means only care about the sparse vector.\n",
    "def hybrid_query(question, alpha=0.8):\n",
    "    qv = q_vec(question)\n",
    "    sv = s_vec(question)\n",
    "    qv = [v * alpha for v in qv]\n",
    "    sv['values'] = [v * (1 - alpha) for v in sv['values']]\n",
    "    results = ntsb.query(\n",
    "        top_k=5,\n",
    "        vector=qv,\n",
    "        sparse_vector=sv,\n",
    "        include_metadata=True,\n",
    "    )\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rh = hybrid_query(\"plane crashes in california\", alpha=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results_pretty(rh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we're looking for specific pieces of information, 'fuzzy' hybrid/semantic search\n",
    "# won't quite cut it.\n",
    "print_results_pretty(hybrid_query(\"plane crashes where the plane was destroyed\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead we can add filters.\n",
    "def hybrid_query_filtered(question, filter, alpha=0.8):\n",
    "    qv = q_vec(question)\n",
    "    sv = s_vec(question)\n",
    "    qv = [v * alpha for v in qv]\n",
    "    sv['values'] = [v * (1 - alpha) for v in sv['values']]\n",
    "    results = ntsb.query(\n",
    "        top_k=5,\n",
    "        vector=qv,\n",
    "        sparse_vector=sv,\n",
    "        include_metadata=True,\n",
    "        filter=filter\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results_pretty(hybrid_query_filtered(\"plane crashes where the plane was destroyed\", filter={\"properties.entity.aircraftDamage\": {\"$eq\": \"Destroyed\"}}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
