{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Aryn Partitioner in this job is configured to use the Aryn Partitioning Service to provide fast, GPU-powered performance. Go to [aryn.ai/sign-up ](aryn.ai/get-started) to get a free API key for the service. This is the recommended configuration.\n",
    "\n",
    "You can also run the Aryn Partitioner locally by setting `use_partitioning_service` to `False`. Though you can use CPU to run the Aryn Partitioner, it is recommended to use an NVIDIA GPU for good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First some imports\n",
    "import pyarrow.fs\n",
    "import sycamore\n",
    "from sycamore.functions.tokenizer import HuggingFaceTokenizer\n",
    "from sycamore.llms import OpenAIModels, OpenAI\n",
    "from sycamore.transforms import COALESCE_WHITESPACE\n",
    "from sycamore.transforms.merge_elements import MarkedMerger\n",
    "from sycamore.transforms.partition import ArynPartitioner\n",
    "from sycamore.transforms.extract_schema import OpenAISchemaExtractor, OpenAIPropertyExtractor\n",
    "from sycamore.transforms.embed import SentenceTransformerEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a function to convert llm-generated date/time strings into well-structured datetime strings\n",
    "# We also extract the exact day, month, and year as integers in order to do range filtering in our\n",
    "# queries\n",
    "from sycamore.data.document import Document\n",
    "from dateutil import parser\n",
    "def convert_timestamp(doc: Document) -> Document:\n",
    "    if \"dateAndTime\" not in doc.properties['entity'] and \"dateTime\" not in doc.properties['entity']:\n",
    "        return doc\n",
    "    raw_date: str = doc.properties['entity'].get('dateAndTime') or doc.properties['entity'].get('dateTime')\n",
    "    raw_date = raw_date.replace(\"Local\", \"\")\n",
    "    parsed_date = parser.parse(raw_date, fuzzy=True)\n",
    "    extracted_date = parsed_date.date()\n",
    "    doc.properties['entity']['day'] = extracted_date.day\n",
    "    doc.properties['entity']['month'] = extracted_date.month\n",
    "    doc.properties['entity']['year'] = extracted_date.year\n",
    "    if parsed_date.utcoffset():\n",
    "        doc.properties['entity']['dateTime'] = parsed_date.isoformat()\n",
    "    else:\n",
    "        doc.properties['entity']['dateTime'] = parsed_date.isoformat() + \"Z\"\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sycamore.utils.aryn_config import ArynConfig, _DEFAULT_PATH\n",
    "assert ArynConfig.get_aryn_api_key() != \"\", f\"Unable to find aryn API key.  Looked in {_DEFAULT_PATH}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the above assertion fails, you can either set the environment variable ARYN_API_KEY and restart jupyter\n",
    "or make a yaml file at the specified path in the assertion error that looks like:\n",
    "\n",
    "```\n",
    "aryn_token: \"YOUR-ARYN-API-KEY\"\n",
    "```\n",
    "\n",
    "It is unsafe, but if neither of those options work, you can put it in this notebook with\n",
    "```\n",
    "import os\n",
    "os.environ[\"ARYN_API_KEY\"] = \"UNSAFE-ARYN-API-KEY-LOCATION\" \n",
    "```\n",
    "\n",
    "but beware that it is easy to accidentally commit the notebook file and have it include your key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"s3://aryn-public/ntsb/\"]\n",
    "fsys = pyarrow.fs.S3FileSystem(region=\"us-east-1\", anonymous=True)\n",
    "\n",
    "llm = OpenAI(OpenAIModels.GPT_3_5_TURBO.value)\n",
    "tokenizer = HuggingFaceTokenizer(\"thenlper/gte-small\")\n",
    "\n",
    "ctx = sycamore.init()\n",
    "\n",
    "# Main ingest pipeline. Note the use of `.term_frequency()`, which will enable hybrid search in pinecone\n",
    "ds = (\n",
    "    ctx.read.binary(paths, binary_format=\"pdf\", filesystem=fsys)\n",
    "    # Partition with the Aryn partitioner remotely, pulling out tables and images.\n",
    "    .partition(partitioner=ArynPartitioner(extract_images=True,  extract_table_structure=True))\n",
    "    # Get rid of spurious whitespace charaters\n",
    "    .regex_replace(COALESCE_WHITESPACE)\n",
    "    # Automatically determine a schema of additional metadata to extract from documents\n",
    "    .extract_batch_schema(schema_extractor=OpenAISchemaExtractor(\"FlightAccidentReport\", llm=llm, num_of_elements=35))\n",
    "    # Extract the metadata specified by that schema\n",
    "    .extract_properties(property_extractor=OpenAIPropertyExtractor(llm=llm, num_of_elements=35))\n",
    "    # Merge elements into larger chunks\n",
    "    .mark_bbox_preset(tokenizer=tokenizer)\n",
    "    .merge(merger=MarkedMerger())\n",
    "    # Convert extracted timestamps to better-structured form using the function above\n",
    "    .map(convert_timestamp)\n",
    "    # Copy document properties to each document's sub-elements\n",
    "    .spread_properties([\"path\", \"entity\"])\n",
    "    # Split elements that are too big to embed\n",
    "    .split_elements(tokenizer=tokenizer, max_tokens=512)\n",
    "    # Convert all Elements to Documents\n",
    "    .explode()\n",
    "    # Generate a series of hashes to represent each document. For use with near-duplicate detection\n",
    "    .sketch()\n",
    "    # Embed each document\n",
    "    .embed(embedder=SentenceTransformerEmbedder(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", batch_size=100))\n",
    "    # Count the number of occurrences of every token for each document\n",
    "    .term_frequency(tokenizer=tokenizer, with_token_ids=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to pinecone\n",
    "# - into an index named 'ntsb'\n",
    "# - into a serverless index in aws/us-east-1\n",
    "# - into a 384-dimensional index. The embedding model we used (all-MiniLM-L6-v2) generates 384-dimensional vectors\n",
    "# - into an index using the dotproduct distance metric. This is necessary to do sparse vector search\n",
    "import pinecone\n",
    "import os\n",
    "\n",
    "ds.write.pinecone(\n",
    "    index_name=\"ntsb\",\n",
    "    index_spec=pinecone.ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    dimensions=384,\n",
    "    distance_metric=\"dotproduct\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tell me about my index\n",
    "import pinecone\n",
    "pc = pinecone.Pinecone()\n",
    "pc.describe_index(\"ntsb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many things are in my index?\n",
    "ntsb = pc.Index(name=\"ntsb\")\n",
    "ntsb.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pinecone needs us to generate our own query vectors, so we define a function to simplify this\n",
    "from sentence_transformers import SentenceTransformer\n",
    "minilm = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def q_vec(question):\n",
    "    v = minilm.encode(question).tolist()\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to query the ntsb index with only dense semantic search (using q_vec to generate the vector)\n",
    "def pure_semantic_query(question):\n",
    "    results = ntsb.query(\n",
    "        top_k=5,\n",
    "        vector=q_vec(question),\n",
    "        include_metadata=True,\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pure_semantic_query(\"incidents in california\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty printing\n",
    "def print_results_pretty(results):\n",
    "    hits = results.get(\"matches\")\n",
    "    metadata = [h.get(\"metadata\") for h in hits]\n",
    "    for m in metadata:\n",
    "        day = int(m.get('properties.entity.day', -1))\n",
    "        month = int(m.get('properties.entity.month', -1))\n",
    "        year = int(m.get('properties.entity.year', -1))\n",
    "        print(f\"{m.get('properties.entity.accidentNumber', 'UNKNOWN')} {'='*80}\")\n",
    "        print(f\"Aircraft: {m.get('properties.entity.aircraft', 'UNKNOWN')}\")\n",
    "        print(f\"Location: {m.get('properties.entity.location', 'UNKNOWN')}\")\n",
    "        print(f\"Date:     {year}-{month}-{day}\")\n",
    "        print(f\"Damage:   {m.get('properties.entity.aircraftDamage', 'UNKNOWN')}\")\n",
    "        print(f\"Text:     {m.get('text_representation')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results_pretty(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also have to generate our own sparse vectors. Note that the pinecone implementation of\n",
    "# sparse vector search makes it difficult to perform BM-25 (TF/IDF) search. Instead we use \n",
    "# pure term frequency, which will cause some common words to be inordinately 'meaningful'.\n",
    "from collections import Counter\n",
    "\n",
    "def s_vec(question):\n",
    "    tokens = tokenizer.tokenize(question, as_ints=True)\n",
    "    table = dict(Counter(tokens))\n",
    "    indices = list(table.keys())\n",
    "    values = [float(v) for v in table.values()]\n",
    "    return {\"indices\": indices, \"values\": values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example...\n",
    "s_vec(\"Mary had a little little lamb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to combine sparse (TF) and dense (embedding) vectors into a \n",
    "# query. Alpha is the weight to give to the dense vector as opposed to\n",
    "# the sparse vector... alpha=1 means only care about the dense vector \n",
    "# and alpha=0 means only care about the sparse vector.\n",
    "def hybrid_query(question, alpha=0.8):\n",
    "    qv = q_vec(question)\n",
    "    sv = s_vec(question)\n",
    "    qv = [v * alpha for v in qv]\n",
    "    sv['values'] = [v * (1 - alpha) for v in sv['values']]\n",
    "    results = ntsb.query(\n",
    "        top_k=5,\n",
    "        vector=qv,\n",
    "        sparse_vector=sv,\n",
    "        include_metadata=True,\n",
    "    )\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rh = hybrid_query(\"incidents in california\", alpha=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results_pretty(rh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we're looking for specific pieces of information, 'fuzzy' hybrid/semantic search\n",
    "# won't quite cut it. In this example, we get a bunch of results outside of the specified\n",
    "# time window.\n",
    "print_results_pretty(hybrid_query(\"incidents in the last 2 weeks of january 2023\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead we can add filters.\n",
    "def hybrid_query_filtered(question, filter, alpha=0.8):\n",
    "    qv = q_vec(question)\n",
    "    sv = s_vec(question)\n",
    "    qv = [v * alpha for v in qv]\n",
    "    sv['values'] = [v * (1 - alpha) for v in sv['values']]\n",
    "    results = ntsb.query(\n",
    "        top_k=5,\n",
    "        vector=qv,\n",
    "        sparse_vector=sv,\n",
    "        include_metadata=True,\n",
    "        filter=filter\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This filter says year=2023, month=1, and day>=17 (31 - 14 = 17)\n",
    "# When we includ it, all of our search results are from the correct time window\n",
    "filter = {\n",
    "    \"$and\": [\n",
    "        {\"properties.entity.year\": {\"$eq\": 2023}},\n",
    "        {\"properties.entity.month\": {\"$eq\": 1}},\n",
    "        {\"properties.entity.day\": {\"$gte\": 17}}\n",
    "    ]\n",
    "}\n",
    "print_results_pretty(hybrid_query_filtered(\"incidents in the last two weeks of january\", filter=filter))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
