{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3fccd3-55d4-49e1-bc41-5b52ce2cd216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sycamore\n",
    "from sycamore.data import Document\n",
    "from sycamore.execution.transforms.embedding import SentenceTransformerEmbedder\n",
    "\n",
    "from sycamore.execution.transforms.entity_extraction import OpenAIEntityExtractor\n",
    "from sycamore.execution.transforms.llms.llms import OpenAIModels, OpenAI, LLM\n",
    "from sycamore.execution.transforms.partition import UnstructuredPdfPartitioner, HtmlPartitioner\n",
    "from sycamore.execution.transforms.prompts.default_prompts import TEXT_SUMMARIZER_GUIDANCE_PROMPT_CHAT\n",
    "from sycamore.execution.transforms.summarize import Summarizer\n",
    "from sycamore.execution.transforms.table_extraction import TextractTableExtractor\n",
    "from sycamore.tests.config import TEST_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e330cdd-ed54-4f78-b284-58fdd0c8e51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = str(TEST_DIR / \"resources/data/pdfs/\")\n",
    "\n",
    "openai_llm = OpenAI(OpenAIModels.GPT_3_5_TURBO.value, \"api-key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac722fc-a8ac-4d93-a719-9c308ecabeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    " title_context_template = \"\"\"\n",
    "    ELEMENT 1: Jupiter's Moons\n",
    "    ELEMENT 2: Ganymede 2020\n",
    "    ELEMENT 3: by Audi Lauper and Serena K. Goldberg. 2011\n",
    "    ELEMENT 4: From Wikipedia, the free encyclopedia\n",
    "    ELEMENT 5: Ganymede, or Jupiter III, is the largest and most massive natural satellite of Jupiter as well as in the Solar System, being a planetary-mass moon. It is the largest Solar System object without an atmosphere, despite being the only moon of the Solar System with a magnetic field. Like Titan, it is larger than the planet Mercury, but has somewhat less surface gravity than Mercury, Io or the Moon.\n",
    "    =========\n",
    "    \"Ganymede 2020\"\n",
    "\n",
    "    ELEMENT 1: FLAVR: Flow-Agnostic Video Representations for Fast Frame Interpolation\n",
    "    ELEMENT 2: Tarun Kalluri * UCSD\n",
    "    ELEMENT 3: Deepak Pathak CMU\n",
    "    ELEMENT 4: Manmohan Chandraker UCSD\n",
    "    ELEMENT 5: Du Tran Facebook AI\n",
    "    ELEMENT 6: https://tarun005.github.io/FLAVR/\n",
    "    ELEMENT 7: 2 2 0 2\n",
    "    ELEMENT 8: b e F 4 2\n",
    "    ELEMENT 9: ]\n",
    "    ELEMENT 10: V C . s c [\n",
    "    ========\n",
    "    \"FLAVR: Flow-Agnostic Video Representations for Fast Frame Interpolation\"\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8eaea7-fb3f-409a-b037-7b2affbd1d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_context_template = \"\"\"\n",
    "        ELEMENT 1: Jupiter's Moons\n",
    "        ELEMENT 2: Ganymede 2020\n",
    "        ELEMENT 3: by Audi Lauper and Serena K. Goldberg. 2011\n",
    "        ELEMENT 4: From Wikipedia, the free encyclopedia\n",
    "        ELEMENT 5: Ganymede, or Jupiter III, is the largest and most massive natural satellite of Jupiter as well as in the Solar System, being a planetary-mass moon. It is the largest Solar System object without an atmosphere, despite being the only moon of the Solar System with a magnetic field. Like Titan, it is larger than the planet Mercury, but has somewhat less surface gravity than Mercury, Io or the Moon.\n",
    "        =========\n",
    "        Audi Laupe, Serena K. Goldberg\n",
    "\n",
    "        ELEMENT 1: FLAVR: Flow-Agnostic Video Representations for Fast Frame Interpolation\n",
    "        ELEMENT 2: Tarun Kalluri * UCSD\n",
    "        ELEMENT 3: Deepak Pathak CMU\n",
    "        ELEMENT 4: Manmohan Chandraker UCSD\n",
    "        ELEMENT 5: Du Tran Facebook AI\n",
    "        ELEMENT 6: https://tarun005.github.io/FLAVR/\n",
    "        ELEMENT 7: 2 2 0 2\n",
    "        ELEMENT 8: b e F 4 2\n",
    "        ELEMENT 9: ]\n",
    "        ELEMENT 10: V C . s c [\n",
    "        ========\n",
    "        Tarun Kalluri, Deepak Pathak, Manmohan Chandraker, Du Tran\n",
    "\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c8276f-d47a-4780-8da5-6f47796d78f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "  abstract_prompt_template = \"\"\"    \n",
    "        ELEMENT 1: Attention Is All You Need\n",
    "        ELEMENT 2: Ashish Vaswani∗ Google Brain avaswani@google.com\n",
    "        ELEMENT 3: Noam Shazeer∗ Google Brain noam@google.com\n",
    "        ELEMENT 4: Niki Parmar∗ Google Research nikip@google.com\n",
    "        ELEMENT 5: Jakob Uszkoreit∗ Google Research usz@google.com\n",
    "        ELEMENT 6: Llion Jones∗ Google Research llion@google.com\n",
    "        ELEMENT 7: Aidan N. Gomez∗ † University of Toronto aidan@cs.toronto.edu\n",
    "        ELEMENT 8: Łukasz Kaiser∗ Google Brain lukaszkaiser@google.com\n",
    "        ELEMENT 9: Abstract\n",
    "        ELEMENT 10: The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.\n",
    "        ========\n",
    "        \"abstract\": The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.\n",
    "        ELEMENT 1: Ray: A Distributed Framework for Emerging AI Applications\n",
    "        ELEMENT 2: Philipp Moritz∗, Robert Nishihara∗, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang, Melih Elibol, Zongheng Yang, William Paul, Michael I. Jordan, Ion Stoica University of California, Berkeley\n",
    "        ELEMENT 3: 8 1 0 2\n",
    "        ELEMENT 4: Abstract\n",
    "        ELEMENT 5: and their use in prediction. These frameworks often lever- age specialized hardware (e.g., GPUs and TPUs), with the goal of reducing training time in a batch setting. Examples include TensorFlow [7], MXNet [18], and PyTorch [46]. The promise of AI is, however, far broader than classi- cal supervised learning. Emerging AI applications must increasingly operate in dynamic environments, react to changes in the environment, and take sequences of ac- tions to accomplish long-term goals [8, 43]. They must aim not only to exploit the data gathered, but also to ex- plore the space of possible actions. These broader require- ments are naturally framed within the paradigm of rein- forcement learning (RL). RL deals with learning to oper- ate continuously within an uncertain environment based on delayed and limited feedback [56]. RL-based systems have already yielded remarkable results, such as Google's AlphaGo beating a human world champion [54], and are beginning to ﬁnd their way into dialogue systems, UAVs [42], and robotic manipulation [25, 60].\n",
    "        ELEMENT 6: p e S 0 3\n",
    "        ELEMENT 7: ]\n",
    "        ELEMENT 8: C D . s c [\n",
    "        ELEMENT 9: 2 v 9 8 8 5 0 . 2 1 7 1 : v i X r a\n",
    "        ELEMENT 10: The next generation of AI applications will continuously interact with the environment and learn from these inter- actions. These applications impose new and demanding systems requirements, both in terms of performance and ﬂexibility. In this paper, we consider these requirements and present Ray—a distributed system to address them. Ray implements a uniﬁed interface that can express both task-parallel and actor-based computations, supported by a single dynamic execution engine. To meet the perfor- mance requirements, Ray employs a distributed scheduler and a distributed and fault-tolerant store to manage the system's control state. In our experiments, we demon- strate scaling beyond 1.8 million tasks per second and better performance than existing specialized systems for several challenging reinforcement learning applications.\n",
    "        ========\n",
    "        \"abstract\": The next generation of AI applications will continuously interact with the environment and learn from these inter- actions. These applications impose new and demanding systems requirements, both in terms of performance and ﬂexibility. In this paper, we consider these requirements and present Ray—a distributed system to address them. Ray implements a uniﬁed interface that can express both task-parallel and actor-based computations, supported by a single dynamic execution engine. To meet the perfor- mance requirements, Ray employs a distributed scheduler and a distributed and fault-tolerant store to manage the system's control state. In our experiments, we demon- strate scaling beyond 1.8 million tasks per second and better performance than existing specialized systems for several challenging reinforcement learning applications.\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b000a40-7df6-44b0-9deb-444038d9a221",
   "metadata": {},
   "outputs": [],
   "source": [
    "  class AbstractSummarizer(Summarizer):\n",
    "        def __init__(self, llm: LLM):\n",
    "            self._llm = llm\n",
    "\n",
    "        def summarize(self, document: Document) -> Document:\n",
    "            prompt = TEXT_SUMMARIZER_GUIDANCE_PROMPT_CHAT\n",
    "            response = self._llm.generate(prompt_kwargs={\"prompt\": prompt, \"query\": document.properties[\"abstract\"]})\n",
    "            document.properties[\"abstract_summary\"] = response[\"summary\"]\n",
    "            return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5168af9a-5351-4f72-ba6b-7afe6d9bfc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = sycamore.init()\n",
    "pdf_docset = ds = (\n",
    "        context.read.binary(paths, binary_format=\"pdf\")\n",
    "        .partition(partitioner=UnstructuredPdfPartitioner())\n",
    "        .extract_entity(entity_extractor=OpenAIEntityExtractor(\"title\", llm=openai_llm, prompt_template=title_context_template))\n",
    "        .extract_entity(entity_extractor=OpenAIEntityExtractor(\"authors\", llm=openai_llm, prompt_template=author_context_template))\n",
    "        .extract_entity(entity_extractor=OpenAIEntityExtractor(\"abstract\", llm=openai_llm, prompt_template=abstract_prompt_template))\n",
    "        .summarize(summarizer=AbstractSummarizer(openai_llm))\n",
    "        # .explode()\n",
    "        # .embed(embedder=SentenceTransformerEmbedder(batch_size=100, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"))\n",
    "    )\n",
    "\n",
    "\n",
    "for document in pdf_docset.take():\n",
    "    for property, value in document.properties.items():\n",
    "        print(f\"{property}: {value}\", \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de65e16-69c1-423a-ab37-3f8243206ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_paths = str(TEST_DIR / \"resources/data/htmls/\")\n",
    "html_docset = (\n",
    "        context.read.binary(html_paths, binary_format=\"html\")\n",
    "        .partition(partitioner=HtmlPartitioner())\n",
    "        # .explode()\n",
    "        # .embed(SentenceTransformerEmbedder(batch_size=100, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"))\n",
    "    )\n",
    "\n",
    "for document in html_docset.take():\n",
    "    print(document.properties, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a6f202-c8ba-4b25-bc9c-3cd27f1ff359",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
